### 整体架构与数据流
- 核心流程：`FastAPI(app.py)` → 表单/SSE → `KnowledgeFlow` → 个性化搜索(`SearchManager`/`ArxivSearcher`/Google Serper) → 报告生成(`ReportGenerator` + `LLMInterface`) → 流式回传到前端。
- 画像与存储：`UserProfileManager` + `db_utils` 维护 `SQLite` 表，支持兴趣/技能/教育/工作/搜索历史等。
- LLM与流式：`LLMInterface` 统一调用 DeepSeek Chat Completions 流式与非流式接口，进程级 `_GLOBAL_SESSION` 统一会话在 `shutdown` 关闭。
- 前端：`templates/index.html` 以 FormData POST `/process`，使用 EventSource 解析器以自定义 SSE payload 渲染 Markdown 与进度条。

### 主要模块
- `src/core`
  - `knowledge_flow.py`: 组织画像→翻译/优化查询→多平台并发搜索→流式生成报告；支持 literature_review/industry_research/popular_science/standard。
  - `generators.py`: 四类报告的 prompt 模板与分段/并行合并（综述）实现；尾部统一 `ReferenceFormatter`。
  - `search.py`: 多平台抽象，Google 使用 Serper API，同步封装到 asyncio.to_thread；ArXiv 调 `improved_arxiv_search.ArxivSearcher`，支持排序/时间/类别。
  - `llm_interface.py`: `aiohttp` 流式解析 `data:` 行；并行 `parallel_process`。
  - `reference_formatter.py`: 提取/去重/格式化引用，支持 markdown/html/text 多格式。
- `src/models`
  - `user_profile.py`: 用户画像管理（大量 DB 写入），亦含对 OpenAI SDK 的使用以解析简历技能/兴趣（走 DeepSeek 兼容）。
  - `resume_reader.py`: 支持 txt/pdf/docx/excel/image OCR。
- `src/utils.py`: 日志、DB 验证自愈、BERT 相似度工具。
- `scripts`: 初始化/备份/优化/查看/验证等维护脚本；附 `interest_categories.json` 示例。

### FastAPI 路由与SSE协议
- `GET /`: 渲染 `templates/index.html`
- `POST /process`: 将表单映射为 `user_input_data`，预先落地上传简历到临时文件，再创建 `EventSourceResponse(knowledge_flow_sse_generator(...))`
- `POST /api/enhanced_literature_review` 与 `/api/enhanced_literature_review_stream`: 面向增强综述直连 API
- `POST /api/get_references`: 直接返回格式化引用片段

前端事件：
- progress: {"step","total_steps","message"}
- report_start: 开始渲染
- report_chunk: 纯文本 markdown 片段
- complete/error: 完成或失败

### 配置与依赖
- `src/config.Config`: 环境变量读取（DEEPSEEK_API_KEY、LLM_API_BASE、LLM_MODEL、SERPER_API_KEY 等）；数据目录默认 `./user_data`。
- 依赖以 `src/requirements.txt` 为准（FastAPI/uvicorn/sse-starlette/aiohttp/transformers/torch 等）。根 `requirements.txt` 混杂旧 Flask/Quart 依赖，建议以 `src/requirements.txt` 为安装基准。

### 已发现的问题与改进建议（高优先级）
- 配置重复与不一致
  - `src/config.py` 定义了 `CONFIG` 常量与 `Config` 类双轨；`models/user_profile.py` 依赖 `CONFIG`（以及 `openai.OpenAI`），其余大多依赖 `Config` 类与自研 `LLMInterface`。建议统一到 `Config` 与 `LLMInterface`，移除或最小化 `CONFIG` 的使用，避免 API 基础与模型名分裂。
- Serper 依赖未配置时回退行为
  - `search.google_search` 在无 `SERPER_API_KEY` 时直接返回空结果；前端体验为报告基本无内容。建议在 UI/日志中明确提示或提供本地/开源引擎回退（如 `scholarly` 或简单 Web 抓取），并在 `/process` 开始阶段进行配置检查，提前抛出友好错误。
- `scripts` 与 DB 表名不一致
  - 多处脚本仍使用 `user_searches` 表名，但 DB schema 使用 `search_history`。如 `scripts/clean_database.py`、`scripts/view_database.py`、`scripts/verify_profile.py`。运行会失败。建议脚本统一为 `search_history` 字段结构。
- 旧依赖与重复依赖
  - 根 `requirements.txt` 存在 Flask/Quart 与重复 `Flask`/`flask_cors`，与 FastAPI 栈不一致，易误导。建议统一依赖到 `src/requirements.txt`，根文件置空或仅指向 src。
- LLM 双体系混用
  - `models/user_profile.py` 直接用 `openai.OpenAI`，而主线使用 `LLMInterface(aiohttp)`。建议统一走 `LLMInterface`，避免 session 重复、Key/URL 不一致与 SDK 版本漂移。
- 翻译 API 稳健性
  - `knowledge_flow._traditional_translate` 与 `improved_arxiv_search._traditional_translate` 均使用 Google 免费接口，网络不可达时回退简单映射。建议：在 UI 提示“可能降级翻译”，并在日志里明确回退策略；可选支持本地小模型翻译或可配置代理。
- SSE 兼容性
  - 目前自定义 JSON 包装良好，但若后端 chunk 含有非 UTF-8 或大段 markdown 代码块时，前端 `marked` 渲染异常概率增加。建议在生成端对 chunk 做轻量清洗（控制台已做），或在前端对异常时降级为纯文本显示。
- 性能与成本
  - 文献综述采用“每批并行生成，再合并”策略，较稳健但调用次数较多。建议在 `generate_literature_review_stream` 合并阶段引入缓存，或对“批”-“章”粒度加上最小分片数阈值，减少小批过多带来的调用数量。

### 使用与部署要点
- 运行推荐
  - 首选安装 `src/requirements.txt`：
    - Windows PowerShell: 建议 `python -m venv .venv && .venv\Scripts\Activate.ps1 && pip install -r src/requirements.txt`
  - 初始化：`python scripts/init_system.py`
  - 启动：`cd src && python -m uvicorn app:app --reload --port 5001`
- 必需环境变量
  - `DEEPSEEK_API_KEY` 与 `SERPER_API_KEY` 至少设置，`LLM_API_BASE`、`LLM_MODEL` 默认可用。
  - `.env` 推荐从 `src/template.env` 复制。

### 可以立刻着手的修复（建议顺序）
- 统一脚本中的表名为 `search_history`（避免运行报错）。
- 将 `models/user_profile.py` 的 OpenAI 客户端改为调用 `LLMInterface`；同时其依赖的 `CONFIG` 改为读取 `Config()`。
- 将根 `requirements.txt` 替换为简短说明或引用 `src/requirements.txt`，避免混淆。
- 在 `/process` 入口对关键配置做早期校验（无 SERPER/DEEPSEEK 时，前端弹出可操作提示）。
- 在文献综述合并阶段增加简单缓存或最小批尺寸，降低多次 LLM 调用。

如果你需要，我可以直接提交上述统一与修复的编辑（不改功能逻辑），或先从数据库脚本对齐开始。