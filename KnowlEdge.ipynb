{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ef6044-3020-4ec5-b314-07833bdd811f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T03:24:58.060216Z",
     "start_time": "2025-03-31T03:22:55.022344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tool\\cursor\\KnowlEdge\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-28 16:20:22,871 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-04-28 16:20:24,830 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-04-28 16:20:25,927 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-04-28 16:20:26,819 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== KnowlEdge系统启动 =====\n",
      "\n",
      "验证数据库...\n",
      "数据库验证成功：可以正常写入和读取数据\n",
      "数据库包含以下表: users, user_interests, sqlite_sequence, user_searches, user_interactions, user_skills, sqlite_stat1\n",
      "表 users: 5 条记录\n",
      "表 user_interests: 16 条记录\n",
      "表 sqlite_sequence: 4 条记录\n",
      "表 user_searches: 8 条记录\n",
      "表 user_interactions: 0 条记录\n",
      "表 user_skills: 168 条记录\n",
      "表 sqlite_stat1: 4 条记录\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 16:20:28,092 - INFO - 已加载兴趣分类体系，共 6 个类别\n",
      "2025-04-28 16:20:28,094 - INFO - 用户画像管理器初始化完成\n",
      "2025-04-28 16:20:28,095 - INFO - 开始收集用户输入信息...\n",
      "2025-04-28 16:20:28,096 - DEBUG - 用户输入信息: {'user_name': 'Tssword4', 'occupation': '算法工程师', 'day': 7, 'platform': '学术期刊', 'content_type': '自然语言处理', 'email': '1145144@qq.com'}\n",
      "2025-04-28 16:20:28,100 - INFO - 用户已存在: 3146f6521574cb418bed3635d6627cb0\n",
      "2025-04-28 16:20:28,101 - INFO - 计算时间范围...\n",
      "2025-04-28 16:20:28,102 - DEBUG - 时间范围计算结果: 起始日期=2025-04-21 08:20:28.102991+00:00, 结束日期=2025-04-28 08:20:28.102991+00:00\n",
      "2025-04-28 16:20:28,104 - INFO - 初始化ResumeReader，支持的格式：['.txt', '.pdf', '.docx', '.doc', '.xlsx', '.xls', '.jpg', '.jpeg', '.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeFlow引擎已初始化\n",
      "\n",
      "步骤 1/6: 收集用户信息\n",
      "\n",
      "===== 欢迎使用KnowlEdge系统 =====\n",
      "请提供以下信息，以便我们为您提供个性化的行业知识更新\n",
      "\n",
      "您的信息已收集完毕，系统将基于这些信息为您提供个性化服务\n",
      "用户信息已收集并处理\n",
      "\n",
      "步骤 2/6: 用户画像分析\n",
      "请提供您的简历以进行更精确的用户画像分析（可选）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 16:22:25,854 - INFO - 成功读取.pdf格式简历文件\n",
      "2025-04-28 16:22:25,857 - INFO - 执行任务：analyze_resume，分析简历内容...\n",
      "2025-04-28 16:22:25,868 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专业的简历分析助手，擅长提取简历中的技能信息并进行分类和评估。请只返回JSON格式的结果，不要添加任何其他标记。'}, {'role': 'user', 'content': '\\n        请从以下简历文本中提取最重要的8项技能，并为每个技能提供以下信息：\\n        1. 技能名称\\n        2. 熟练程度（初级/中级/高级/专家）\\n        3. 技能类别（技术技能、软技能、语言技能、管理技能等）\\n        \\n        请按照技能的重要性和熟练程度排序，最重要和最熟练的技能排在前面。\\n        \\n        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\\n        [\\n            {\"skill\": \"技能名称\", \"level\": \"熟练程度\", \"category\": \"技能类别\"},\\n            ...\\n        ]\\n        \\n        简历文本：\\n        所处院校：浙江财经⼤学 邮箱：1259579739@qq.com出⽣年⽉：2004年8⽉ ⼿机号码：15990682004丁⼦健\\n教育背景\\n2022.09 ~ ⾄今 ⼈⼯智能专业\\nGPA：4.17（专业前10%）\\n主修课程：程序设计 95、⾯向对象程序设计 96、桌⾯应⽤开发 95、前端开发技术 97、机器学习 92、\\n⾃然语⾔处理90、⼤数据开发技术 90、算法设计 91、操作系统 89、数学建模 94、数据库 89Educational background\\n竞赛获奖\\n2024浙江财经⼤学“年度⼤学⽣” (10/22000)   连续俩年专业综测前⼆  浙江省政府奖学⾦ \\n2024全国财经⾼校创新创业⼤赛国家级⾦牌、⼤学⽣创新创业省级银牌、电⼦商务⼤赛省级⾦牌\\n（在项⽬中担任技术部分负责⼈，开发平台的前后端以及整个项⽬框架的构架）\\n2024年美国数学建模⼤赛H奖(国际⼆等奖)、华数杯数学建模国家⼆等奖、⻓三⻆数学建模国家⼆\\n等奖  （在三个⽐赛中均担任队⻓，负责建模及代码实现）\\n2024中国⾼校计算机⼤赛国家三等奖、蓝桥杯C++算法组省⼆、浙江财经⼤学程序设计⼤赛校⼀\\n此外，曾获服务外包省⼆、蓝桥杯C++组省三等校级及以上荣誉⼆⼗余项。Awards and HonorsPERSONAL RESUME\\nExpertise 专业技能\\n熟悉⼤模型Agent开发、RAG架构、⼤模型测评集及评估指标建⽴、Chain-of-Thought推理等技术\\n理解 CNN、RNN、Attention、Embedding、Transformer、BERT、GPT 等模型原理和架构\\n熟悉⾃然语⾔处理：多语⾔翻译及准确度评测、query解析与改写、语义检索、跨模态理解等\\n熟悉C++、python及常⽤的数据结构，熟悉acm常⽤算法，如贪⼼，图论，搜索，动态规划等\\n熟悉数学建模，包括SVM、XGBoost、Bp神经⽹络、LSTM、随机森林、K-Means等算法\\n具备Python爬⾍、数据可视化、机器学习、⾃然语⾔处理、计算机视觉等相关项⽬经验\\n校园经历\\n2023.09 ~ 2024.06 院体育部部⻓\\n2024.04 ~ ⾄今Campus experience项⽬经历 Project Experience政治⾯貌：预备党员 所处年级：⼤三\\n2024.1-2024.7                                 基于SVM算法与⽣成式模型的⼤理⽯⾃动分类与设计图⽣成系统\\n⼤理⽯⼊库时扫描⼤理⽯获取理化特征，通过SVM算法⾃动分类并将数据与结果传输收集后利⽤LLM\\n⽣成专业提⽰词，最终使⽤Stable Diffusion模型⽣成设计图，⾃动化完成⼤理⽯设计与分类流程\\n校AIGC协会发起⼈\\n实习经历 Internship Experience\\n组织了⼗余次⼤型体育赛事，熟悉团队合作、部⻔对接与管理\\n共创杭州⾼校⾸部AIMV，获得杭州⽹、新浪⽹等多家媒体报道2025.2-⾄今                                百度在线⽹络技术有限公司                               AI算法/⼤语⾔模型实习⽣ \\n设计并实现了⼀个智能化的⾏业知识更新推送系统，为⽤⼾提供个性化的专业信息服务。核⼼功能：实现\\n多格式简历解析器以及⽤⼾画像构建、使⽤mBERT进⾏翻译校验、利⽤多引擎翻译系统提升准确性、开\\n发异步搜索引擎(Serper、ArXiv)整合框架、使⽤LLM对检索内容进⾏筛选与总结、最新资讯邮箱推送、建\\n⽴专业评测集校验结果质量。项⽬链接：https://github.com/Tssword21/KnowlEdge/tree/master\\n2024.9-2024.12                              基于物联⽹与云平台的智慧⼩区管理系统 \\n采⽤STM32、Jetson Nano、树莓派⽹关与阿⾥云平台，利⽤MQTT协议进⾏数据传输，集成传感\\n器监控、边缘计算与远程控制功能，实现⼈脸识别，⻋牌识别，以及智能路灯、⻔禁与⻋道控制。\\n\\n    '}], 'model': 'deepseek-chat'}}\n",
      "2025-04-28 16:22:25,872 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-04-28 16:22:25,875 - DEBUG - connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=5.0 socket_options=None\n",
      "2025-04-28 16:22:25,877 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001DE616E3A40>\n",
      "2025-04-28 16:22:25,879 - DEBUG - send_request_headers.started request=<Request [b'CONNECT']>\n",
      "2025-04-28 16:22:25,881 - DEBUG - send_request_headers.complete\n",
      "2025-04-28 16:22:25,882 - DEBUG - send_request_body.started request=<Request [b'CONNECT']>\n",
      "2025-04-28 16:22:25,883 - DEBUG - send_request_body.complete\n",
      "2025-04-28 16:22:25,886 - DEBUG - receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "2025-04-28 16:22:25,889 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "2025-04-28 16:22:25,890 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001DE60B6CA50> server_hostname='api.deepseek.com' timeout=5.0\n",
      "2025-04-28 16:22:26,042 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001DE6487BA40>\n",
      "2025-04-28 16:22:26,043 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已收到简历，开始分析...\n",
      "\n",
      "===== 开始执行用户画像分析任务：analyze_resume =====\n",
      "用户ID: 3146f6521574cb418bed3635d6627cb0\n",
      "\n",
      "第1步：提取用户技能\n",
      "\n",
      "开始从简历中提取最重要的8项技能...\n",
      "正在分析简历中的技能...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 16:22:26,046 - DEBUG - send_request_headers.complete\n",
      "2025-04-28 16:22:26,048 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:26,050 - DEBUG - send_request_body.complete\n",
      "2025-04-28 16:22:26,051 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:26,244 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 28 Apr 2025 08:22:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Set-Cookie', b'HWWAFSESID=cbc2c1638ec6c75efda; path=/'), (b'Set-Cookie', b'HWWAFSESTIME=1745828544756; path=/'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'11884f252f6190b8247e6f3c83146a9d'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-04-28 16:22:26,246 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-28 16:22:26,249 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:38,239 - DEBUG - receive_response_body.complete\n",
      "2025-04-28 16:22:38,240 - DEBUG - response_closed.started\n",
      "2025-04-28 16:22:38,241 - DEBUG - response_closed.complete\n",
      "2025-04-28 16:22:38,242 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers([('date', 'Mon, 28 Apr 2025 08:22:26 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('set-cookie', 'HWWAFSESID=cbc2c1638ec6c75efda; path=/'), ('set-cookie', 'HWWAFSESTIME=1745828544756; path=/'), ('vary', 'origin, access-control-request-method, access-control-request-headers'), ('access-control-allow-credentials', 'true'), ('x-ds-trace-id', '11884f252f6190b8247e6f3c83146a9d'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('x-content-type-options', 'nosniff'), ('content-encoding', 'gzip'), ('server', 'elb')])\n",
      "2025-04-28 16:22:38,244 - DEBUG - request_id: None\n",
      "2025-04-28 16:22:38,253 - DEBUG - 清理后的技能JSON文本: [\n",
      "    {\"skill\": \"Python\", \"level\": \"高级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"C++\", \"level\": \"高级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"机器学习\", \"level\": \"高级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"自然语言处理\", \"level\": \"高级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"大模型开发\", \"level\": \"中级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"算法设计\", \"level\": \"高级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"数学建模\", \"level\": \"高级\", \"category\": \"技术技能\"},\n",
      "    {\"skill\": \"团队合作\", \"level\": \"中级\", \"category\": \"软技能\"}\n",
      "]\n",
      "2025-04-28 16:22:38,264 - INFO - 从简历中提取了 8 项技能\n",
      "2025-04-28 16:22:38,265 - ERROR - 分析用户画像时出错: 'NoneType' object has no attribute 'items'\n",
      "2025-04-28 16:22:38,273 - INFO - 构建搜索查询并进行翻译...\n",
      "2025-04-28 16:22:38,277 - DEBUG - Starting new HTTPS connection (1): translate.google.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取 8 项技能\n",
      "保存技能 1/8: Python\n",
      "保存技能 2/8: C++\n",
      "保存技能 3/8: 机器学习\n",
      "保存技能 4/8: 自然语言处理\n",
      "保存技能 5/8: 大模型开发\n",
      "保存技能 6/8: 算法设计\n",
      "保存技能 7/8: 数学建模\n",
      "保存技能 8/8: 团队合作\n",
      "所有技能已保存到数据库\n",
      "技能提取完成，共 8 项\n",
      "\n",
      "第2步：提取用户兴趣\n",
      "\n",
      "开始从简历中提取最重要的8项兴趣...\n",
      "分析用户画像时出错: 'NoneType' object has no attribute 'items'\n",
      "将继续使用基本用户信息\n",
      "\n",
      "--- 用户画像摘要 ---\n",
      "用户ID: 3146f6521574cb418bed3635d6627cb0\n",
      "用户名: Tssword4\n",
      "职业: 算法工程师\n",
      "技能数量: 16\n",
      "顶级兴趣:\n",
      "  暂无兴趣数据\n",
      "-----------------\n",
      "\n",
      "\n",
      "步骤 3/6: 构建搜索参数\n",
      "正在根据您的需求和兴趣构建搜索参数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 16:22:39,240 - DEBUG - https://translate.google.com:443 \"GET /m?tl=en&sl=auto&q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86 HTTP/1.1\" 200 None\n",
      "2025-04-28 16:22:39,251 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是专业翻译验证助手。回答只用翻译后的内容本身！以下是不同引擎翻译的结果，请你思考它们从中文到英文翻译的准确性，并提供最准确的翻译。最终结果只用翻译后的内容本身。'}, {'role': 'user', 'content': '原文：自然语言处理\\n谷歌翻译：Natural Language Processing'}], 'model': 'deepseek-chat'}}\n",
      "2025-04-28 16:22:39,254 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-04-28 16:22:39,256 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:39,258 - DEBUG - send_request_headers.complete\n",
      "2025-04-28 16:22:39,259 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:39,262 - DEBUG - send_request_body.complete\n",
      "2025-04-28 16:22:39,263 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:39,373 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 28 Apr 2025 08:22:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'b2f0c97bd2440f05b46cfaea77522921'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-04-28 16:22:39,375 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-28 16:22:39,376 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:44,863 - DEBUG - receive_response_body.complete\n",
      "2025-04-28 16:22:44,864 - DEBUG - response_closed.started\n",
      "2025-04-28 16:22:44,864 - DEBUG - response_closed.complete\n",
      "2025-04-28 16:22:44,865 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers({'date': 'Mon, 28 Apr 2025 08:22:39 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-allow-credentials': 'true', 'x-ds-trace-id': 'b2f0c97bd2440f05b46cfaea77522921', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'content-encoding': 'gzip', 'server': 'elb'})\n",
      "2025-04-28 16:22:44,866 - DEBUG - request_id: None\n",
      "2025-04-28 16:22:44,874 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专业的主题提取助手，擅长从文本中识别核心主题。'}, {'role': 'user', 'content': '\\n        请从以下搜索查询中提取最多3个主要的兴趣领域或关键主题。\\n        只返回提取的主题列表，格式为JSON数组，例如：[\"人工智能\", \"机器学习\", \"自然语言处理\"]\\n\\n        搜索查询: 自然语言处理\\n        '}], 'model': 'deepseek-chat'}}\n",
      "2025-04-28 16:22:44,876 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-04-28 16:22:44,880 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:44,881 - DEBUG - send_request_headers.complete\n",
      "2025-04-28 16:22:44,883 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:44,885 - DEBUG - send_request_body.complete\n",
      "2025-04-28 16:22:44,885 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:44,963 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 28 Apr 2025 08:22:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'cd73e9d938f3cb3bb1b80cdb990b1f23'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-04-28 16:22:44,964 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-28 16:22:44,965 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:22:48,833 - DEBUG - receive_response_body.complete\n",
      "2025-04-28 16:22:48,834 - DEBUG - response_closed.started\n",
      "2025-04-28 16:22:48,834 - DEBUG - response_closed.complete\n",
      "2025-04-28 16:22:48,835 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers({'date': 'Mon, 28 Apr 2025 08:22:44 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-allow-credentials': 'true', 'x-ds-trace-id': 'cd73e9d938f3cb3bb1b80cdb990b1f23', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'content-encoding': 'gzip', 'server': 'elb'})\n",
      "2025-04-28 16:22:48,837 - DEBUG - request_id: None\n",
      "2025-04-28 16:22:48,842 - ERROR - 提取查询主题时出错: 'NoneType' object has no attribute 'items'\n",
      "2025-04-28 16:22:49,391 - INFO - 谷歌翻译为：Natural Language Processing，相似度: 0.7835429310798645\n",
      "2025-04-28 16:22:49,465 - INFO - 大模型翻译为：Natural Language Processing，相似度: 0.7835429310798645\n",
      "2025-04-28 16:22:49,466 - INFO - 最终翻译结果:Natural Language Processing ，相似度是：0.7835429310798645\n",
      "2025-04-28 16:22:49,467 - DEBUG - 构建的搜索查询：query_google=Natural Language Processing after:2025-04-21 before:2025-04-28 site:google.com\n",
      "2025-04-28 16:22:49,473 - INFO - 记录用户搜索: 3146f6521574cb418bed3635d6627cb0, 查询: 自然语言处理\n",
      "2025-04-28 16:22:49,474 - INFO - 执行搜索操作...\n",
      "2025-04-28 16:22:49,475 - INFO - 用户选择学术期刊类，执行Google_ArXiv搜索...\n",
      "2025-04-28 16:22:49,476 - DEBUG - 执行Google_ArXiv搜索: Natural Language Processing after:2025-04-21 before:2025-04-28 arXiv site:arxiv.org\n",
      "2025-04-28 16:22:49,476 - INFO - 执行ArXiv搜索，限制结果数量为7...\n",
      "2025-04-28 16:22:49,480 - DEBUG - Starting new HTTP connection (1): 127.0.0.1:7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索参数构建完成，将在以下平台搜索: query_google, query_arxiv, query_google_arxiv\n",
      "\n",
      "步骤 4/6: 执行搜索\n",
      "正在搜索与自然语言处理相关的最新信息...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 16:22:50,404 - DEBUG - http://127.0.0.1:7890 \"GET http://export.arxiv.org/api/query?search_query=Natural%20Language%20Processing%20after:2025-04-21%20before:2025-04-28%20arXiv%20site:arxiv.org&start=0&max_results=7 HTTP/1.1\" 200 4568\n",
      "2025-04-28 16:22:52,676 - DEBUG - ArXiv搜索返回结果数量: 7\n",
      "2025-04-28 16:22:52,676 - INFO - 用户选择学术期刊类，执行ArXiv搜索...\n",
      "2025-04-28 16:22:52,678 - DEBUG - 执行ArXiv搜索: \"Natural Language Processing\" AND submittedDate:[20250421 TO 20250428]\n",
      "2025-04-28 16:22:52,679 - INFO - 执行ArXiv搜索，限制结果数量为7...\n",
      "2025-04-28 16:22:52,680 - DEBUG - Starting new HTTP connection (1): 127.0.0.1:7890\n",
      "2025-04-28 16:22:59,982 - DEBUG - http://127.0.0.1:7890 \"GET http://export.arxiv.org/api/query?search_query=%22Natural%20Language%20Processing%22%20AND%20submittedDate:%5B20250421%20TO%2020250428%5D&start=0&max_results=7 HTTP/1.1\" 502 0\n",
      "2025-04-28 16:22:59,983 - ERROR - ArXiv搜索失败，状态码: 502\n",
      "2025-04-28 16:22:59,986 - DEBUG - 搜索结果: {'google_arxiv': {'results': [{'title': 'Specifying and Verbalising Answer Set Programs in Controlled Natural\\n  Language', 'snippet': '  We show how a bi-directional grammar can be used to specify and verbalise\\nanswer set programs in controlled natural language. We start from a program\\nspecification in controlled natural language and translate this specification\\nautomatically into an executable answer set program. The resulting answer set\\nprogram can be modified following certain naming conventions and the revised\\nversion of the program can then be verbalised in the same subset of natural\\nlanguage that was used as specification language. The bi-directional grammar is\\nparametrised for processing and generation, deals with referring expressions,\\nand exploits symmetries in the data structure of the grammar rules whenever\\nthese grammar rules need to be duplicated. We demonstrate that verbalisation\\nrequires sentence planning in order to aggregate similar structures with the\\naim to improve the readability of the generated specification. Without\\nmodifications, the generated specification is always semantically equivalent to\\nthe original one; our bi-directional grammar is the first one that allows for\\nsemantic round-tripping in the context of controlled natural language\\nprocessing. This paper is under consideration for acceptance in TPLP.\\n', 'link': 'http://arxiv.org/abs/1804.10765v1', 'similarity': np.float32(0.773938)}, {'title': 'Classical Proofs as Parallel Programs', 'snippet': '  We introduce a first proofs-as-parallel-programs correspondence for classical\\nlogic. We define a parallel and more powerful extension of the simply typed\\nlambda calculus corresponding to an analytic natural deduction based on the\\nexcluded middle law. The resulting functional language features a natural\\nhigher-order communication mechanism between processes, which also supports\\nbroadcasting. The normalization procedure makes use of reductions that\\nimplement novel techniques for handling and transmitting process closures.\\n', 'link': 'http://arxiv.org/abs/1809.03094v1', 'similarity': np.float32(0.770141)}, {'title': 'Language and Intelligence, Artificial vs. Natural or What Can and What\\n  Cannot AI Do with NL?', 'snippet': \"  In this talk, I argue that there are certain pragmatic features of natural\\nlanguage (that I will call 'productivity' and 'malleability', on top of\\nsyntactical generativity and semantical compositionality), which are not only\\nhard, but even impossible to capture in an artificial language used by an AI\\nsystem, and the reason for this is to be found in certain deep, metaphysical\\ndifferences between artificial and natural intelligence, accounting for the\\ndifferences in their respective processes of concept-formation.\\n\", 'link': 'http://arxiv.org/abs/2209.12829v1', 'similarity': np.float32(0.7457623)}, {'title': 'Spark NLP: Natural Language Understanding at Scale', 'snippet': '  Spark NLP is a Natural Language Processing (NLP) library built on top of\\nApache Spark ML. It provides simple, performant and accurate NLP annotations\\nfor machine learning pipelines that can scale easily in a distributed\\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\\nhealthcare organizations as the worlds most widely used NLP library in the\\nenterprise.\\n', 'link': 'http://arxiv.org/abs/2101.10848v1', 'similarity': np.float32(0.7310276)}, {'title': 'Machine Translation: A Literature Review', 'snippet': '  Machine translation (MT) plays an important role in benefiting linguists,\\nsociologists, computer scientists, etc. by processing natural language to\\ntranslate it into some other natural language. And this demand has grown\\nexponentially over past couple of years, considering the enormous exchange of\\ninformation between different regions with different regional languages.\\nMachine Translation poses numerous challenges, some of which are: a) Not all\\nwords in one language has equivalent word in another language b) Two given\\nlanguages may have completely different structures c) Words can have more than\\none meaning. Owing to these challenges, along with many others, MT has been\\nactive area of research for more than five decades. Numerous methods have been\\nproposed in the past which either aim at improving the quality of the\\ntranslations generated by them, or study the robustness of these systems by\\nmeasuring their performance on many different languages. In this literature\\nreview, we discuss statistical approaches (in particular word-based and\\nphrase-based) and neural approaches which have gained widespread prominence\\nowing to their state-of-the-art results across multiple major languages.\\n', 'link': 'http://arxiv.org/abs/1901.01122v1', 'similarity': np.float32(0.6986025)}, {'title': 'Information Flow in Pregroup Models of Natural Language', 'snippet': \"  This paper is about pregroup models of natural languages, and how they relate\\nto the explicitly categorical use of pregroups in Compositional Distributional\\nSemantics and Natural Language Processing. These categorical interpretations\\nmake certain assumptions about the nature of natural languages that, when\\nstated formally, may be seen to impose strong restrictions on pregroup grammars\\nfor natural languages.\\n  We formalize this as a hypothesis about the form that pregroup models of\\nnatural languages must take, and demonstrate by an artificial language example\\nthat these restrictions are not imposed by the pregroup axioms themselves. We\\ncompare and contrast the artificial language examples with natural languages\\n(using Welsh, a language where the 'noun' type cannot be taken as primitive, as\\nan illustrative example).\\n  The hypothesis is simply that there must exist a causal connection, or\\ninformation flow, between the words of a sentence in a language whose purpose\\nis to communicate information. This is not necessarily the case with formal\\nlanguages that are simply generated by a series of 'meaning-free' rules. This\\nimposes restrictions on the types of pregroup grammars that we expect to find\\nin natural languages; we formalize this in algebraic, categorical, and\\ngraphical terms.\\n  We take some preliminary steps in providing conditions that ensure pregroup\\nmodels satisfy these co\", 'link': 'http://arxiv.org/abs/1811.03273v1', 'similarity': np.float32(0.578347)}, {'title': 'Functorial Language Games for Question Answering', 'snippet': \"  We present some categorical investigations into Wittgenstein's\\nlanguage-games, with applications to game-theoretic pragmatics and\\nquestion-answering in natural language processing.\\n\", 'link': 'http://arxiv.org/abs/2005.09439v2', 'similarity': np.float32(0.54705125)}]}, 'arxiv': {'error': 'ArXiv搜索失败'}}\n",
      "2025-04-28 16:22:59,987 - INFO - 生成最终的搜索报告...\n",
      "2025-04-28 16:22:59,988 - INFO - 调用大语言模型进行整合搜索结果...\n",
      "2025-04-28 16:22:59,995 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个内容整理助手，负责根据用户提供的信息整合搜索结果并生成报告。使用中文。不要md格式。将报告中的内容，以：\"来源：...（Google/arXiv/Google_arXiv等平台） \\n 标题：... \\n 摘要：... \\n 原文网址：...（只使用搜索结果中提供的真实链接）\\n BERT嵌入的余弦相似度:... \"的形式呈现出来。如果搜索结果中没有提供原文网址，则写\\'原文网址：未提供\\'。不要编造或猜测网址。报告使用用户交谈时的语言，如果原文不是，则准确的转化为用户使用的语言。目前的用户使用的是中文，将结果也转化为中文！如果无法完成就直接翻译用snippet的内容回答将\"摘要：\"改为\"片段：\"。并且回答严格按照规范来，就算无法完成任务也不要说别的不符合规范的话。'}, {'role': 'user', 'content': '请整合以下搜索结果并生成最终报告：{\"google_arxiv\": {\"results\": [{\"title\": \"Specifying and Verbalising Answer Set Programs in Controlled Natural\\\\n  Language\", \"snippet\": \"  We show how a bi-directional grammar can be used to specify and verbalise\\\\nanswer set programs in controlled natural language. We start from a program\\\\nspecification in controlled natural language and translate this specification\\\\nautomatically into an executable answer set program. The resulting answer set\\\\nprogram can be modified following certain naming conventions and the revised\\\\nversion of the program can then be verbalised in the same subset of natural\\\\nlanguage that was used as specification language. The bi-directional grammar is\\\\nparametrised for processing and generation, deals with referring expressions,\\\\nand exploits symmetries in the data structure of the grammar rules whenever\\\\nthese grammar rules need to be duplicated. We demonstrate that verbalisation\\\\nrequires sentence planning in order to aggregate similar structures with the\\\\naim to improve the readability of the generated specification. Without\\\\nmodifications, the generated specification is always semantically equivalent to\\\\nthe original one; our bi-directional grammar is the first one that allows for\\\\nsemantic round-tripping in the context of controlled natural language\\\\nprocessing. This paper is under consideration for acceptance in TPLP.\\\\n\", \"link\": \"http://arxiv.org/abs/1804.10765v1\", \"similarity\": 0.773938000202179}, {\"title\": \"Classical Proofs as Parallel Programs\", \"snippet\": \"  We introduce a first proofs-as-parallel-programs correspondence for classical\\\\nlogic. We define a parallel and more powerful extension of the simply typed\\\\nlambda calculus corresponding to an analytic natural deduction based on the\\\\nexcluded middle law. The resulting functional language features a natural\\\\nhigher-order communication mechanism between processes, which also supports\\\\nbroadcasting. The normalization procedure makes use of reductions that\\\\nimplement novel techniques for handling and transmitting process closures.\\\\n\", \"link\": \"http://arxiv.org/abs/1809.03094v1\", \"similarity\": 0.7701410055160522}, {\"title\": \"Language and Intelligence, Artificial vs. Natural or What Can and What\\\\n  Cannot AI Do with NL?\", \"snippet\": \"  In this talk, I argue that there are certain pragmatic features of natural\\\\nlanguage (that I will call \\'productivity\\' and \\'malleability\\', on top of\\\\nsyntactical generativity and semantical compositionality), which are not only\\\\nhard, but even impossible to capture in an artificial language used by an AI\\\\nsystem, and the reason for this is to be found in certain deep, metaphysical\\\\ndifferences between artificial and natural intelligence, accounting for the\\\\ndifferences in their respective processes of concept-formation.\\\\n\", \"link\": \"http://arxiv.org/abs/2209.12829v1\", \"similarity\": 0.745762288570404}, {\"title\": \"Spark NLP: Natural Language Understanding at Scale\", \"snippet\": \"  Spark NLP is a Natural Language Processing (NLP) library built on top of\\\\nApache Spark ML. It provides simple, performant and accurate NLP annotations\\\\nfor machine learning pipelines that can scale easily in a distributed\\\\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\\\\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\\\\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\\\\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\\\\nhealthcare organizations as the worlds most widely used NLP library in the\\\\nenterprise.\\\\n\", \"link\": \"http://arxiv.org/abs/2101.10848v1\", \"similarity\": 0.7310276031494141}, {\"title\": \"Machine Translation: A Literature Review\", \"snippet\": \"  Machine translation (MT) plays an important role in benefiting linguists,\\\\nsociologists, computer scientists, etc. by processing natural language to\\\\ntranslate it into some other natural language. And this demand has grown\\\\nexponentially over past couple of years, considering the enormous exchange of\\\\ninformation between different regions with different regional languages.\\\\nMachine Translation poses numerous challenges, some of which are: a) Not all\\\\nwords in one language has equivalent word in another language b) Two given\\\\nlanguages may have completely different structures c) Words can have more than\\\\none meaning. Owing to these challenges, along with many others, MT has been\\\\nactive area of research for more than five decades. Numerous methods have been\\\\nproposed in the past which either aim at improving the quality of the\\\\ntranslations generated by them, or study the robustness of these systems by\\\\nmeasuring their performance on many different languages. In this literature\\\\nreview, we discuss statistical approaches (in particular word-based and\\\\nphrase-based) and neural approaches which have gained widespread prominence\\\\nowing to their state-of-the-art results across multiple major languages.\\\\n\", \"link\": \"http://arxiv.org/abs/1901.01122v1\", \"similarity\": 0.6986024975776672}, {\"title\": \"Information Flow in Pregroup Models of Natural Language\", \"snippet\": \"  This paper is about pregroup models of natural languages, and how they relate\\\\nto the explicitly categorical use of pregroups in Compositional Distributional\\\\nSemantics and Natural Language Processing. These categorical interpretations\\\\nmake certain assumptions about the nature of natural languages that, when\\\\nstated formally, may be seen to impose strong restrictions on pregroup grammars\\\\nfor natural languages.\\\\n  We formalize this as a hypothesis about the form that pregroup models of\\\\nnatural languages must take, and demonstrate by an artificial language example\\\\nthat these restrictions are not imposed by the pregroup axioms themselves. We\\\\ncompare and contrast the artificial language examples with natural languages\\\\n(using Welsh, a language where the \\'noun\\' type cannot be taken as primitive, as\\\\nan illustrative example).\\\\n  The hypothesis is simply that there must exist a causal connection, or\\\\ninformation flow, between the words of a sentence in a language whose purpose\\\\nis to communicate information. This is not necessarily the case with formal\\\\nlanguages that are simply generated by a series of \\'meaning-free\\' rules. This\\\\nimposes restrictions on the types of pregroup grammars that we expect to find\\\\nin natural languages; we formalize this in algebraic, categorical, and\\\\ngraphical terms.\\\\n  We take some preliminary steps in providing conditions that ensure pregroup\\\\nmodels satisfy these co\", \"link\": \"http://arxiv.org/abs/1811.03273v1\", \"similarity\": 0.5783470273017883}, {\"title\": \"Functorial Language Games for Question Answering\", \"snippet\": \"  We present some categorical investigations into Wittgenstein\\'s\\\\nlanguage-games, with applications to game-theoretic pragmatics and\\\\nquestion-answering in natural language processing.\\\\n\", \"link\": \"http://arxiv.org/abs/2005.09439v2\", \"similarity\": 0.5470512509346008}]}, \"arxiv\": {\"error\": \"ArXiv搜索失败\"}}'}], 'model': 'deepseek-chat', 'stream': False}}\n",
      "2025-04-28 16:22:59,999 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-04-28 16:23:00,002 - DEBUG - connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=5.0 socket_options=None\n",
      "2025-04-28 16:23:00,003 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001DE6172D2B0>\n",
      "2025-04-28 16:23:00,004 - DEBUG - send_request_headers.started request=<Request [b'CONNECT']>\n",
      "2025-04-28 16:23:00,006 - DEBUG - send_request_headers.complete\n",
      "2025-04-28 16:23:00,007 - DEBUG - send_request_body.started request=<Request [b'CONNECT']>\n",
      "2025-04-28 16:23:00,008 - DEBUG - send_request_body.complete\n",
      "2025-04-28 16:23:00,009 - DEBUG - receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "2025-04-28 16:23:00,010 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "2025-04-28 16:23:00,012 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001DE60B6CA50> server_hostname='api.deepseek.com' timeout=5.0\n",
      "2025-04-28 16:23:00,157 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001DE6116C9E0>\n",
      "2025-04-28 16:23:00,158 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-04-28 16:23:00,160 - DEBUG - send_request_headers.complete\n",
      "2025-04-28 16:23:00,161 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:23:00,163 - DEBUG - send_request_body.complete\n",
      "2025-04-28 16:23:00,165 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索完成，共找到 7 条相关信息\n",
      "\n",
      "步骤 5/6: 生成报告\n",
      "正在整合搜索结果并生成报告...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 16:23:00,264 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 28 Apr 2025 08:23:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'2f4aac9993eb2c2528e991baa4324238'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-04-28 16:23:00,266 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-28 16:23:00,267 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-04-28 16:24:02,202 - DEBUG - receive_response_body.complete\n",
      "2025-04-28 16:24:02,203 - DEBUG - response_closed.started\n",
      "2025-04-28 16:24:02,203 - DEBUG - response_closed.complete\n",
      "2025-04-28 16:24:02,204 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers({'date': 'Mon, 28 Apr 2025 08:23:00 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-allow-credentials': 'true', 'x-ds-trace-id': '2f4aac9993eb2c2528e991baa4324238', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'content-encoding': 'gzip', 'server': 'elb'})\n",
      "2025-04-28 16:24:02,206 - DEBUG - request_id: None\n",
      "2025-04-28 16:24:02,208 - DEBUG - 大语言模型整合结果: 来源：Google_arXiv  \n",
      "标题：Specifying and Verbalising Answer Set Programs in Controlled Natural Language  \n",
      "摘要：我们展示了一种双向语法如何用于在受控自然语言中指定和表达答案集程序。我们从受控自然语言中的程序规范开始，自动将此规范翻译为可执行的答案集程序。生成的答案集程序可以按照特定的命名约定进行修改，然后可以在用于规范语言的同一自然语言子集中表达修订后的程序版本。双向语法在处理和生成时进行了参数化，处理指代表达式，并在需要复制语法规则时利用语法规则数据结构中的对称性。我们证明表达需要句子规划，以聚合类似结构，旨在提高生成规范的可读性。未经修改时，生成的规范始终与原始规范语义等价；我们的双向语法是第一个允许在受控自然语言处理中进行语义往返的语法。  \n",
      "原文网址：http://arxiv.org/abs/1804.10765v1  \n",
      "BERT嵌入的余弦相似度:0.773938000202179  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Classical Proofs as Parallel Programs  \n",
      "摘要：我们为经典逻辑引入了第一个证明作为并行程序的对应关系。我们定义了一个并行且更强大的简单类型lambda演算扩展，对应于基于排中律的分析自然演绎。生成的功能语言具有一种自然的高阶进程间通信机制，也支持广播。规范化过程使用了实现处理与传输进程闭包的新技术的归约。  \n",
      "原文网址：http://arxiv.org/abs/1809.03094v1  \n",
      "BERT嵌入的余弦相似度:0.7701410055160522  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Language and Intelligence, Artificial vs. Natural or What Can and What Cannot AI Do with NL?  \n",
      "摘要：在这次演讲中，我认为自然语言具有某些语用特征（我称之为“生产力”和“可塑性”，除了句法生成性和语义组合性之外），这些特征不仅难以捕捉，甚至不可能在AI系统使用的人工语言中捕捉到，原因在于人工与自然智能之间某些深层的形而上学差异，这些差异导致了它们各自概念形成过程的不同。  \n",
      "原文网址：http://arxiv.org/abs/2209.12829v1  \n",
      "BERT嵌入的余弦相似度:0.745762288570404  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Spark NLP: Natural Language Understanding at Scale  \n",
      "摘要：Spark NLP是一个构建在Apache Spark ML之上的自然语言处理（NLP）库。它为机器学习管道提供简单、高性能和准确的NLP注释，可以在分布式环境中轻松扩展。Spark NLP提供了1100多个预训练管道和模型，支持超过192种语言。它支持几乎所有NLP任务和模块，可以在集群中无缝使用。自2020年1月以来下载量超过270万次，增长了九倍，Spark NLP被54%的医疗保健组织使用，成为企业中最广泛使用的NLP库。  \n",
      "原文网址：http://arxiv.org/abs/2101.10848v1  \n",
      "BERT嵌入的余弦相似度:0.7310276031494141  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Machine Translation: A Literature Review  \n",
      "摘要：机器翻译（MT）在处理自然语言以将其翻译成其他自然语言方面发挥着重要作用，使语言学家、社会学家、计算机科学家等受益。过去几年，考虑到不同地区之间以不同区域语言进行的大量信息交换，这一需求呈指数级增长。机器翻译面临诸多挑战，例如：a）并非所有单词在一种语言中都有另一种语言的等价词；b）两种给定语言可能具有完全不同的结构；c）单词可能有多个含义。由于这些挑战以及其他许多挑战，MT在过去五十多年中一直是活跃的研究领域。过去提出了许多方法，旨在提高它们生成的翻译质量，或通过测量它们在许多不同语言上的性能来研究这些系统的稳健性。在这篇文献综述中，我们讨论了统计方法（特别是基于词和基于短语的方法）和神经方法，这些方法由于在多种主要语言上取得了最先进的结果而获得了广泛的关注。  \n",
      "原文网址：http://arxiv.org/abs/1901.01122v1  \n",
      "BERT嵌入的余弦相似度:0.6986024975776672  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Information Flow in Pregroup Models of Natural Language  \n",
      "摘要：本文讨论了自然语言的pregroup模型，以及它们如何与组合分布语义学和自然语言处理中pregroup的显式范畴使用相关联。这些范畴解释对自然语言的性质做出了一些假设，当正式陈述时，可能会被视为对自然语言的pregroup语法施加了严格的限制。我们将其形式化为一个假设，关于自然语言的pregroup模型必须采取的形式，并通过一个人工语言示例证明这些限制并非由pregroup公理本身施加。我们将人工语言示例与自然语言（以威尔士语为例，在这种语言中“名词”类型不能被视为原始类型）进行比较和对比。该假设简单地说，在旨在交流信息的语言中，句子的单词之间必须存在因果关系或信息流。对于仅由一系列“无意义”规则生成的形式语言而言，情况并非如此。这限制了我们在自然语言中期望找到的pregroup语法的类型；我们在代数、范畴和图形术语中形式化这一点。我们初步提供了一些条件，确保pregroup模型满足这些条件。  \n",
      "原文网址：http://arxiv.org/abs/1811.03273v1  \n",
      "BERT嵌入的余弦相似度:0.5783470273017883  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Functorial Language Games for Question Answering  \n",
      "摘要：我们提出了一些对维特根斯坦语言游戏的范畴论研究，应用于自然语言处理中的博弈论语用学和问答。  \n",
      "原文网址：http://arxiv.org/abs/2005.09439v2  \n",
      "BERT嵌入的余弦相似度:0.5470512509346008\n",
      "2025-04-28 16:24:02,209 - DEBUG - 生成的报告内容: ['\\n\\n--- 根据您选择的【学术期刊】平台，近几日的行业内最新进展已整理好，请查收！ ---\\n', '来源：Google_arXiv  \\n标题：Specifying and Verbalising Answer Set Programs in Controlled Natural Language  \\n摘要：我们展示了一种双向语法如何用于在受控自然语言中指定和表达答案集程序。我们从受控自然语言中的程序规范开始，自动将此规范翻译为可执行的答案集程序。生成的答案集程序可以按照特定的命名约定进行修改，然后可以在用于规范语言的同一自然语言子集中表达修订后的程序版本。双向语法在处理和生成时进行了参数化，处理指代表达式，并在需要复制语法规则时利用语法规则数据结构中的对称性。我们证明表达需要句子规划，以聚合类似结构，旨在提高生成规范的可读性。未经修改时，生成的规范始终与原始规范语义等价；我们的双向语法是第一个允许在受控自然语言处理中进行语义往返的语法。  \\n原文网址：http://arxiv.org/abs/1804.10765v1  \\nBERT嵌入的余弦相似度:0.773938000202179  \\n\\n来源：Google_arXiv  \\n标题：Classical Proofs as Parallel Programs  \\n摘要：我们为经典逻辑引入了第一个证明作为并行程序的对应关系。我们定义了一个并行且更强大的简单类型lambda演算扩展，对应于基于排中律的分析自然演绎。生成的功能语言具有一种自然的高阶进程间通信机制，也支持广播。规范化过程使用了实现处理与传输进程闭包的新技术的归约。  \\n原文网址：http://arxiv.org/abs/1809.03094v1  \\nBERT嵌入的余弦相似度:0.7701410055160522  \\n\\n来源：Google_arXiv  \\n标题：Language and Intelligence, Artificial vs. Natural or What Can and What Cannot AI Do with NL?  \\n摘要：在这次演讲中，我认为自然语言具有某些语用特征（我称之为“生产力”和“可塑性”，除了句法生成性和语义组合性之外），这些特征不仅难以捕捉，甚至不可能在AI系统使用的人工语言中捕捉到，原因在于人工与自然智能之间某些深层的形而上学差异，这些差异导致了它们各自概念形成过程的不同。  \\n原文网址：http://arxiv.org/abs/2209.12829v1  \\nBERT嵌入的余弦相似度:0.745762288570404  \\n\\n来源：Google_arXiv  \\n标题：Spark NLP: Natural Language Understanding at Scale  \\n摘要：Spark NLP是一个构建在Apache Spark ML之上的自然语言处理（NLP）库。它为机器学习管道提供简单、高性能和准确的NLP注释，可以在分布式环境中轻松扩展。Spark NLP提供了1100多个预训练管道和模型，支持超过192种语言。它支持几乎所有NLP任务和模块，可以在集群中无缝使用。自2020年1月以来下载量超过270万次，增长了九倍，Spark NLP被54%的医疗保健组织使用，成为企业中最广泛使用的NLP库。  \\n原文网址：http://arxiv.org/abs/2101.10848v1  \\nBERT嵌入的余弦相似度:0.7310276031494141  \\n\\n来源：Google_arXiv  \\n标题：Machine Translation: A Literature Review  \\n摘要：机器翻译（MT）在处理自然语言以将其翻译成其他自然语言方面发挥着重要作用，使语言学家、社会学家、计算机科学家等受益。过去几年，考虑到不同地区之间以不同区域语言进行的大量信息交换，这一需求呈指数级增长。机器翻译面临诸多挑战，例如：a）并非所有单词在一种语言中都有另一种语言的等价词；b）两种给定语言可能具有完全不同的结构；c）单词可能有多个含义。由于这些挑战以及其他许多挑战，MT在过去五十多年中一直是活跃的研究领域。过去提出了许多方法，旨在提高它们生成的翻译质量，或通过测量它们在许多不同语言上的性能来研究这些系统的稳健性。在这篇文献综述中，我们讨论了统计方法（特别是基于词和基于短语的方法）和神经方法，这些方法由于在多种主要语言上取得了最先进的结果而获得了广泛的关注。  \\n原文网址：http://arxiv.org/abs/1901.01122v1  \\nBERT嵌入的余弦相似度:0.6986024975776672  \\n\\n来源：Google_arXiv  \\n标题：Information Flow in Pregroup Models of Natural Language  \\n摘要：本文讨论了自然语言的pregroup模型，以及它们如何与组合分布语义学和自然语言处理中pregroup的显式范畴使用相关联。这些范畴解释对自然语言的性质做出了一些假设，当正式陈述时，可能会被视为对自然语言的pregroup语法施加了严格的限制。我们将其形式化为一个假设，关于自然语言的pregroup模型必须采取的形式，并通过一个人工语言示例证明这些限制并非由pregroup公理本身施加。我们将人工语言示例与自然语言（以威尔士语为例，在这种语言中“名词”类型不能被视为原始类型）进行比较和对比。该假设简单地说，在旨在交流信息的语言中，句子的单词之间必须存在因果关系或信息流。对于仅由一系列“无意义”规则生成的形式语言而言，情况并非如此。这限制了我们在自然语言中期望找到的pregroup语法的类型；我们在代数、范畴和图形术语中形式化这一点。我们初步提供了一些条件，确保pregroup模型满足这些条件。  \\n原文网址：http://arxiv.org/abs/1811.03273v1  \\nBERT嵌入的余弦相似度:0.5783470273017883  \\n\\n来源：Google_arXiv  \\n标题：Functorial Language Games for Question Answering  \\n摘要：我们提出了一些对维特根斯坦语言游戏的范畴论研究，应用于自然语言处理中的博弈论语用学和问答。  \\n原文网址：http://arxiv.org/abs/2005.09439v2  \\nBERT嵌入的余弦相似度:0.5470512509346008']\n",
      "2025-04-28 16:24:02,210 - INFO - 准备发送邮件...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "报告生成完成\n",
      "\n",
      "步骤 6/6: 发送报告\n",
      "已发送邮件到 1145144@qq.com:\n",
      "\n",
      "\n",
      "--- 根据您选择的【学术期刊】平台，近几日的行业内最新进展已整理好，请查收！ ---\n",
      "\n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Specifying and Verbalising Answer Set Programs in Controlled Natural Language  \n",
      "摘要：我们展示了一种双向语法如何用于在受控自然语言中指定和表达答案集程序。我们从受控自然语言中的程序规范开始，自动将此规范翻译为可执行的答案集程序。生成的答案集程序可以按照特定的命名约定进行修改，然后可以在用于规范语言的同一自然语言子集中表达修订后的程序版本。双向语法在处理和生成时进行了参数化，处理指代表达式，并在需要复制语法规则时利用语法规则数据结构中的对称性。我们证明表达需要句子规划，以聚合类似结构，旨在提高生成规范的可读性。未经修改时，生成的规范始终与原始规范语义等价；我们的双向语法是第一个允许在受控自然语言处理中进行语义往返的语法。  \n",
      "原文网址：http://arxiv.org/abs/1804.10765v1  \n",
      "BERT嵌入的余弦相似度:0.773938000202179  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Classical Proofs as Parallel Programs  \n",
      "摘要：我们为经典逻辑引入了第一个证明作为并行程序的对应关系。我们定义了一个并行且更强大的简单类型lambda演算扩展，对应于基于排中律的分析自然演绎。生成的功能语言具有一种自然的高阶进程间通信机制，也支持广播。规范化过程使用了实现处理与传输进程闭包的新技术的归约。  \n",
      "原文网址：http://arxiv.org/abs/1809.03094v1  \n",
      "BERT嵌入的余弦相似度:0.7701410055160522  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Language and Intelligence, Artificial vs. Natural or What Can and What Cannot AI Do with NL?  \n",
      "摘要：在这次演讲中，我认为自然语言具有某些语用特征（我称之为“生产力”和“可塑性”，除了句法生成性和语义组合性之外），这些特征不仅难以捕捉，甚至不可能在AI系统使用的人工语言中捕捉到，原因在于人工与自然智能之间某些深层的形而上学差异，这些差异导致了它们各自概念形成过程的不同。  \n",
      "原文网址：http://arxiv.org/abs/2209.12829v1  \n",
      "BERT嵌入的余弦相似度:0.745762288570404  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Spark NLP: Natural Language Understanding at Scale  \n",
      "摘要：Spark NLP是一个构建在Apache Spark ML之上的自然语言处理（NLP）库。它为机器学习管道提供简单、高性能和准确的NLP注释，可以在分布式环境中轻松扩展。Spark NLP提供了1100多个预训练管道和模型，支持超过192种语言。它支持几乎所有NLP任务和模块，可以在集群中无缝使用。自2020年1月以来下载量超过270万次，增长了九倍，Spark NLP被54%的医疗保健组织使用，成为企业中最广泛使用的NLP库。  \n",
      "原文网址：http://arxiv.org/abs/2101.10848v1  \n",
      "BERT嵌入的余弦相似度:0.7310276031494141  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Machine Translation: A Literature Review  \n",
      "摘要：机器翻译（MT）在处理自然语言以将其翻译成其他自然语言方面发挥着重要作用，使语言学家、社会学家、计算机科学家等受益。过去几年，考虑到不同地区之间以不同区域语言进行的大量信息交换，这一需求呈指数级增长。机器翻译面临诸多挑战，例如：a）并非所有单词在一种语言中都有另一种语言的等价词；b）两种给定语言可能具有完全不同的结构；c）单词可能有多个含义。由于这些挑战以及其他许多挑战，MT在过去五十多年中一直是活跃的研究领域。过去提出了许多方法，旨在提高它们生成的翻译质量，或通过测量它们在许多不同语言上的性能来研究这些系统的稳健性。在这篇文献综述中，我们讨论了统计方法（特别是基于词和基于短语的方法）和神经方法，这些方法由于在多种主要语言上取得了最先进的结果而获得了广泛的关注。  \n",
      "原文网址：http://arxiv.org/abs/1901.01122v1  \n",
      "BERT嵌入的余弦相似度:0.6986024975776672  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Information Flow in Pregroup Models of Natural Language  \n",
      "摘要：本文讨论了自然语言的pregroup模型，以及它们如何与组合分布语义学和自然语言处理中pregroup的显式范畴使用相关联。这些范畴解释对自然语言的性质做出了一些假设，当正式陈述时，可能会被视为对自然语言的pregroup语法施加了严格的限制。我们将其形式化为一个假设，关于自然语言的pregroup模型必须采取的形式，并通过一个人工语言示例证明这些限制并非由pregroup公理本身施加。我们将人工语言示例与自然语言（以威尔士语为例，在这种语言中“名词”类型不能被视为原始类型）进行比较和对比。该假设简单地说，在旨在交流信息的语言中，句子的单词之间必须存在因果关系或信息流。对于仅由一系列“无意义”规则生成的形式语言而言，情况并非如此。这限制了我们在自然语言中期望找到的pregroup语法的类型；我们在代数、范畴和图形术语中形式化这一点。我们初步提供了一些条件，确保pregroup模型满足这些条件。  \n",
      "原文网址：http://arxiv.org/abs/1811.03273v1  \n",
      "BERT嵌入的余弦相似度:0.5783470273017883  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Functorial Language Games for Question Answering  \n",
      "摘要：我们提出了一些对维特根斯坦语言游戏的范畴论研究，应用于自然语言处理中的博弈论语用学和问答。  \n",
      "原文网址：http://arxiv.org/abs/2005.09439v2  \n",
      "BERT嵌入的余弦相似度:0.5470512509346008\n",
      "流程执行完成\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from db_utils import get_db_connection\n",
    "import datetime\n",
    "import logging\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from xml.etree import ElementTree\n",
    "from deep_translator import GoogleTranslator, BaiduTranslator\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import PyPDF2\n",
    "import docx\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# 配置日志\n",
    "# logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 常量配置\n",
    "CONFIG = {\n",
    "    \"API_KEYS\": {\n",
    "        \"deepseek\": os.environ.get(\"DEEPSEEK_API_KEY\", \"sk-58c6314fd9ae47a6a493d0d8499d2271\"),\n",
    "        \"serper\": os.environ.get(\"SERPER_API_KEY\", \"a196a1abc535244a7430523550c88adb422d893e\"),\n",
    "        \"baidu_translate\": os.environ.get(\"BAIDU_API_KEY\", \"\"),\n",
    "    },\n",
    "    \"MODELS\": {\n",
    "        \"BERT\": \"bert-base-multilingual-cased\",\n",
    "        \"LLM\": \"deepseek-chat\",\n",
    "    },\n",
    "    \"DATA_DIR\": \"./user_data\",\n",
    "    \"DB_PATH\": \"./user_data/user_profiles.db\"\n",
    "}\n",
    "\n",
    "# 确保数据目录存在\n",
    "os.makedirs(CONFIG[\"DATA_DIR\"], exist_ok=True)\n",
    "\n",
    "# 初始化BERT模型\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG[\"MODELS\"][\"BERT\"])\n",
    "model = BertModel.from_pretrained(CONFIG[\"MODELS\"][\"BERT\"])\n",
    "\n",
    "# 创建数据库连接\n",
    "def verify_database():\n",
    "    \"\"\"验证数据库是否正确创建和可写入\"\"\"\n",
    "    print(\"\\n验证数据库...\")\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if conn is None:\n",
    "        print(\"无法连接到数据库，请检查路径和权限\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 尝试写入测试数据\n",
    "        test_id = f\"test_{int(time.time())}\"\n",
    "        conn.execute(\n",
    "            \"INSERT INTO users (id, name, occupation, email) VALUES (?, ?, ?, ?)\",\n",
    "            (test_id, \"测试用户\", \"测试职业\", \"test@example.com\")\n",
    "        )\n",
    "        conn.commit()\n",
    "        \n",
    "        # 验证是否写入成功\n",
    "        user = conn.execute(\"SELECT * FROM users WHERE id = ?\", (test_id,)).fetchone()\n",
    "        if user:\n",
    "            print(\"数据库验证成功：可以正常写入和读取数据\")\n",
    "            \n",
    "            # 清理测试数据\n",
    "            conn.execute(\"DELETE FROM users WHERE id = ?\", (test_id,))\n",
    "            conn.commit()\n",
    "            \n",
    "            # 显示数据库信息\n",
    "            tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "            print(f\"数据库包含以下表: {', '.join([t['name'] for t in tables])}\")\n",
    "            \n",
    "            for table in [t['name'] for t in tables]:\n",
    "                count = conn.execute(f\"SELECT COUNT(*) as count FROM {table}\").fetchone()['count']\n",
    "                print(f\"表 {table}: {count} 条记录\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"数据库验证失败：无法读取写入的测试数据\")\n",
    "            return False\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"数据库验证失败: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 创建一个自定义的 NumpyEncoder 类来处理 NumPy 数据类型\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        import numpy as np\n",
    "        if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def get_bert_embeddings(text: str) -> torch.Tensor:\n",
    "    \"\"\" 获取文本的BERT嵌入表示 \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "def compute_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\" 计算两个文本之间的余弦相似度 \"\"\"\n",
    "    embedding1 = get_bert_embeddings(text1)\n",
    "    embedding2 = get_bert_embeddings(text2)\n",
    "    similarity = cosine_similarity(embedding1.numpy(), embedding2.numpy())\n",
    "    return similarity[0][0]\n",
    "\n",
    "class UserProfileManager:\n",
    "    \"\"\"用户画像管理器，负责创建、更新和存储用户画像\"\"\"\n",
    "\n",
    "    def __init__(self, client=None):\n",
    "        \"\"\"初始化用户画像管理器\"\"\"\n",
    "        self.client = client or OpenAI(\n",
    "            api_key=CONFIG[\"API_KEYS\"][\"deepseek\"],\n",
    "            base_url=\"https://api.deepseek.com\"\n",
    "        )\n",
    "        self.interest_categories = self._load_interest_categories()\n",
    "        logging.info(\"用户画像管理器初始化完成\")\n",
    "\n",
    "    def _load_interest_categories(self):\n",
    "        \"\"\"加载预定义的兴趣分类体系\"\"\"\n",
    "        categories_file = os.path.join(CONFIG[\"DATA_DIR\"], \"interest_categories.json\")\n",
    "\n",
    "        if os.path.exists(categories_file):\n",
    "            try:\n",
    "                with open(categories_file, 'r', encoding='utf-8') as f:\n",
    "                    self.interest_categories = json.load(f)\n",
    "                logging.info(f\"已加载兴趣分类体系，共 {len(self.interest_categories)} 个类别\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"加载兴趣分类体系出错: {e}\")\n",
    "                self._create_default_categories()\n",
    "        else:\n",
    "            logging.warning(f\"兴趣分类文件不存在: {categories_file}，将创建默认分类\")\n",
    "            self._create_default_categories()\n",
    "\n",
    "    def _create_default_categories(self):\n",
    "        \"\"\"创建默认的兴趣分类体系\"\"\"\n",
    "        self.interest_categories = {\n",
    "            \"技术\": [\"人工智能\", \"机器学习\", \"深度学习\", \"自然语言处理\", \"计算机视觉\", \"大语言模型\",\n",
    "                   \"大数据\", \"云计算\", \"区块链\", \"物联网\", \"网络安全\", \"数据库\"],\n",
    "            \"科学\": [\"物理学\", \"化学\", \"生物学\", \"天文学\", \"数学\", \"医学\", \"地质学\", \"环境科学\"],\n",
    "            \"商业\": [\"管理\", \"市场营销\", \"金融\", \"创业\", \"投资\", \"电子商务\", \"人力资源\"],\n",
    "            \"艺术\": [\"绘画\", \"音乐\", \"电影\", \"文学\", \"设计\", \"摄影\", \"建筑\"],\n",
    "            \"教育\": [\"教学方法\", \"学习理论\", \"教育技术\", \"高等教育\", \"职业教育\"],\n",
    "            \"健康\": [\"营养\", \"健身\", \"心理健康\", \"医疗技术\", \"公共卫生\"]\n",
    "        }\n",
    "\n",
    "        # 保存到文件\n",
    "        categories_file = os.path.join(CONFIG[\"DATA_DIR\"], \"interest_categories.json\")\n",
    "        try:\n",
    "            with open(categories_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.interest_categories, f, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"已创建默认兴趣分类体系并保存到: {categories_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存默认兴趣分类体系出错: {e}\")\n",
    "\n",
    "    def _generate_user_id(self, user_info: Dict) -> str:\n",
    "        \"\"\"根据用户信息生成唯一ID\"\"\"\n",
    "        key_string = f\"{user_info.get('name', '')}-{user_info.get('email', '')}-{user_info.get('occupation', '')}\"\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "\n",
    "    def create_user(self, user_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        创建新用户并存储基本信息\n",
    "\n",
    "        Args:\n",
    "            user_info: 用户基本信息，包含姓名、职业、邮箱等\n",
    "\n",
    "        Returns:\n",
    "            用户ID\n",
    "        \"\"\"\n",
    "        user_id = self._generate_user_id(user_info)\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 检查用户是否已存在\n",
    "            existing_user = conn.execute(\"SELECT id FROM users WHERE id = ?\", (user_id,)).fetchone()\n",
    "\n",
    "            if not existing_user:\n",
    "                conn.execute(\n",
    "                    \"INSERT INTO users (id, name, occupation, email) VALUES (?, ?, ?, ?)\",\n",
    "                    (user_id, user_info.get(\"name\", \"\"), user_info.get(\"occupation\", \"\"), user_info.get(\"email\", \"\"))\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"创建新用户: {user_id}\")\n",
    "            else:\n",
    "                logging.info(f\"用户已存在: {user_id}\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return user_id\n",
    "\n",
    "    def extract_skills_from_resume(self, user_id: str, resume_text: str, max_skills: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从简历文本中提取技能信息\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            resume_text: 简历文本内容\n",
    "            max_skills: 最多提取的技能数量\n",
    "\n",
    "        Returns:\n",
    "            技能列表，每个技能包含名称、级别和分类\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始从简历中提取最重要的{max_skills}项技能...\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请从以下简历文本中提取最重要的{max_skills}项技能，并为每个技能提供以下信息：\n",
    "        1. 技能名称\n",
    "        2. 熟练程度（初级/中级/高级/专家）\n",
    "        3. 技能类别（技术技能、软技能、语言技能、管理技能等）\n",
    "        \n",
    "        请按照技能的重要性和熟练程度排序，最重要和最熟练的技能排在前面。\n",
    "        \n",
    "        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\n",
    "        [\n",
    "            {{\"skill\": \"技能名称\", \"level\": \"熟练程度\", \"category\": \"技能类别\"}},\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        简历文本：\n",
    "        {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"正在分析简历中的技能...\")\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的简历分析助手，擅长提取简历中的技能信息并进行分类和评估。请只返回JSON格式的结果，不要添加任何其他标记。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            skills_text = response.choices[0].message.content\n",
    "            \n",
    "            # 清理可能的格式标记\n",
    "            skills_text = skills_text.strip()\n",
    "            if skills_text.startswith(\"```json\"):\n",
    "                skills_text = skills_text[7:]\n",
    "            if skills_text.startswith(\"```\"):\n",
    "                skills_text = skills_text[3:]\n",
    "            if skills_text.endswith(\"```\"):\n",
    "                skills_text = skills_text[:-3]\n",
    "            skills_text = skills_text.strip()\n",
    "            \n",
    "            logging.debug(f\"清理后的技能JSON文本: {skills_text}\")\n",
    "    \n",
    "            # 尝试解析JSON\n",
    "            try:\n",
    "                skills = json.loads(skills_text)\n",
    "                print(f\"成功提取 {len(skills)} 项技能\")\n",
    "                \n",
    "                # 确保skills是列表类型\n",
    "                if not isinstance(skills, list):\n",
    "                    logging.error(f\"解析的技能不是列表类型: {type(skills)}\")\n",
    "                    skills = []\n",
    "                    print(\"解析的技能格式不正确，将使用空列表\")\n",
    "                \n",
    "                # 保存到数据库\n",
    "                if skills:  # 只有当skills非空时才尝试保存\n",
    "                    conn = get_db_connection()\n",
    "                    try:\n",
    "                        for i, skill in enumerate(skills):\n",
    "                            if isinstance(skill, dict):  # 确保每个技能是字典类型\n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_skills (user_id, skill, level, category) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, skill.get(\"skill\", \"\"), skill.get(\"level\", \"\"), skill.get(\"category\", \"\"))\n",
    "                                )\n",
    "                                # 显示进度\n",
    "                                print(f\"保存技能 {i+1}/{len(skills)}: {skill.get('skill', '')}\")\n",
    "                        conn.commit()\n",
    "                        print(\"所有技能已保存到数据库\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"保存技能到数据库时出错: {e}\")\n",
    "                        print(f\"保存技能时出错: {e}\")\n",
    "                    finally:\n",
    "                        conn.close()\n",
    "                \n",
    "                return skills if skills else []\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"无法解析技能JSON: {skills_text}, 错误: {e}\")\n",
    "                print(f\"解析技能信息失败: {e}\")\n",
    "                # 尝试手动解析简单格式\n",
    "                if \"[\" in skills_text and \"]\" in skills_text:\n",
    "                    try:\n",
    "                        # 尝试修复常见的JSON格式问题\n",
    "                        fixed_text = skills_text.replace(\"'\", \"\\\"\")\n",
    "                        skills = json.loads(fixed_text)\n",
    "                        print(f\"修复后成功解析，提取了 {len(skills)} 项技能\")\n",
    "                        return skills\n",
    "                    except:\n",
    "                        pass\n",
    "                return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取技能时出错: {e}\")\n",
    "            print(f\"提取技能时出错: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_interests_from_resume(self, user_id: str, resume_text: str, max_interests: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从简历中提取用户兴趣并分类\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            resume_text: 简历文本\n",
    "            max_interests: 最多提取的兴趣数量\n",
    "\n",
    "        Returns:\n",
    "            兴趣列表，每个兴趣包含主题、分类和初始权重\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始从简历中提取最重要的{max_interests}项兴趣...\")\n",
    "    \n",
    "        # 构建兴趣分类提示\n",
    "        categories_text = \"\\n\".join([f\"{cat}: {', '.join(topics)}\" for cat, topics in self.interest_categories.items()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请分析以下简历文本，提取用户最重要的{max_interests}项兴趣领域和专业方向。\n",
    "        将提取的兴趣根据以下分类系统进行归类：\n",
    "        \n",
    "        {categories_text}\n",
    "        \n",
    "        如果发现的兴趣不在上述分类中，请归入最相近的类别。\n",
    "        对于每个识别的兴趣，根据在简历中的明显程度，给出一个0到1之间的权重。\n",
    "        请按照兴趣的重要性排序，最重要的兴趣排在前面。\n",
    "        \n",
    "        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\n",
    "        [\n",
    "            {{\"topic\": \"兴趣主题\", \"category\": \"所属类别\", \"weight\": 权重值}},\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        简历文本：\n",
    "        {resume_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"正在分析简历中的兴趣...\")\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长从文本中提取人们的兴趣爱好并进行分类。请只返回JSON格式的结果，不要添加任何其他标记。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            interests_text = response.choices[0].message.content\n",
    "\n",
    "            # 记录原始响应以便调试\n",
    "            logging.debug(f\"原始兴趣响应: {interests_text}\")\n",
    "            \n",
    "            # 清理可能的格式标记\n",
    "            interests_text = interests_text.strip()\n",
    "            if interests_text.startswith(\"```json\"):\n",
    "                interests_text = interests_text[7:]\n",
    "            if interests_text.startswith(\"```\"):\n",
    "                interests_text = interests_text[3:]\n",
    "            if interests_text.endswith(\"```\"):\n",
    "                interests_text = interests_text[:-3]\n",
    "            interests_text = interests_text.strip()\n",
    "            \n",
    "            logging.debug(f\"清理后的兴趣JSON文本: {interests_text}\")\n",
    "        \n",
    "            # 尝试解析JSON\n",
    "            try:\n",
    "                interests = json.loads(interests_text)\n",
    "                print(f\"成功提取 {len(interests)} 项兴趣\")\n",
    "                \n",
    "                # 确保interests是列表类型\n",
    "                if not isinstance(interests, list):\n",
    "                    logging.error(f\"解析的兴趣不是列表类型: {type(interests)}\")\n",
    "                    interests = []\n",
    "                    print(\"解析的兴趣格式不正确，将使用空列表\")\n",
    "                \n",
    "                # 保存到数据库\n",
    "                if interests:  # 只有当interests非空时才尝试保存\n",
    "                    conn = get_db_connection()\n",
    "                    try:\n",
    "                        for i, interest in enumerate(interests):\n",
    "                            if isinstance(interest, dict):  # 确保每个兴趣是字典类型\n",
    "                                # 确保所有必要的键都存在\n",
    "                                topic = interest.get(\"topic\", \"未知兴趣\")\n",
    "                                category = interest.get(\"category\", \"未分类\")\n",
    "                                weight = interest.get(\"weight\", 0.5)\n",
    "                                \n",
    "                                # 确保weight是浮点数\n",
    "                                try:\n",
    "                                    weight = float(weight)\n",
    "                                except (ValueError, TypeError):\n",
    "                                    weight = 0.5\n",
    "                                \n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, topic, category, weight)\n",
    "                                )\n",
    "                                # 显示进度\n",
    "                                print(f\"保存兴趣 {i+1}/{len(interests)}: {topic} (权重: {weight:.2f})\")\n",
    "                        conn.commit()\n",
    "                        print(\"所有兴趣已保存到数据库\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"保存兴趣到数据库时出错: {e}\")\n",
    "                        print(f\"保存兴趣时出错: {e}\")\n",
    "                        conn.rollback()  # 回滚事务\n",
    "                    finally:\n",
    "                        conn.close()\n",
    "                \n",
    "                return interests if interests else []\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"无法解析兴趣JSON: {interests_text}, 错误: {e}\")\n",
    "                print(f\"解析兴趣信息失败: {e}\")\n",
    "                \n",
    "                # 尝试手动解析简单格式\n",
    "                if \"[\" in interests_text and \"]\" in interests_text:\n",
    "                    try:\n",
    "                        # 尝试修复常见的JSON格式问题\n",
    "                        fixed_text = interests_text.replace(\"'\", \"\\\"\").replace(\"None\", \"null\")\n",
    "                        interests = json.loads(fixed_text)\n",
    "                        print(f\"修复后成功解析，提取了 {len(interests)} 项兴趣\")\n",
    "                        return interests\n",
    "                    except Exception as parse_e:\n",
    "                        logging.error(f\"尝试修复JSON后仍然失败: {parse_e}\")\n",
    "                \n",
    "                # 如果无法解析，创建一些基本兴趣\n",
    "                print(\"无法解析兴趣，将创建基本兴趣\")\n",
    "                basic_interests = []\n",
    "                \n",
    "                # 从简历文本中提取一些关键词作为基本兴趣\n",
    "                keywords = [\"人工智能\", \"机器学习\", \"数据分析\", \"编程\", \"算法\"]\n",
    "                for i, keyword in enumerate(keywords):\n",
    "                    if keyword.lower() in resume_text.lower():\n",
    "                        interest = {\n",
    "                            \"topic\": keyword,\n",
    "                            \"category\": \"技术\",\n",
    "                            \"weight\": 0.7\n",
    "                        }\n",
    "                        basic_interests.append(interest)\n",
    "                        \n",
    "                        # 保存到数据库\n",
    "                        try:\n",
    "                            conn = get_db_connection()\n",
    "                            if conn:\n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, keyword, \"技术\", 0.7)\n",
    "                                )\n",
    "                                conn.commit()\n",
    "                                print(f\"保存基本兴趣: {keyword}\")\n",
    "                                conn.close()\n",
    "                        except Exception as db_e:\n",
    "                            logging.error(f\"保存基本兴趣到数据库时出错: {db_e}\")\n",
    "                \n",
    "                return basic_interests\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取兴趣时出错: {e}\")\n",
    "            print(f\"提取兴趣时出错: {e}\")\n",
    "            return []\n",
    "\n",
    "    def record_search(self, user_id: str, query: str, platform: str):\n",
    "        \"\"\"\n",
    "        记录用户搜索行为\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            query: 搜索查询\n",
    "            platform: 搜索平台\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO user_searches (user_id, query, platform) VALUES (?, ?, ?)\",\n",
    "                (user_id, query, platform)\n",
    "            )\n",
    "            conn.commit()\n",
    "            logging.info(f\"记录用户搜索: {user_id}, 查询: {query}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def record_interaction(self, user_id: str, content_id: str, action_type: str):\n",
    "        \"\"\"\n",
    "        记录用户与内容的交互\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            content_id: 内容ID（如文章URL）\n",
    "            action_type: 交互类型（如\"点击\"、\"收藏\"、\"分享\"）\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO user_interactions (user_id, content_id, action_type) VALUES (?, ?, ?)\",\n",
    "                (user_id, content_id, action_type)\n",
    "            )\n",
    "            conn.commit()\n",
    "            logging.info(f\"记录用户交互: {user_id}, 内容: {content_id}, 行为: {action_type}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def update_interest_weights(self, user_id: str, topic: str, adjustment: float):\n",
    "        \"\"\"\n",
    "        更新用户兴趣权重\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            topic: 兴趣主题\n",
    "            adjustment: 权重调整值，正数表示增加，负数表示减少\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 查找现有兴趣\n",
    "            interest = conn.execute(\n",
    "                \"SELECT id, weight FROM user_interests WHERE user_id = ? AND topic = ? ORDER BY timestamp DESC LIMIT 1\",\n",
    "                (user_id, topic)\n",
    "            ).fetchone()\n",
    "\n",
    "            if interest:\n",
    "                # 更新权重，确保在0-1范围内\n",
    "                new_weight = max(0, min(1, interest[\"weight\"] + adjustment))\n",
    "\n",
    "                conn.execute(\n",
    "                    \"UPDATE user_interests SET weight = ?, timestamp = CURRENT_TIMESTAMP WHERE id = ?\",\n",
    "                    (new_weight, interest[\"id\"])\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"更新用户兴趣权重: {user_id}, 主题: {topic}, 新权重: {new_weight}\")\n",
    "            else:\n",
    "                # 如果不存在，创建新的兴趣项\n",
    "                weight = max(0, min(1, 0.5 + adjustment))  # 默认权重0.5加上调整值\n",
    "\n",
    "                # 尝试确定类别\n",
    "                category = \"未分类\"\n",
    "                for cat, topics in self.interest_categories.items():\n",
    "                    if any(compute_similarity(topic, t) > 0.7 for t in topics):\n",
    "                        category = cat\n",
    "                        break\n",
    "\n",
    "                conn.execute(\n",
    "                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                    (user_id, topic, category, weight)\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"创建新用户兴趣: {user_id}, 主题: {topic}, 权重: {weight}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def apply_time_decay(self, user_id: str, decay_factor: float = 0.9, days_threshold: int = 30):\n",
    "        \"\"\"\n",
    "        应用时间衰减模型，降低旧兴趣的权重\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            decay_factor: 衰减因子(0-1)\n",
    "            days_threshold: 多少天前的兴趣开始衰减\n",
    "        \"\"\"\n",
    "        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days_threshold)\n",
    "        threshold_str = threshold_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            old_interests = conn.execute(\n",
    "                \"SELECT id, topic, category, weight, timestamp FROM user_interests WHERE user_id = ? AND timestamp < ?\",\n",
    "                (user_id, threshold_str)\n",
    "            ).fetchall()\n",
    "\n",
    "            for interest in old_interests:\n",
    "                # 计算时间差（天数）\n",
    "                interest_date = datetime.datetime.strptime(interest[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "                days_diff = (datetime.datetime.now() - interest_date).days\n",
    "\n",
    "                # 计算衰减倍数（随时间增加而增加衰减）\n",
    "                decay_multiplier = days_diff // days_threshold\n",
    "\n",
    "                # 计算新权重\n",
    "                new_weight = interest[\"weight\"] * (decay_factor ** decay_multiplier)\n",
    "\n",
    "                # 更新权重\n",
    "                conn.execute(\n",
    "                    \"UPDATE user_interests SET weight = ? WHERE id = ?\",\n",
    "                    (new_weight, interest[\"id\"])\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            logging.info(f\"应用时间衰减模型: {user_id}, 处理 {len(old_interests)} 条旧兴趣\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_top_interests(self, user_id: str, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        获取用户的顶级兴趣\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            limit: 返回的兴趣数量\n",
    "\n",
    "        Returns:\n",
    "            兴趣列表，按权重排序\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 对于每个主题，只取最新的一条记录\n",
    "            interests = conn.execute(\"\"\"\n",
    "                SELECT i1.topic, i1.category, i1.weight, i1.timestamp\n",
    "                FROM user_interests i1\n",
    "                INNER JOIN (\n",
    "                    SELECT topic, MAX(timestamp) as max_time\n",
    "                    FROM user_interests\n",
    "                    WHERE user_id = ?\n",
    "                    GROUP BY topic\n",
    "                ) i2 ON i1.topic = i2.topic AND i1.timestamp = i2.max_time\n",
    "                WHERE user_id = ?\n",
    "                ORDER BY i1.weight DESC\n",
    "                LIMIT ?\n",
    "            \"\"\", (user_id, user_id, limit)).fetchall()\n",
    "\n",
    "            return [dict(i) for i in interests]\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def analyze_search_patterns(self, user_id: str, days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        分析用户搜索模式\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            days: 分析的天数范围\n",
    "\n",
    "        Returns:\n",
    "            分析结果，包含常用平台、热门查询等\n",
    "        \"\"\"\n",
    "        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days)\n",
    "        threshold_str = threshold_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 获取搜索记录\n",
    "            searches = conn.execute(\n",
    "                \"SELECT query, platform, timestamp FROM user_searches WHERE user_id = ? AND timestamp > ?\",\n",
    "                (user_id, threshold_str)\n",
    "            ).fetchall()\n",
    "\n",
    "            if not searches:\n",
    "                return {\"status\": \"无搜索记录\"}\n",
    "\n",
    "            # 统计平台使用情况\n",
    "            platforms = {}\n",
    "            for search in searches:\n",
    "                platform = search[\"platform\"]\n",
    "                platforms[platform] = platforms.get(platform, 0) + 1\n",
    "\n",
    "            # 提取查询内容用于语义分析\n",
    "            queries = [search[\"query\"] for search in searches]\n",
    "\n",
    "            # 使用LLM分析查询主题\n",
    "            prompt = f\"\"\"\n",
    "            请分析以下搜索查询列表，识别主要的搜索主题和模式。\n",
    "            将分析结果以JSON格式返回，包含以下字段：\n",
    "            1. dominant_topics: 主导主题列表，按重要性排序\n",
    "            2. search_patterns: 搜索模式描述\n",
    "\n",
    "            搜索查询列表：\n",
    "            {queries}\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的搜索行为分析助手。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                analysis_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    analysis = json.loads(analysis_text)\n",
    "                except json.JSONDecodeError:\n",
    "                    analysis = {\"dominant_topics\": [], \"search_patterns\": \"无法解析分析结果\"}\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"分析搜索模式时出错: {e}\")\n",
    "                analysis = {\"dominant_topics\": [], \"search_patterns\": f\"分析出错: {str(e)}\"}\n",
    "\n",
    "            # 整合结果\n",
    "            return {\n",
    "                \"platform_stats\": platforms,\n",
    "                \"search_count\": len(searches),\n",
    "                \"analysis\": analysis,\n",
    "                \"timeframe\": f\"过去{days}天\"\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def generate_recommendations(self, user_id: str, count: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        基于用户画像生成内容推荐\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            count: 推荐数量\n",
    "\n",
    "        Returns:\n",
    "            推荐的主题列表\n",
    "        \"\"\"\n",
    "        # 获取用户顶级兴趣\n",
    "        top_interests = self.get_top_interests(user_id, limit=5)\n",
    "\n",
    "        if not top_interests:\n",
    "            return [\"未找到用户兴趣数据\"]\n",
    "\n",
    "        # 提取兴趣主题\n",
    "        interest_topics = [i[\"topic\"] for i in top_interests]\n",
    "\n",
    "        # 使用LLM生成推荐\n",
    "        prompt = f\"\"\"\n",
    "        基于以下用户兴趣主题，推荐{count}个具体的、精细的研究或学习主题，这些主题应该是前沿的、有深度的，并与用户的兴趣紧密相关。\n",
    "\n",
    "        用户兴趣主题：{\", \".join(interest_topics)}\n",
    "\n",
    "        请列出具体的推荐主题，每个主题应包含足够的细节和专业性，以便能够直接用于学术研究或专业学习。\n",
    "        以JSON数组格式返回，每个元素包含'topic'和'reason'字段。\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的学习内容推荐助手，擅长为用户提供高质量、前沿的学习主题推荐。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            recs_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                recommendations = json.loads(recs_text)\n",
    "                return recommendations\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析推荐JSON: {recs_text}\")\n",
    "                return [{\"topic\": \"解析推荐失败\", \"reason\": \"请稍后再试\"}]\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"生成推荐时出错: {e}\")\n",
    "            return [{\"topic\": \"生成推荐出错\", \"reason\": str(e)}]\n",
    "\n",
    "    def get_user_profile_summary(self, user_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        获取用户画像摘要\n",
    "        \n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            \n",
    "        Returns:\n",
    "            用户画像摘要信息\n",
    "        \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = get_db_connection()\n",
    "            if not conn:\n",
    "                return {\"error\": \"无法连接到数据库\"}\n",
    "                \n",
    "            # 获取基本信息\n",
    "            user = conn.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,)).fetchone()\n",
    "            \n",
    "            if not user:\n",
    "                return {\"error\": \"用户不存在\"}\n",
    "            \n",
    "            # 获取顶级兴趣\n",
    "            interests = []\n",
    "            try:\n",
    "                interests_query = conn.execute(\"\"\"\n",
    "                    SELECT i1.topic, i1.category, i1.weight, i1.timestamp \n",
    "                    FROM user_interests i1\n",
    "                    INNER JOIN (\n",
    "                        SELECT topic, MAX(timestamp) as max_time\n",
    "                        FROM user_interests\n",
    "                        WHERE user_id = ?\n",
    "                        GROUP BY topic\n",
    "                    ) i2 ON i1.topic = i2.topic AND i1.timestamp = i2.max_time\n",
    "                    WHERE user_id = ?\n",
    "                    ORDER BY i1.weight DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\", (user_id, user_id))\n",
    "                \n",
    "                interests = [dict(i) for i in interests_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取用户兴趣时出错: {e}\")\n",
    "                interests = []\n",
    "            \n",
    "            # 获取技能\n",
    "            skills = []\n",
    "            try:\n",
    "                skills_query = conn.execute(\n",
    "                    \"SELECT skill, level, category FROM user_skills WHERE user_id = ? ORDER BY level DESC\",\n",
    "                    (user_id,)\n",
    "                )\n",
    "                skills = [dict(s) for s in skills_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取用户技能时出错: {e}\")\n",
    "                skills = []\n",
    "            \n",
    "            # 获取搜索统计\n",
    "            search_count = 0\n",
    "            try:\n",
    "                search_count_query = conn.execute(\n",
    "                    \"SELECT COUNT(*) as count FROM user_searches WHERE user_id = ?\", \n",
    "                    (user_id,)\n",
    "                ).fetchone()\n",
    "                if search_count_query:\n",
    "                    search_count = search_count_query[\"count\"]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取搜索统计时出错: {e}\")\n",
    "            \n",
    "            # 获取交互统计\n",
    "            interaction_count = 0\n",
    "            try:\n",
    "                interaction_count_query = conn.execute(\n",
    "                    \"SELECT COUNT(*) as count FROM user_interactions WHERE user_id = ?\", \n",
    "                    (user_id,)\n",
    "                ).fetchone()\n",
    "                if interaction_count_query:\n",
    "                    interaction_count = interaction_count_query[\"count\"]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取交互统计时出错: {e}\")\n",
    "            \n",
    "            # 获取最近5次搜索\n",
    "            recent_searches = []\n",
    "            try:\n",
    "                searches_query = conn.execute(\n",
    "                    \"SELECT query, platform, timestamp FROM user_searches WHERE user_id = ? ORDER BY timestamp DESC LIMIT 5\",\n",
    "                    (user_id,)\n",
    "                )\n",
    "                recent_searches = [dict(s) for s in searches_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取最近搜索时出错: {e}\")\n",
    "            \n",
    "            # 整合数据\n",
    "            profile = {\n",
    "                \"basic_info\": dict(user),\n",
    "                \"top_interests\": interests,\n",
    "                \"skills\": skills,\n",
    "                \"activity\": {\n",
    "                    \"search_count\": search_count,\n",
    "                    \"interaction_count\": interaction_count,\n",
    "                    \"recent_searches\": recent_searches\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return profile\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"获取用户画像摘要时出错: {e}\")\n",
    "            return {\"error\": f\"获取用户画像摘要时出错: {str(e)}\"}\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "class KnowledgeFlow:\n",
    "    def __init__(self):\n",
    "        self.context = {}\n",
    "        self.client = OpenAI(api_key=CONFIG[\"API_KEYS\"][\"deepseek\"], base_url=\"https://api.deepseek.com\")\n",
    "        self.serper_api_key = CONFIG[\"API_KEYS\"][\"serper\"]\n",
    "        self.user_profile_manager = UserProfileManager(client=self.client)\n",
    "\n",
    "    def start_node(self, user_input: Dict[str, Any]) -> Dict:\n",
    "        \"\"\" 收集用户初始信息 \"\"\"\n",
    "        logging.info(\"开始收集用户输入信息...\")  \n",
    "        required_fields = ['occupation', 'day', 'platform']\n",
    "        for field in required_fields:\n",
    "            if field not in user_input:\n",
    "                raise ValueError(f\"缺少必要字段: {field}\")\n",
    "\n",
    "        logging.debug(f\"用户输入信息: {user_input}\") \n",
    "        self.context.update(user_input)\n",
    "\n",
    "        # 创建或获取用户ID\n",
    "        if 'user_id' not in self.context and 'email' in user_input:\n",
    "            user_info = {\n",
    "                'name': user_input.get('name', '未知用户'),\n",
    "                'occupation': user_input.get('occupation', ''),\n",
    "                'email': user_input.get('email', '')\n",
    "            }\n",
    "            self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "\n",
    "        # platform_type = self.context.get('platform')\n",
    "        self.context['update_cycle'] = self.calculate_update_cycle(user_input['day'])\n",
    "        return self.context\n",
    "    \n",
    "    def calculate_update_cycle(self, days: int) -> Dict:\n",
    "        \"\"\" 计算时间范围 \"\"\"\n",
    "        logging.info(\"计算时间范围...\") \n",
    "        end_date = datetime.datetime.now(datetime.timezone.utc)\n",
    "        start_date = end_date - datetime.timedelta(days=days)\n",
    "        logging.debug(f\"时间范围计算结果: 起始日期={start_date}, 结束日期={end_date}\")\n",
    "        return {\n",
    "            \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": end_date.strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "\n",
    "    def get_user_profile(self, cv_text: str, task: str) -> Dict:\n",
    "        \"\"\"使用大语言模型API初步分析用户画像，并根据不同任务执行不同的prompt\"\"\"\n",
    "        logging.info(f\"执行任务：{task}，分析简历内容...\")\n",
    "        print(f\"\\n===== 开始执行用户画像分析任务：{task} =====\")\n",
    "\n",
    "        # 初始化返回值，确保始终返回一个字典\n",
    "        profile_analysis = {\"skills\": [], \"interests\": []}\n",
    "\n",
    "        # 如果有用户ID，先调用用户画像管理器处理简历\n",
    "        if 'user_id' in self.context:\n",
    "            user_id = self.context['user_id']\n",
    "            print(f\"用户ID: {user_id}\")\n",
    "\n",
    "            # 提取技能\n",
    "            print(\"\\n第1步：提取用户技能\")\n",
    "            skills = self.user_profile_manager.extract_skills_from_resume(user_id, cv_text)\n",
    "            logging.info(f\"从简历中提取了 {len(skills)} 项技能\")\n",
    "            print(f\"技能提取完成，共 {len(skills)} 项\")\n",
    "\n",
    "            # 提取兴趣\n",
    "            print(\"\\n第2步：提取用户兴趣\")\n",
    "            interests = self.user_profile_manager.extract_interests_from_resume(user_id, cv_text)\n",
    "            logging.info(f\"从简历中提取了 {len(interests)} 项兴趣\")\n",
    "            print(f\"兴趣提取完成，共 {len(interests)} 项\")\n",
    "\n",
    "            # 合并技能和兴趣信息到分析结果\n",
    "            profile_analysis = {\n",
    "                \"skills\": skills,\n",
    "                \"interests\": interests\n",
    "            }\n",
    "\n",
    "            # 定义不同任务的prompt模板\n",
    "            print(\"\\n第3步：生成综合分析报告\")\n",
    "            prompt_templates = {\n",
    "                \"analyze_resume\": f\"分析以下简历内容，提供全面的职业画像分析：{cv_text}\",\n",
    "                \"user_interest\": f\"根据以下简历内容，识别用户的职业兴趣和专注领域：{cv_text}\",\n",
    "                \"skill_assessment\": f\"根据以下简历内容，评估用户的技能并提出改进建议：{cv_text}\",\n",
    "                \"career_development\": f\"根据以下简历内容，分析用户的职业发展路径并提供建议：{cv_text}\"\n",
    "            }\n",
    "\n",
    "            # 确保指定的任务在模板中存在\n",
    "            if task not in prompt_templates:\n",
    "                logging.warning(f\"未知任务: {task}，将使用默认分析\")\n",
    "                task = \"analyze_resume\"\n",
    "            \n",
    "            prompt = prompt_templates[task]\n",
    "            logging.debug(f\"任务的prompt: {prompt}\") \n",
    "\n",
    "            try:\n",
    "                # 调用大语言模型API获取任务的处理结果\n",
    "                print(\"正在生成综合分析报告...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个帮助助手，负责根据用户提供的信息分析并生成建议。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                # 从大语言模型响应中提取分析结果\n",
    "                llm_analysis = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型返回的分析结果: {llm_analysis}\") \n",
    "                \n",
    "                # 合并LLM分析结果到profile_analysis\n",
    "                profile_analysis[\"llm_analysis\"] = llm_analysis\n",
    "            except Exception as e:\n",
    "                logging.error(f\"生成综合分析报告时出错: {e}\")\n",
    "                print(f\"生成综合分析报告时出错: {e}\")\n",
    "                profile_analysis[\"llm_analysis\"] = \"无法生成分析报告\"\n",
    "\n",
    "            # 更新上下文，保存用户画像分析结果\n",
    "            self.context.update({\"profile_analysis\": profile_analysis})\n",
    "            print(\"\\n用户画像分析完成！\")\n",
    "            return {\"profile_analysis\": profile_analysis}\n",
    "        else:\n",
    "            # 没有用户ID，回退到原来的方法\n",
    "            print(\"未找到用户ID，将使用简化版用户画像分析\")\n",
    "            # 定义不同任务的prompt模板\n",
    "            prompt_templates = {\n",
    "                \"analyze_resume\": f\"分析以下简历内容：{cv_text}\",\n",
    "                \"user_interest\": f\"根据以下简历内容，识别用户的职业兴趣和专注领域：{cv_text}\",\n",
    "                \"skill_assessment\": f\"根据以下简历内容，评估用户的技能并提出改进建议：{cv_text}\",\n",
    "            }\n",
    "\n",
    "            # 确保指定的任务在模板中存在\n",
    "            if task not in prompt_templates:\n",
    "                tem_task = task\n",
    "                task = \"analyze_resume\"\n",
    "                raise ValueError(f\"未知任务: {tem_task}. 请定义一个有效的任务。\")\n",
    "\n",
    "            prompt = prompt_templates[task]\n",
    "            logging.debug(f\"任务的prompt: {prompt}\")\n",
    "\n",
    "            try:\n",
    "                # 调用大语言模型API获取任务的处理结果\n",
    "                print(\"正在生成简化版分析报告...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个帮助助手，负责根据用户提供的信息分析并生成建议。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                # 从大语言模型响应中提取分析结果\n",
    "                profile_data = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型返回的分析结果: {profile_data}\") \n",
    "                \n",
    "                # 更新上下文，保存用户画像分析结果\n",
    "                profile_analysis = {\"text_analysis\": profile_data}\n",
    "                self.context.update({\"profile_analysis\": profile_analysis})\n",
    "            except Exception as e:\n",
    "                logging.error(f\"生成简化版分析报告时出错: {e}\")\n",
    "                print(f\"生成简化版分析报告时出错: {e}\")\n",
    "                profile_analysis = {\"text_analysis\": \"无法生成分析报告\"}\n",
    "                self.context.update({\"profile_analysis\": profile_analysis})\n",
    "                \n",
    "            print(\"\\n简化版用户画像分析完成！\")\n",
    "            return {\"profile_analysis\": profile_analysis}\n",
    "\n",
    "    def build_user_profile(self, user_input: Dict, cv_text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        构建用户画像，确保用户ID存在并分析用户简历\n",
    "        \n",
    "        Args:\n",
    "            user_input: 用户输入的基本信息\n",
    "            cv_text: 用户简历文本\n",
    "            \n",
    "        Returns:\n",
    "            用户画像信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if cv_text.strip():\n",
    "                print(\"已收到简历，开始分析...\")\n",
    "\n",
    "                # 确保用户有ID - 如果没有，创建一个新用户\n",
    "                if 'user_id' not in self.context and 'email' in user_input:\n",
    "                    print(\"检测到新用户，正在创建用户档案...\")\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', ''),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': user_input.get('email', '')\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建新用户，ID: {self.context['user_id']}\")\n",
    "\n",
    "                # 如果仍然没有用户ID（可能是因为没有提供邮箱），创建一个临时ID\n",
    "                if 'user_id' not in self.context:\n",
    "                    import hashlib\n",
    "                    import time\n",
    "                    temp_id = hashlib.md5(f\"{time.time()}-{user_input.get('occupation', '')}-temp\".encode()).hexdigest()\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '临时用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': f\"temp_{temp_id[:8]}@example.com\"  # 创建临时邮箱\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建临时用户，ID: {self.context['user_id']}\")\n",
    "                    print(\"注意：由于未提供邮箱，此用户为临时用户，数据可能不会长期保存\")\n",
    "\n",
    "                # 现在可以确保有用户ID了，继续进行用户画像分析\n",
    "                try:\n",
    "                    task = \"analyze_resume\"  # 或根据需要选择其他任务\n",
    "                    user_profile = self.get_user_profile(cv_text, task)\n",
    "                    # 更新用户画像信息\n",
    "                    if user_profile:\n",
    "                        self.context.update(user_profile)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"分析用户画像时出错: {e}\")\n",
    "                    print(f\"分析用户画像时出错: {e}\")\n",
    "                    print(\"将继续使用基本用户信息\")\n",
    "                    user_profile = {\"basic_profile\": True}\n",
    "\n",
    "                # 显示用户画像摘要\n",
    "                try:\n",
    "                    self.display_profile_summary()\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"显示用户画像摘要时出错: {e}\")\n",
    "                    print(f\"显示用户画像摘要时出错: {e}\")\n",
    "                \n",
    "                return user_profile or {\"basic_profile\": True}\n",
    "            else:\n",
    "                print(\"未提供简历，但仍将创建基本用户档案\")\n",
    "\n",
    "                # 即使没有简历，也要确保用户有ID\n",
    "                if 'user_id' not in self.context and 'email' in user_input:\n",
    "                    print(\"创建基本用户档案...\")\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '未知用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': user_input.get('email', '')\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建新用户，ID: {self.context['user_id']}\")\n",
    "\n",
    "                    # 从用户输入中提取基本兴趣\n",
    "                    if 'content_type' in user_input and user_input['content_type']:\n",
    "                        print(f\"基于您提供的关注领域'{user_input['content_type']}'添加初始兴趣\")\n",
    "                        try:\n",
    "                            self.user_profile_manager.update_interest_weights(\n",
    "                                self.context['user_id'],\n",
    "                                user_input['content_type'],\n",
    "                                0.8  # 较高的初始权重\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"添加初始兴趣时出错: {e}\")\n",
    "                            print(f\"添加初始兴趣时出错: {e}\")\n",
    "                \n",
    "                return {\"basic_profile\": True}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"构建用户画像时出错: {e}\")\n",
    "            print(f\"构建用户画像时出错: {e}\")\n",
    "            print(\"将继续使用基本用户信息\")\n",
    "            return {\"basic_profile\": True}\n",
    "\n",
    "    def display_profile_summary(self):\n",
    "        \"\"\"显示用户画像摘要\"\"\"\n",
    "        if 'user_id' in self.context:\n",
    "            try:\n",
    "                profile_summary = self.user_profile_manager.get_user_profile_summary(self.context['user_id'])\n",
    "                if not profile_summary or 'error' in profile_summary:\n",
    "                    print(\"\\n--- 用户画像摘要 ---\")\n",
    "                    print(f\"用户ID: {self.context['user_id']}\")\n",
    "                    print(\"无法获取完整的用户画像信息\")\n",
    "                    print(\"-----------------\\n\")\n",
    "                    return\n",
    "                    \n",
    "                print(\"\\n--- 用户画像摘要 ---\")\n",
    "                print(f\"用户ID: {self.context['user_id']}\")\n",
    "                print(f\"用户名: {self.context['user_name']}\")\n",
    "                print(f\"职业: {profile_summary.get('basic_info', {}).get('occupation', '未知')}\")\n",
    "                \n",
    "                # 安全地获取技能数量\n",
    "                skills = profile_summary.get('skills', [])\n",
    "                print(f\"技能数量: {len(skills)}\")\n",
    "                \n",
    "                # 安全地获取顶级兴趣\n",
    "                interests = profile_summary.get('top_interests', [])\n",
    "                print(\"顶级兴趣:\")\n",
    "                if interests:\n",
    "                    for interest in interests[:3]:\n",
    "                        print(f\"  - {interest.get('topic', '未知')} (权重: {interest.get('weight', 0):.2f})\")\n",
    "                else:\n",
    "                    print(\"  暂无兴趣数据\")\n",
    "                print(\"-----------------\\n\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"显示用户画像摘要时出错: {e}\")\n",
    "                print(f\"\\n显示用户画像摘要时出错: {e}\")\n",
    "                print(\"将继续执行后续步骤\")\n",
    "\n",
    "    def _update_interests_from_query(self, query: str, weight_adjustment: float = 0.05):\n",
    "        \"\"\"\n",
    "        从用户查询中提取可能的兴趣点并更新用户画像\n",
    "\n",
    "        Args:\n",
    "            query: 用户查询\n",
    "            weight_adjustment: 权重调整幅度\n",
    "        \"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "\n",
    "        # 使用LLM提取查询中的兴趣点\n",
    "        prompt = f\"\"\"\n",
    "        请从以下搜索查询中提取最多3个主要的兴趣领域或关键主题。\n",
    "        只返回提取的主题列表，格式为JSON数组，例如：[\"人工智能\", \"机器学习\", \"自然语言处理\"]\n",
    "\n",
    "        搜索查询: {query}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的主题提取助手，擅长从文本中识别核心主题。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            topics_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                topics = json.loads(topics_text)\n",
    "\n",
    "                # 更新每个主题的权重\n",
    "                for topic in topics:\n",
    "                    self.user_profile_manager.update_interest_weights(user_id, topic, weight_adjustment)\n",
    "                    logging.info(f\"从查询中更新用户兴趣: {topic}, 调整: +{weight_adjustment}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取查询主题时出错: {e}\")\n",
    "\n",
    "    async def translate_query(self, query):\n",
    "        \"\"\"使用多引擎翻译并比较结果\"\"\"\n",
    "        translations = {}\n",
    "\n",
    "        try:\n",
    "            translations['谷歌'] = GoogleTranslator(source='auto', target='en').translate(query)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"谷歌翻译失败: {e}\")\n",
    "\n",
    "        # try:\n",
    "        #     translations['百度'] = BaiduTranslator(appid='YOUR_ID', appkey='YOUR_KEY').translate(query, dst='en')\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"百度翻译失败: {e}\")\n",
    "\n",
    "        # 其他翻译API\n",
    "        # try:\n",
    "        #     translations['DeepL'] = DeepLTranslator(source='ZH', target='EN').translate(query)\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"DeepL翻译失败: {e}\")\n",
    "\n",
    "        if not translations:\n",
    "            logging.error(\"所有翻译引擎均失败\")\n",
    "            return query\n",
    "\n",
    "        translations_text = \"\\n\".join([f\"{engine}翻译：{result}\" for engine, result in translations.items()])\n",
    "\n",
    "        try:\n",
    "            validation = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是专业翻译验证助手。回答只用翻译后的内容本身！以下是不同引擎翻译的结果，请你思考它们从中文到英文翻译的准确性，并提供最准确的翻译。最终结果只用翻译后的内容本身。\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"原文：{query}\\n{translations_text}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            translated_query = validation.choices[0].message.content\n",
    "            translations['大模型'] = translated_query\n",
    "\n",
    "            # 如果有用户ID，记录这次翻译\n",
    "            if 'user_id' in self.context:\n",
    "                # 更新用户兴趣 - 从查询中提取可能的兴趣点\n",
    "                self._update_interests_from_query(query)\n",
    "\n",
    "            best_translation = None\n",
    "            best_similarity = -1\n",
    "\n",
    "            for engine, translated_text in translations.items():\n",
    "                similarity = compute_similarity(query, translated_text)\n",
    "                logging.info(f\"{engine}翻译为：{translated_text}，相似度: {similarity}\")\n",
    "\n",
    "                if similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_translation = translated_text\n",
    "\n",
    "            logging.info(f\"最终翻译结果:{translated_query} ，相似度是：{best_similarity}\")\n",
    "            return translated_query # 目前匹配效果不佳，暂用大模型结果\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"大语言模型验证失败: {e}\")\n",
    "            # 如果验证失败，返回第一个可用的翻译结果\n",
    "            return next(iter(translations.values()))\n",
    "            \n",
    "    async def build_search_query(self) -> Dict:\n",
    "        \"\"\"根据用户需求构建搜索查询，并将查询词转换为英文\"\"\"\n",
    "        logging.info(\"构建搜索查询并进行翻译...\")  \n",
    "\n",
    "        query_base = self.context.get('content_type', \" \".join(self.context.get('content_focus', [])))\n",
    "\n",
    "        # 翻译查询\n",
    "        translated_query_base = await self.translate_query(query_base)\n",
    "        if translated_query_base == query_base:\n",
    "            logging.warning(\"翻译内容与原查询一致，可能未成功翻译。\")\n",
    "    \n",
    "        # 构建基于时间范围的搜索查询\n",
    "        time_range = f\"after:{self.context['update_cycle']['start_date']} before:{self.context['update_cycle']['end_date']}\"\n",
    "        logging.debug(f\"构建的搜索查询：query_google={translated_query_base} {time_range} site:google.com\")  \n",
    "        \n",
    "        # 如果有用户ID，记录这次搜索\n",
    "        if 'user_id' in self.context:\n",
    "            self.user_profile_manager.record_search(\n",
    "                self.context['user_id'],\n",
    "                query_base,\n",
    "                self.context.get('platform', '')\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"query_google\": f\"{translated_query_base} {time_range} site:google.com\",\n",
    "            \"query_arxiv\": f'\\\"{translated_query_base}\\\" AND submittedDate:[{self.context[\"update_cycle\"][\"start_date\"].replace(\"-\", \"\")} TO {self.context[\"update_cycle\"][\"end_date\"].replace(\"-\", \"\")}]',\n",
    "            \"query_google_arxiv\": f\"{translated_query_base} {time_range} arXiv site:arxiv.org\"\n",
    "        }\n",
    "\n",
    "    def execute_search(self, queries: Dict) -> Dict:\n",
    "        \"\"\"执行实际的搜索操作\"\"\"\n",
    "        logging.info(\"执行搜索操作...\")  \n",
    "        results = {}\n",
    "\n",
    "        # 获取用户选择的平台类型\n",
    "        platform_type = self.context.get('platform', '新闻类')\n",
    "\n",
    "        # 根据不同的平台类型调整搜索策略和结果数量\n",
    "        if platform_type == \"学术期刊\":\n",
    "            # 学术期刊类：只使用ArXiv搜索，最多返回7条结果\n",
    "            if queries.get('query_google_arxiv'):\n",
    "                logging.info(\"用户选择学术期刊类，执行Google_ArXiv搜索...\")\n",
    "                logging.debug(f\"执行Google_ArXiv搜索: {queries['query_google_arxiv']}\")\n",
    "                results['google_arxiv'] = self.arxiv_search(queries['query_google_arxiv'], max_results=7)\n",
    "            if queries.get('query_arxiv'):\n",
    "                logging.info(\"用户选择学术期刊类，执行ArXiv搜索...\")\n",
    "                logging.debug(f\"执行ArXiv搜索: {queries['query_arxiv']}\") \n",
    "                results['arxiv'] = self.arxiv_search(queries['query_arxiv'], max_results=7)\n",
    "\n",
    "                \n",
    "        elif platform_type == \"新闻类\":\n",
    "            # 新闻类：只使用Google搜索，最多返回7条结果\n",
    "            logging.info(\"用户选择新闻类，执行Google搜索...\")\n",
    "            if queries.get('query_google'):\n",
    "                logging.debug(f\"执行Google搜索: {queries['query_google']}\")  \n",
    "                results['google'] = self.google_search(queries['query_google'], max_results=7)\n",
    "                \n",
    "        else:  # 综合类或其他类型\n",
    "            # 综合类：同时使用Google和ArXiv搜索，各返回最多4条结果\n",
    "            logging.info(\"用户选择综合类，执行Google和ArXiv搜索...\")\n",
    "            if queries.get('query_google'):\n",
    "                logging.debug(f\"执行Google搜索: {queries['query_google']}\")  \n",
    "                results['google'] = self.google_search(queries['query_google'], max_results=4)\n",
    "            if queries.get('query_arxiv'):\n",
    "                logging.debug(f\"执行ArXiv搜索: {queries['query_arxiv']}\") \n",
    "                results['arxiv'] = self.arxiv_search(queries['query_arxiv'], max_results=4)\n",
    "            if queries.get('query_google_arxiv'):\n",
    "                logging.debug(f\"执行Google_ArXiv搜索: {queries['query_google_arxiv']}\") \n",
    "                results['google_arxiv'] = self.arxiv_search(queries['query_google_arxiv'], max_results=4)\n",
    "\n",
    "        logging.debug(f\"搜索结果: {results}\")  \n",
    "        return results\n",
    "\n",
    "    def google_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行Google搜索，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行Google搜索，限制结果数量为{max_results}...\")  \n",
    "        api_url = f\"https://google.serper.dev/search?q={query}&num={max_results}\"\n",
    "        headers = {'X-API-KEY': self.serper_api_key}\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            logging.debug(f\"Google搜索返回结果数量: {len(search_results.get('organic', []))}\")\n",
    "            logging.debug(f\"Google搜索结果: {search_results}\")\n",
    "            # 限制结果数量\n",
    "            if 'organic' in search_results:\n",
    "                search_results['organic'] = search_results['organic'][:max_results]\n",
    "            return self.parse_google_results(search_results, query)\n",
    "        else:\n",
    "            logging.error(f\"Google搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"Google搜索失败\"}\n",
    "    \n",
    "    def parse_google_results(self, data: Dict, query: str) -> Dict:\n",
    "        \"\"\"解析Google搜索结果\"\"\"\n",
    "        results = []\n",
    "        for item in data.get('organic', []):\n",
    "            title = item.get('title', '')\n",
    "            snippet = item.get('snippet', '')[:1400]  # 限制摘要长度\n",
    "            link = item.get('link', '')\n",
    "            date = item.get('date', '')\n",
    "            position = item.get('position', '')\n",
    "\n",
    "            # 计算标题和摘要与查询的相似度\n",
    "            title_similarity = compute_similarity(query, title)\n",
    "            snippet_similarity = compute_similarity(query, snippet)\n",
    "            overall_similarity = 0.3 * title_similarity + 0.7 * snippet_similarity\n",
    "\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'snippet': snippet,\n",
    "                'link': link,\n",
    "                'date': date,\n",
    "                'position': position,\n",
    "                'similarity': overall_similarity,\n",
    "            })\n",
    "\n",
    "        # 按相似度排序\n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "        return {'results': results}\n",
    "\n",
    "    def arxiv_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行ArXiv搜索，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行ArXiv搜索，限制结果数量为{max_results}...\")\n",
    "        api_url = f'http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}'\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            #logging.debug(f\"ArXiv API response (first 500 chars): {response.text[:500]}...\")\n",
    "            arxiv_results = self.parse_arxiv_response(response.text, query)\n",
    "            logging.debug(f\"ArXiv搜索返回结果数量: {len(arxiv_results.get('results', []))}\")\n",
    "            return arxiv_results\n",
    "        else:\n",
    "            logging.error(f\"ArXiv搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"ArXiv搜索失败\"}\n",
    "\n",
    "    def parse_arxiv_response(self, xml_data: str, query: str) -> Dict:\n",
    "        \"\"\"解析ArXiv的响应数据\"\"\"\n",
    "        tree = ElementTree.fromstring(xml_data)\n",
    "        results = []\n",
    "        for entry in tree.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "\n",
    "            link = \"\"\n",
    "            for link_element in entry.findall(\"{http://www.w3.org/2005/Atom}link\"):\n",
    "                if link_element.get(\"rel\") == \"alternate\":\n",
    "                    link = link_element.get(\"href\")\n",
    "                    break\n",
    "\n",
    "            if not link:\n",
    "                link_element = entry.find(\"{http://www.w3.org/2005/Atom}link\")\n",
    "                if link_element is not None:\n",
    "                    link = link_element.get(\"href\", \"\")\n",
    "\n",
    "            # 计算标题和摘要与查询的相似度\n",
    "            title_similarity = compute_similarity(query, title)\n",
    "            summary_similarity = compute_similarity(query, summary)\n",
    "            overall_similarity = 0.7 * title_similarity + 0.3 * summary_similarity\n",
    "\n",
    "            # 提取发布日期\n",
    "            # date = entry.find(\"{http://www.w3.org/2005/Atom}published\").text if entry.find(\"{http://www.w3.org/2005/Atom}published\") is not None else \"\"\n",
    "\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'snippet': summary[:1400],\n",
    "                'link': link,\n",
    "                #'date': date,\n",
    "                'similarity': overall_similarity\n",
    "            })\n",
    "\n",
    "        # 按照相似度进行排序\n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "        return {\"results\": results}\n",
    "\n",
    "    def google_arxiv_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行Google搜索，用于搜索ArXiv文献，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行Google搜索用于查找ArXiv文献，限制结果数量为{max_results}...\")  \n",
    "        api_url = f\"https://google.serper.dev/search?q={query}&num={max_results}\"\n",
    "        headers = {'X-API-KEY': self.serper_api_key}  \n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            logging.debug(f\"Google_ArXiv搜索返回结果数量: {len(search_results.get('organic', []))}\")\n",
    "            if 'organic' in search_results:\n",
    "                search_results['organic'] = search_results['organic'][:max_results]\n",
    "            #logging.debug(f\"Google ArXiv搜索结果: {search_results}\") \n",
    "            return self.parse_google_results(search_results, query)\n",
    "        else:\n",
    "            logging.error(f\"Google ArXiv搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"Google ArXiv搜索失败\"}\n",
    "    \n",
    "    def integrate_with_large_model(self, search_results: Dict) -> str:\n",
    "        \"\"\"调用大语言模型进行整合\"\"\"\n",
    "        logging.info(\"调用大语言模型进行整合搜索结果...\")  \n",
    "    \n",
    "        # 将搜索结果转换为大语言模型所需的格式\n",
    "        search_results_str = json.dumps(search_results, ensure_ascii=False, cls=NumpyEncoder)\n",
    "\n",
    "        try:\n",
    "            # 调用大语言模型的API进行结果整合\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                # model=\"qwen-plus\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"你是一个内容整理助手，负责根据用户提供的信息整合搜索结果并生成报告。使用中文。不要md格式。\"\n",
    "                            \"将报告中的内容，以：\\\"来源：...（Google/arXiv/Google_arXiv等平台） \\n 标题：... \\n 摘要：... \\n 原文网址：...（只使用搜索结果中提供的真实链接）\\n BERT嵌入的余弦相似度:... \\\"的形式呈现出来。\"\n",
    "                            \"如果搜索结果中没有提供原文网址，则写'原文网址：未提供'。不要编造或猜测网址。\"\n",
    "                            \"报告使用用户交谈时的语言，如果原文不是，则准确的转化为用户使用的语言。目前的用户使用的是中文，将结果也转化为中文！\"\n",
    "                            \"如果无法完成就直接翻译用snippet的内容回答将\\\"摘要：\\\"改为\\\"片段：\\\"。\"\n",
    "                            \"并且回答严格按照规范来，就算无法完成任务也不要说别的不符合规范的话。\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"请整合以下搜索结果并生成最终报告：{search_results_str}\"\n",
    "                    }\n",
    "                ],\n",
    "                stream=False\n",
    "            )\n",
    "            if response and hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "                integration_result = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型整合结果: {integration_result}\")\n",
    "            else:\n",
    "                raise ValueError(\"API响应没有有效的choices字段\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"调用大语言模型整合时发生错误: {e}\")\n",
    "            integration_result = \"由于API错误，无法生成整合结果。\"\n",
    "        \n",
    "        return integration_result\n",
    "    \n",
    "    def _extract_interest_from_content(self, title: str, snippet: str, weight_adjustment: float = 0.03):\n",
    "        \"\"\"从内容中提取兴趣点并更新用户模型\"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "        combined_text = f\"{title}\\n{snippet}\"\n",
    "\n",
    "        # 提取主题\n",
    "        prompt = f\"\"\"\n",
    "        请从以下文本中提取最多3个核心学术或专业主题。\n",
    "        只返回主题列表，格式为JSON数组，例如：[\"强化学习\", \"计算机视觉\", \"神经网络\"]\n",
    "\n",
    "        文本:\n",
    "        {combined_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的主题提取助手，擅长从文本中识别核心学术主题。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            topics_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                topics = json.loads(topics_text)\n",
    "\n",
    "                # 更新每个主题的权重\n",
    "                for topic in topics:\n",
    "                    self.user_profile_manager.update_interest_weights(user_id, topic, weight_adjustment)\n",
    "                    logging.info(f\"从内容中提取用户兴趣: {topic}, 调整: +{weight_adjustment}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取内容主题时出错: {e}\")\n",
    "\n",
    "    def generate_report(self, search_results: Dict) -> str:\n",
    "        \"\"\"生成最终的搜索报告\"\"\"\n",
    "        logging.info(\"生成最终的搜索报告...\") \n",
    "        report = []\n",
    "        platform_type = self.context.get('platform')\n",
    "        \n",
    "        # 先构建Google、ArXiv等来源的报告内容\n",
    "        for source, data in search_results.items():\n",
    "            if 'error' in data:\n",
    "                report.append(f\"来源：{source}\\n错误：{data['error']}\")\n",
    "            else:\n",
    "                for item in data.get('results', [])[:3]: \n",
    "                    if isinstance(item, dict): \n",
    "                        title = item.get('title', '')\n",
    "                        snippet = item.get('snippet', '')\n",
    "                        link = item.get('link', '')\n",
    "                        report.append(f\"来源：{source}\\n标题：{title}\\n摘要：{snippet}...\\n链接：{link}\")\n",
    "\n",
    "                    # 如果有用户ID，记录这个内容为潜在兴趣点\n",
    "                    # if 'user_id' in self.context and link:\n",
    "                    #     # 从标题中提取可能的兴趣点\n",
    "                    #     self._extract_interest_from_content(title, snippet, weight_adjustment=0.03)\n",
    "\n",
    "                    #     # 记录内容交互\n",
    "                    #     self.user_profile_manager.record_interaction(\n",
    "                    #         self.context['user_id'],\n",
    "                    #         link,\n",
    "                    #         \"search_result\"\n",
    "                    #     )\n",
    "\n",
    "        # 再调用大语言模型整合结果\n",
    "        final_report = self.integrate_with_large_model(search_results)\n",
    "    \n",
    "        # 最终替换为大语言模型整合后的内容\n",
    "        report.clear()\n",
    "\n",
    "        # 如果有用户ID，记录这个内容为潜在兴趣点\n",
    "        # if 'user_id' in self.context and link:\n",
    "        #     # 从标题中提取可能的兴趣点\n",
    "        #     self._extract_interest_from_content(title, snippet, weight_adjustment=0.03)\n",
    "\n",
    "        #     # 记录内容交互\n",
    "        #     self.user_profile_manager.record_interaction(\n",
    "        #         self.context['user_id'],\n",
    "        #         link,\n",
    "        #         \"search_result\"\n",
    "        #     )\n",
    "\n",
    "        # 如果有用户ID，添加个性化推荐\n",
    "        # if 'user_id' in self.context:\n",
    "        #     # 应用时间衰减模型\n",
    "        #     self.user_profile_manager.apply_time_decay(self.context['user_id'])\n",
    "\n",
    "        #     # 获取推荐内容\n",
    "        #     try:\n",
    "        #         recommendations = self.user_profile_manager.generate_recommendations(self.context['user_id'], count=3)\n",
    "\n",
    "        #         # 添加推荐内容到报告\n",
    "        #         if recommendations and len(recommendations) > 0:\n",
    "        #             rec_text = \"\\n\\n--- 基于您的兴趣，我们还为您推荐以下主题 ---\\n\\n\"\n",
    "        #             for rec in recommendations:\n",
    "        #                 rec_text += f\"主题：{rec.get('topic', '')}\\n\"\n",
    "        #                 rec_text += f\"原因：{rec.get('reason', '')}\\n\\n\"\n",
    "\n",
    "        #             report.append(rec_text)\n",
    "        #     except Exception as e:\n",
    "        #         logging.error(f\"生成推荐时出错: {e}\")\n",
    "\n",
    "        report.append(f\"\\n\\n--- 根据您选择的【{platform_type}】平台，近几日的行业内最新进展已整理好，请查收！ ---\\n\")\n",
    "        report.append(final_report)\n",
    "\n",
    "        logging.debug(f\"生成的报告内容: {report}\")\n",
    "        return \"\\n\\n\".join(report)\n",
    "\n",
    "    def send_email(self, report: str):\n",
    "        \"\"\"发送邮件（示例）\"\"\"\n",
    "        logging.info(\"准备发送邮件...\")\n",
    "        if 'email' in self.context and report:\n",
    "            print(f\"已发送邮件到 {self.context['email']}:\\n{report}\")\n",
    "        else:\n",
    "            print(\"未找到邮件地址或报告为空，未发送邮件。\")\n",
    "\n",
    "    def process_user_feedback(self, content_id: str, feedback_type: str, feedback_text: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        处理用户对内容的反馈\n",
    "\n",
    "        Args:\n",
    "            content_id: 内容ID（URL或其他标识）\n",
    "            feedback_type: 反馈类型（\"like\"、\"dislike\"、\"save\"、\"share\"等）\n",
    "            feedback_text: 反馈文本（可选）\n",
    "\n",
    "        Returns:\n",
    "            处理结果描述\n",
    "        \"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return \"无用户信息，无法处理反馈\"\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "\n",
    "        # 记录交互\n",
    "        self.user_profile_manager.record_interaction(user_id, content_id, feedback_type)\n",
    "\n",
    "        # 根据反馈类型调整兴趣权重\n",
    "        if feedback_type in (\"like\", \"save\", \"share\"):\n",
    "            # 提取内容关联主题并增加权重\n",
    "            try:\n",
    "                # 获取内容摘要（实际应用中可能需要从数据库或通过URL获取）\n",
    "                content_summary = feedback_text if feedback_text else \"用户喜欢的内容\"\n",
    "\n",
    "                # 提取主题\n",
    "                prompt = f\"\"\"\n",
    "                请从以下用户反馈中提取最多2个可能的兴趣主题。\n",
    "                只返回主题列表，格式为JSON数组。\n",
    "\n",
    "                用户反馈类型: {feedback_type}\n",
    "                反馈内容: {content_summary}\n",
    "                \"\"\"\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长分析用户反馈。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                topics_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    topics = json.loads(topics_text)\n",
    "\n",
    "                    # 增加各主题权重\n",
    "                    for topic in topics:\n",
    "                        self.user_profile_manager.update_interest_weights(user_id, topic, 0.1)\n",
    "                        logging.info(f\"基于正面反馈增加兴趣权重: {topic} +0.1\")\n",
    "\n",
    "                    return f\"已记录您对'{content_id}'的{feedback_type}反馈，并更新了您的兴趣模型\"\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "                    return \"已记录您的反馈，但分析主题时出错\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理反馈时出错: {e}\")\n",
    "                return f\"处理反馈时出错: {str(e)}\"\n",
    "\n",
    "        elif feedback_type == \"dislike\":\n",
    "            # 提取内容关联主题并减少权重\n",
    "            try:\n",
    "                content_summary = feedback_text if feedback_text else \"用户不喜欢的内容\"\n",
    "\n",
    "                # 提取主题\n",
    "                prompt = f\"\"\"\n",
    "                请从以下用户不喜欢的内容中提取最多2个可能的兴趣主题。\n",
    "                只返回主题列表，格式为JSON数组。\n",
    "\n",
    "                不喜欢的内容: {content_summary}\n",
    "                \"\"\"\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长分析用户反馈。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                topics_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    topics = json.loads(topics_text)\n",
    "\n",
    "                    # 减少各主题权重\n",
    "                    for topic in topics:\n",
    "                        self.user_profile_manager.update_interest_weights(user_id, topic, -0.1)\n",
    "                        logging.info(f\"基于负面反馈减少兴趣权重: {topic} -0.1\")\n",
    "\n",
    "                    return f\"已记录您对'{content_id}'的不喜欢反馈，并更新了您的兴趣模型\"\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "                    return \"已记录您的反馈，但分析主题时出错\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理反馈时出错: {e}\")\n",
    "                return f\"处理反馈时出错: {str(e)}\"\n",
    "\n",
    "        return f\"已记录您对'{content_id}'的{feedback_type}反馈\"\n",
    "\n",
    "class ResumeReader:\n",
    "    \"\"\"用于读取多种格式简历文件的类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.supported_formats = {\n",
    "            '.txt': self.read_txt,\n",
    "            '.pdf': self.read_pdf,\n",
    "            '.docx': self.read_docx,\n",
    "            '.doc': self.read_doc,\n",
    "            '.xlsx': self.read_excel,\n",
    "            '.xls': self.read_excel,\n",
    "            '.jpg': self.read_image,\n",
    "            '.jpeg': self.read_image,\n",
    "            '.png': self.read_image,\n",
    "        }\n",
    "        logging.info(\"初始化ResumeReader，支持的格式：%s\", list(self.supported_formats.keys()))\n",
    "\n",
    "    def read_resume(self, file_path=None):\n",
    "        \"\"\"读取简历文件或请求用户输入\"\"\"\n",
    "        if not file_path:\n",
    "            choice = input(\"请选择输入方式：1.直接输入文本 2.上传文件 (如有需要进行用户画像构建请输入数字，不需要可回车（或确认）跳过): \")\n",
    "\n",
    "            if choice == \"\":\n",
    "                return \"\"\n",
    "            elif choice == \"1\":\n",
    "                return input(\"请输入您的简历文本：\")\n",
    "            elif choice == \"2\":\n",
    "                file_path = input(\"请输入简历文件的完整路径：\").strip().strip('\"')\n",
    "            else:\n",
    "                logging.warning(\"无效的选择，默认使用文本输入方式\")\n",
    "                return input(\"请输入您的简历文本：\")\n",
    "\n",
    "        while not os.path.exists(file_path):\n",
    "            logging.error(f\"文件不存在: {file_path}\")\n",
    "            choice = input(f\"文件不存在: {file_path}\\n请输入有效的文件路径，或输入 'q' 退出：\").strip()\n",
    "\n",
    "            if choice.lower() == 'q':\n",
    "                logging.info(\"用户选择退出\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                file_path = choice.strip()\n",
    "\n",
    "        file_path = file_path.strip('\\\"')\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if file_ext not in self.supported_formats:\n",
    "            logging.warning(f\"不支持的文件格式: {file_ext}，支持的格式有: {list(self.supported_formats.keys())}\")\n",
    "            return self.ask_for_input()\n",
    "\n",
    "        # 调用相应的文件读取方法\n",
    "        try:\n",
    "            text = self.supported_formats[file_ext](file_path)\n",
    "            logging.info(f\"成功读取{file_ext}格式简历文件\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"读取文件时出错: {e}\")\n",
    "            return self.ask_for_input()\n",
    "\n",
    "    def ask_for_input(self):\n",
    "        \"\"\"帮助用户提供错误反馈并重新输入\"\"\"\n",
    "        return input(\"请输入您的简历文本：\")\n",
    "\n",
    "    def read_txt(self, file_path):\n",
    "        \"\"\"读取txt文本文件\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # 尝试其他编码\n",
    "            with open(file_path, 'r', encoding='gbk') as f:\n",
    "                return f.read()\n",
    "\n",
    "    def read_pdf(self, file_path):\n",
    "        \"\"\"读取PDF文件\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def read_docx(self, file_path):\n",
    "        \"\"\"读取Word docx文件\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    def read_doc(self, file_path):\n",
    "        \"\"\"读取旧版 Word doc文件 (需要转换)\"\"\"\n",
    "        logging.warning(\"直接读取.doc文件需要额外依赖，建议转换为.docx或.pdf格式\")\n",
    "        return f\"无法直接读取.doc文件: {file_path}，请转换为.docx或.pdf格式后重试。\"\n",
    "\n",
    "    def read_excel(self, file_path):\n",
    "        \"\"\"读取Excel文件\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    def read_image(self, file_path):\n",
    "        \"\"\"使用OCR读取图片中的文本\"\"\"\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(img, lang='chi_sim+eng')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"OCR处理图片时出错: {e}\")\n",
    "            return f\"OCR处理图片时出错: {e}\"\n",
    "\n",
    "def collect_user_input() -> Dict:\n",
    "    \"\"\"收集真实用户输入\"\"\"\n",
    "    print(\"\\n===== 欢迎使用KnowlEdge系统 =====\")\n",
    "    print(\"请提供以下信息，以便我们为您提供个性化的行业知识更新\")\n",
    "\n",
    "    # user_name = input(\"请输入您的用户名: \").strip()\n",
    "    # occupation = input(\"请输入您的职业: \").strip() or \"算法工程师\"\n",
    "    # days = int(input(\"请输入获取知识更新周期（天数，默认10）: \").strip() or \"10\")\n",
    "    # platform = input(\"请输入消息来源平台（学术期刊/新闻类/综合类，默认学术期刊）: \").strip() or \"学术期刊\"\n",
    "    # content_type = input(\"请输入关注领域（如：大语言模型，默认大语言模型）: \").strip() or \"大语言模型\"\n",
    "    # email = input(\"请输入您的邮箱（用于接收报告和识别用户）: \").strip() or \"example@example.com\"\n",
    "\n",
    "    print(\"\\n您的信息已收集完毕，系统将基于这些信息为您提供个性化服务\")\n",
    "\n",
    "    user_name = \"Tssword4\"\n",
    "    occupation = \"算法工程师\"\n",
    "    days = 7\n",
    "    platform = \"学术期刊\"\n",
    "    content_type = \"自然语言处理\"\n",
    "    email = \"1145144@qq.com\"\n",
    "\n",
    "    return {\n",
    "        \"user_name\": user_name,\n",
    "        \"occupation\": occupation,\n",
    "        \"day\": days,\n",
    "        \"platform\": platform,\n",
    "        \"content_type\": content_type,\n",
    "        \"email\": email\n",
    "    }\n",
    "    \n",
    "async def main():\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    print(\"\\n===== KnowlEdge系统启动 =====\")\n",
    "\n",
    "    # 验证数据库\n",
    "    if not verify_database():\n",
    "        print(\"数据库验证失败，系统可能无法正常工作\")\n",
    "        choice = input(\"是否继续运行? (y/n): \").strip().lower()\n",
    "        if choice != 'y':\n",
    "            print(\"系统退出\")\n",
    "            return\n",
    "\n",
    "    # 检查系统初始化状态\n",
    "    if not os.path.exists(CONFIG[\"DATA_DIR\"]) or not os.path.exists(CONFIG[\"DB_PATH\"]):\n",
    "        print(\"系统尚未初始化，正在进行初始化...\")\n",
    "        # 导入并运行初始化脚本\n",
    "        try:\n",
    "            import init_system\n",
    "            init_result = init_system.main()\n",
    "            if not init_result:\n",
    "                print(\"系统初始化失败，请检查日志并解决问题后重试\")\n",
    "                return\n",
    "        except ImportError:\n",
    "            print(\"找不到初始化脚本，请确保init_system.py文件存在\")\n",
    "            return\n",
    "\n",
    "    workflow = KnowledgeFlow()\n",
    "    print(\"KnowledgeFlow引擎已初始化\")\n",
    "\n",
    "    # 步骤 1：收集用户输入\n",
    "    print(\"\\n步骤 1/6: 收集用户信息\")\n",
    "    user_input = collect_user_input()\n",
    "    workflow.start_node(user_input)\n",
    "    print(\"用户信息已收集并处理\")\n",
    "\n",
    "    # 步骤 2：分析用户画像（可选）\n",
    "    print(\"\\n步骤 2/6: 用户画像分析\")\n",
    "    resume_reader = ResumeReader()\n",
    "    print(\"请提供您的简历以进行更精确的用户画像分析（可选）\")\n",
    "    cv_text = resume_reader.read_resume()\n",
    "\n",
    "    # 构建用户画像\n",
    "    workflow.build_user_profile(user_input, cv_text)\n",
    "\n",
    "    # 步骤 3：构建搜索参数\n",
    "    print(\"\\n步骤 3/6: 构建搜索参数\")\n",
    "    print(\"正在根据您的需求和兴趣构建搜索参数...\")\n",
    "    queries = await workflow.build_search_query()\n",
    "    print(f\"搜索参数构建完成，将在以下平台搜索: {', '.join(queries.keys())}\")\n",
    "\n",
    "\n",
    "    # 步骤 4：执行Google搜索、ArXiv搜索、和Google搜索ArXiv文献\n",
    "    print(\"\\n步骤 4/6: 执行搜索\")\n",
    "    print(f\"正在搜索与{user_input['content_type']}相关的最新信息...\")\n",
    "    search_results = workflow.execute_search(queries)\n",
    "    result_count = sum(len(data.get('results', [])) for source, data in search_results.items() if 'results' in data)\n",
    "    print(f\"搜索完成，共找到 {result_count} 条相关信息\")\n",
    "\n",
    "    # 步骤 5：生成报告\n",
    "    print(\"\\n步骤 5/6: 生成报告\")\n",
    "    print(\"正在整合搜索结果并生成报告...\")\n",
    "    report = workflow.generate_report(search_results)\n",
    "    print(\"报告生成完成\")\n",
    "\n",
    "    # 步骤 6：发送邮件\n",
    "    print(\"\\n步骤 6/6: 发送报告\")\n",
    "    workflow.send_email(report)\n",
    "\n",
    "    print(\"流程执行完成\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b0e6bdca43999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
