{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef6044-3020-4ec5-b314-07833bdd811f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T02:43:42.835946Z",
     "start_time": "2025-03-12T02:42:28.822062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:36:41,601 - DEBUG - Resetting dropped connection: huggingface.co\n",
      "2025-03-12 16:36:44,333 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-03-12 16:36:45,162 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-03-12 16:36:46,475 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== KnowlEdge系统启动 =====\n",
      "\n",
      "验证数据库...\n",
      "数据库验证成功：可以正常写入和读取数据\n",
      "数据库包含以下表: users, user_interests, sqlite_sequence, user_searches, user_interactions, user_skills\n",
      "表 users: 1 条记录\n",
      "表 user_interests: 0 条记录\n",
      "表 sqlite_sequence: 3 条记录\n",
      "表 user_searches: 2 条记录\n",
      "表 user_interactions: 14 条记录\n",
      "表 user_skills: 96 条记录\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:36:47,451 - INFO - 已加载兴趣分类体系，共 6 个类别\n",
      "2025-03-12 16:36:47,452 - INFO - 用户画像管理器初始化完成\n",
      "2025-03-12 16:36:47,452 - INFO - 开始收集用户输入信息...\n",
      "2025-03-12 16:36:47,452 - DEBUG - 用户输入信息: {'user_name': 'Tssword', 'occupation': '算法工程师', 'day': 10, 'platform': '学术期刊', 'content_type': '大语言模型', 'email': '114514@qq.com'}\n",
      "2025-03-12 16:36:47,455 - INFO - 用户已存在: 255d6efd2268b69a25f4c51aae008660\n",
      "2025-03-12 16:36:47,455 - INFO - 计算时间范围...\n",
      "2025-03-12 16:36:47,456 - DEBUG - 时间范围计算结果: 起始日期=2025-03-02 08:36:47.456624+00:00, 结束日期=2025-03-12 08:36:47.456624+00:00\n",
      "2025-03-12 16:36:47,457 - INFO - 初始化ResumeReader，支持的格式：['.txt', '.pdf', '.docx', '.doc', '.xlsx', '.xls', '.jpg', '.jpeg', '.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeFlow引擎已初始化\n",
      "\n",
      "步骤 1/6: 收集用户信息\n",
      "\n",
      "===== 欢迎使用KnowlEdge系统 =====\n",
      "请提供以下信息，以便我们为您提供个性化的行业知识更新\n",
      "\n",
      "您的信息已收集完毕，系统将基于这些信息为您提供个性化服务\n",
      "用户信息已收集并处理\n",
      "\n",
      "步骤 2/6: 用户画像分析\n",
      "请提供您的简历以进行更精确的用户画像分析（可选）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:36:50,733 - INFO - 构建搜索查询并进行翻译...\n",
      "2025-03-12 16:36:50,737 - DEBUG - Starting new HTTPS connection (1): translate.google.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未提供简历，但仍将创建基本用户档案\n",
      "\n",
      "步骤 3/6: 构建搜索参数\n",
      "正在根据您的需求和兴趣构建搜索参数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:36:51,755 - DEBUG - https://translate.google.com:443 \"GET /m?tl=en&sl=auto&q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B HTTP/1.1\" 200 None\n",
      "2025-03-12 16:36:51,801 - DEBUG - Starting new HTTPS connection (1): fanyi-api.baidu.com:443\n",
      "2025-03-12 16:36:52,054 - DEBUG - https://fanyi-api.baidu.com:443 \"POST /api/trans/vip/translate?appid=YOUR_ID&q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&from=en&to=zh&salt=48677&sign=546c0204bec72bcbff05f466fb160850 HTTP/1.1\" 200 54\n",
      "2025-03-12 16:36:52,056 - ERROR - 百度翻译失败: Baidu API returned the following error: UNAUTHORIZED USER\n",
      "2025-03-12 16:36:52,061 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是专业翻译验证助手。回答只用翻译后的内容本身！以下是不同引擎翻译的结果，请你思考它们从中文到英文翻译的准确性，并提供最准确的翻译。最终结果只用翻译后的内容本身。'}, {'role': 'user', 'content': '原文：大语言模型\\n谷歌翻译：Big language model'}], 'model': 'deepseek-chat'}}\n",
      "2025-03-12 16:36:52,061 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-03-12 16:36:52,062 - DEBUG - connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=5.0 socket_options=None\n",
      "2025-03-12 16:36:52,063 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000225F3F5C5C0>\n",
      "2025-03-12 16:36:52,063 - DEBUG - send_request_headers.started request=<Request [b'CONNECT']>\n",
      "2025-03-12 16:36:52,064 - DEBUG - send_request_headers.complete\n",
      "2025-03-12 16:36:52,064 - DEBUG - send_request_body.started request=<Request [b'CONNECT']>\n",
      "2025-03-12 16:36:52,065 - DEBUG - send_request_body.complete\n",
      "2025-03-12 16:36:52,066 - DEBUG - receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "2025-03-12 16:36:52,067 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "2025-03-12 16:36:52,067 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000225F4547D50> server_hostname='api.deepseek.com' timeout=5.0\n",
      "2025-03-12 16:36:52,140 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000225F3FAE480>\n",
      "2025-03-12 16:36:52,142 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:52,143 - DEBUG - send_request_headers.complete\n",
      "2025-03-12 16:36:52,143 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:52,143 - DEBUG - send_request_body.complete\n",
      "2025-03-12 16:36:52,144 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:52,195 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Mar 2025 08:36:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Set-Cookie', b'HWWAFSESID=2f2105356f41e75b410; path=/'), (b'Set-Cookie', b'HWWAFSESTIME=1741768608563; path=/'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'2edc9af65ae313c2760e9f0b1da10a78'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-03-12 16:36:52,196 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:36:52,197 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:58,886 - DEBUG - receive_response_body.complete\n",
      "2025-03-12 16:36:58,887 - DEBUG - response_closed.started\n",
      "2025-03-12 16:36:58,889 - DEBUG - response_closed.complete\n",
      "2025-03-12 16:36:58,891 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers([('date', 'Wed, 12 Mar 2025 08:36:52 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('set-cookie', 'HWWAFSESID=2f2105356f41e75b410; path=/'), ('set-cookie', 'HWWAFSESTIME=1741768608563; path=/'), ('vary', 'origin, access-control-request-method, access-control-request-headers'), ('access-control-allow-credentials', 'true'), ('x-ds-trace-id', '2edc9af65ae313c2760e9f0b1da10a78'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('x-content-type-options', 'nosniff'), ('content-encoding', 'gzip'), ('server', 'elb')])\n",
      "2025-03-12 16:36:58,892 - DEBUG - request_id: None\n",
      "2025-03-12 16:36:58,898 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专业的主题提取助手，擅长从文本中识别核心主题。'}, {'role': 'user', 'content': '\\n        请从以下搜索查询中提取最多3个主要的兴趣领域或关键主题。\\n        只返回提取的主题列表，格式为JSON数组，例如：[\"人工智能\", \"机器学习\", \"自然语言处理\"]\\n\\n        搜索查询: 大语言模型\\n        '}], 'model': 'deepseek-chat'}}\n",
      "2025-03-12 16:36:58,900 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-03-12 16:36:58,903 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:58,904 - DEBUG - send_request_headers.complete\n",
      "2025-03-12 16:36:58,905 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:58,906 - DEBUG - send_request_body.complete\n",
      "2025-03-12 16:36:58,907 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-03-12 16:36:58,964 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Mar 2025 08:36:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'e84a54ada75546f5c734a30a3fc24f5e'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-03-12 16:36:58,964 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:36:58,965 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-03-12 16:37:13,238 - DEBUG - receive_response_body.complete\n",
      "2025-03-12 16:37:13,240 - DEBUG - response_closed.started\n",
      "2025-03-12 16:37:13,241 - DEBUG - response_closed.complete\n",
      "2025-03-12 16:37:13,243 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers({'date': 'Wed, 12 Mar 2025 08:36:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-allow-credentials': 'true', 'x-ds-trace-id': 'e84a54ada75546f5c734a30a3fc24f5e', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'content-encoding': 'gzip', 'server': 'elb'})\n",
      "2025-03-12 16:37:13,244 - DEBUG - request_id: None\n",
      "2025-03-12 16:37:13,246 - ERROR - 提取查询主题时出错: 'NoneType' object has no attribute 'items'\n",
      "2025-03-12 16:37:13,501 - DEBUG - 谷歌翻译为：Big language model，相似度: 0.7646453380584717\n",
      "2025-03-12 16:37:13,554 - DEBUG - 大模型翻译为：Large language model，相似度: 0.5579333901405334\n",
      "2025-03-12 16:37:13,554 - INFO - 最终翻译结果:Big language model ，相似度是：0.7646453380584717\n",
      "2025-03-12 16:37:13,555 - DEBUG - 构建的搜索查询：query_google=Large language model after:2025-03-02 before:2025-03-12 site:google.com\n",
      "2025-03-12 16:37:13,561 - INFO - 记录用户搜索: 255d6efd2268b69a25f4c51aae008660, 查询: 大语言模型\n",
      "2025-03-12 16:37:13,562 - INFO - 执行搜索操作...\n",
      "2025-03-12 16:37:13,562 - INFO - 用户选择学术期刊类，执行Google_ArXiv搜索...\n",
      "2025-03-12 16:37:13,562 - DEBUG - 执行Google_ArXiv搜索: Large language model after:2025-03-02 before:2025-03-12 arXiv site:arxiv.org\n",
      "2025-03-12 16:37:13,563 - INFO - 执行ArXiv搜索，限制结果数量为7...\n",
      "2025-03-12 16:37:13,565 - DEBUG - Starting new HTTP connection (1): 127.0.0.1:7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索参数构建完成，将在以下平台搜索: query_google, query_arxiv, query_google_arxiv\n",
      "\n",
      "步骤 4/6: 执行搜索\n",
      "正在搜索与大语言模型相关的最新信息...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:37:15,781 - DEBUG - http://127.0.0.1:7890 \"GET http://export.arxiv.org/api/query?search_query=Large%20language%20model%20after:2025-03-02%20before:2025-03-12%20arXiv%20site:arxiv.org&start=0&max_results=7 HTTP/1.1\" 200 4915\n",
      "2025-03-12 16:37:17,440 - DEBUG - ArXiv搜索返回结果数量: 7\n",
      "2025-03-12 16:37:17,441 - INFO - 用户选择学术期刊类，执行ArXiv搜索...\n",
      "2025-03-12 16:37:17,441 - DEBUG - 执行ArXiv搜索: \"Large language model\" AND submittedDate:[20250302 TO 20250312]\n",
      "2025-03-12 16:37:17,441 - INFO - 执行ArXiv搜索，限制结果数量为7...\n",
      "2025-03-12 16:37:17,444 - DEBUG - Starting new HTTP connection (1): 127.0.0.1:7890\n",
      "2025-03-12 16:37:19,028 - DEBUG - http://127.0.0.1:7890 \"GET http://export.arxiv.org/api/query?search_query=%22Large%20language%20model%22%20AND%20submittedDate:%5B20250302%20TO%2020250312%5D&start=0&max_results=7 HTTP/1.1\" 200 4844\n",
      "2025-03-12 16:37:20,926 - DEBUG - ArXiv搜索返回结果数量: 7\n",
      "2025-03-12 16:37:20,927 - DEBUG - 搜索结果: {'google_arxiv': {'results': [{'title': 'PLMM: Personal Large Language Models on Mobile Devices', 'snippet': \"  Inspired by Federated Learning, in this paper, we propose personal large\\nmodels that are distilled from traditional large language models but more\\nadaptive to local users' personal information such as education background and\\nhobbies. We classify the large language models into three levels: the personal\\nlevel, expert level and traditional level. The personal level models are\\nadaptive to users' personal information. They encrypt the users' input and\\nprotect their privacy. The expert level models focus on merging specific\\nknowledge such as finance, IT and art. The traditional models focus on the\\nuniversal knowledge discovery and upgrading the expert models. In such\\nclassifications, the personal models directly interact with the user. For the\\nwhole system, the personal models have users' (encrypted) personal information.\\nMoreover, such models must be small enough to be performed on personal\\ncomputers or mobile devices. Finally, they also have to response in real-time\\nfor better user experience and produce high quality results. The proposed\\npersonal large models can be applied in a wide range of applications such as\\nlanguage and vision tasks.\\n\", 'link': 'http://arxiv.org/abs/2309.14726v2', 'similarity': np.float32(0.82576025)}, {'title': 'Beyond Segmentation: Road Network Generation with Multi-Modal LLMs', 'snippet': '  This paper introduces an innovative approach to road network generation\\nthrough the utilization of a multi-modal Large Language Model (LLM). Our model\\nis specifically designed to process aerial images of road layouts and produce\\ndetailed, navigable road networks within the input images. The core innovation\\nof our system lies in the unique training methodology employed for the large\\nlanguage model to generate road networks as its output. This approach draws\\ninspiration from the BLIP-2 architecture arXiv:2301.12597, leveraging\\npre-trained frozen image encoders and large language models to create a\\nversatile multi-modal LLM.\\n  Our work also offers an alternative to the reasoning segmentation method\\nproposed in the LISA paper arXiv:2308.00692. By training the large language\\nmodel with our approach, the necessity for generating binary segmentation\\nmasks, as suggested in the LISA paper arXiv:2308.00692, is effectively\\neliminated. Experimental results underscore the efficacy of our multi-modal LLM\\nin providing precise and valuable navigational guidance. This research\\nrepresents a significant stride in bolstering autonomous navigation systems,\\nespecially in road network scenarios, where accurate guidance is of paramount\\nimportance.\\n', 'link': 'http://arxiv.org/abs/2310.09755v1', 'similarity': np.float32(0.78932506)}, {'title': 'Language Transfer of Audio Word2Vec: Learning Audio Segment\\n  Representations without Target Language Data', 'snippet': '  Audio Word2Vec offers vector representations of fixed dimensionality for\\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\\nThese vector representations are shown to describe the sequential phonetic\\nstructures of the audio segments to a good degree, with real world applications\\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\\ncapability of language transfer of Audio Word2Vec. We train SA from one\\nlanguage (source language) and use it to extract the vector representation of\\nthe audio segments of another language (target language). We found that SA can\\nstill catch phonetic structure from the audio segments of the target language\\nif the source and target languages are similar. In query-by-example STD, we\\nobtain the vector representations from the SA learned from a large amount of\\nsource language data, and found them surpass the representations from naive\\nencoder and SA directly learned from a small amount of target language data.\\nThe result shows that it is possible to learn Audio Word2Vec model from\\nhigh-resource languages and use it on low-resource languages. This further\\nexpands the usability of Audio Word2Vec.\\n', 'link': 'http://arxiv.org/abs/1707.06519v1', 'similarity': np.float32(0.7415923)}, {'title': 'Mathematical Language Models: A Survey', 'snippet': '  In recent years, there has been remarkable progress in leveraging Language\\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\\nlandscape reveals a large number of proposed mathematical LLMs, which are\\nfurther delineated into instruction learning, tool-based methods, fundamental\\nCoT techniques, advanced CoT methodologies and multi-modal methods. To\\ncomprehend the benefits of mathematical LMs more thoroughly, we carry out an\\nin-depth contrast of their characteristics and performance. In addition, our\\nsurvey entails the compilation of over 60 mathematical datasets, including\\ntraining datasets, benchmark datasets, and augmented datasets. Addressing the\\nprimary challenges and delineating future trajectories within the field of\\nmathematical LMs, this survey is poised to facilitate and inspire future\\ninnovation among researchers invested in advancing this domain.\\n', 'link': 'http://arxiv.org/abs/2312.07622v4', 'similarity': np.float32(0.7331765)}, {'title': 'Helping Language Models Learn More: Multi-dimensional Task Prompt for\\n  Few-shot Tuning', 'snippet': \"  Large language models (LLMs) can be used as accessible and intelligent\\nchatbots by constructing natural language queries and directly inputting the\\nprompt into the large language model. However, different prompt' constructions\\noften lead to uncertainty in the answers and thus make it hard to utilize the\\nspecific knowledge of LLMs (like ChatGPT). To alleviate this, we use an\\ninterpretable structure to explain the prompt learning principle in LLMs, which\\ncertificates that the effectiveness of language models is determined by\\nposition changes of the task's related tokens. Therefore, we propose MTPrompt,\\na multi-dimensional task prompt learning method consisting based on\\ntask-related object, summary, and task description information. By\\nautomatically building and searching for appropriate prompts, our proposed\\nMTPrompt achieves the best results on few-shot samples setting and five\\ndifferent datasets. In addition, we demonstrate the effectiveness and stability\\nof our method in different experimental settings and ablation experiments. In\\ninteraction with large language models, embedding more task-related information\\ninto prompts will make it easier to stimulate knowledge embedded in large\\nlanguage models.\\n\", 'link': 'http://arxiv.org/abs/2312.08027v1', 'similarity': np.float32(0.7234684)}, {'title': \"Using Large Language Models for (De-)Formalization and Natural\\n  Argumentation Exercises for Beginner's Students\", 'snippet': '  We describe two systems currently being developed that use large language\\nmodels for the automatized correction of (i) exercises in translating back and\\nforth between natural language and the languages of propositional logic and\\nfirst-order predicate logic and (ii) exercises in writing simple arguments in\\nnatural language in non-mathematical scenarios.\\n', 'link': 'http://arxiv.org/abs/2304.06186v3', 'similarity': np.float32(0.6379888)}, {'title': 'Symbolic and Language Agnostic Large Language Models', 'snippet': '  We argue that the relative success of large language models (LLMs) is not a\\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\\nan appropriate strategy of bottom-up reverse engineering of language at scale.\\nHowever, due to the subsymbolic nature of these models whatever knowledge these\\nsystems acquire about language will always be buried in millions of\\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\\ndue to their stochastic nature, these models will often fail in capturing\\nvarious inferential aspects that are prevalent in natural language. What we\\nsuggest here is employing the successful bottom-up strategy in a symbolic\\nsetting, producing symbolic, language agnostic and ontologically grounded large\\nlanguage models.\\n', 'link': 'http://arxiv.org/abs/2308.14199v1', 'similarity': np.float32(0.6135644)}]}, 'arxiv': {'results': [{'title': 'AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for\\n  Large Language Models using Data Chunking', 'snippet': '  The Unlearning Sensitive Content from Large Language Models task aims to\\nremove targeted datapoints from trained models while minimally affecting their\\ngeneral knowledge. In our work, we leverage parameter-efficient, gradient-based\\nunlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To\\nfurther enhance unlearning effectiveness, we employ data chunking, splitting\\nforget data into disjoint partitions and merging them with cyclically sampled\\nretain samples at a pre-defined ratio. Our task-agnostic method achieves an\\noutstanding forget-retain balance, ranking first on leaderboards and\\nsignificantly outperforming baselines and competing systems.\\n', 'link': 'http://arxiv.org/abs/2503.02443v1', 'similarity': np.float32(0.8754001)}, {'title': 'Large Language Models in Bioinformatics: A Survey', 'snippet': '  Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\\nprovides a systematic review of recent advancements, focusing on genomic\\nsequence modeling, RNA structure prediction, protein function inference, and\\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\\nincluding data scarcity, computational complexity, and cross-omics integration,\\nand explore future directions such as multimodal learning, hybrid AI models,\\nand clinical applications. By offering a comprehensive perspective, this paper\\nunderscores the transformative potential of LLMs in driving innovations in\\nbioinformatics and precision medicine.\\n', 'link': 'http://arxiv.org/abs/2503.04490v1', 'similarity': np.float32(0.80325955)}, {'title': 'A Survey of Large Language Model Empowered Agents for Recommendation and\\n  Search: Towards Next-Generation Information Retrieval', 'snippet': '  Information technology has profoundly altered the way humans interact with\\ninformation. The vast amount of content created, shared, and disseminated\\nonline has made it increasingly difficult to access relevant information. Over\\nthe past two decades, search and recommendation systems (collectively referred\\nto as information retrieval systems) have evolved significantly to address\\nthese challenges. Recent advances in large language models (LLMs) have\\ndemonstrated capabilities that surpass human performance in various\\nlanguage-related tasks and exhibit general understanding, reasoning, and\\ndecision-making abilities. This paper explores the transformative potential of\\nlarge language model agents in enhancing search and recommendation systems. We\\ndiscuss the motivations and roles of LLM agents, and establish a classification\\nframework to elaborate on the existing research. We highlight the immense\\npotential of LLM agents in addressing current challenges in search and\\nrecommendation, providing insights into future research directions. This paper\\nis the first to systematically review and classify the research on LLM agents\\nin these domains, offering a novel perspective on leveraging this advanced AI\\ntechnology for information retrieval. To help understand the existing works, we\\nlist the existing papers on agent-based simulation with large language models\\nat this link:\\nhttps://github', 'link': 'http://arxiv.org/abs/2503.05659v1', 'similarity': np.float32(0.7374977)}, {'title': 'Control Flow-Augmented Decompiler based on Large Language Model', 'snippet': '  Binary decompilation plays a crucial role in various tasks related to\\nsecurity threat analysis and software engineering, such as binary vulnerability\\ndetection and software supply chain analysis. Current prevalent binary\\ndecompilation methods primarily rely on large language models (LLMs) and can be\\nbroadly classified into two main approaches: prompt-based decompilation and\\nend-toend decompilation. Prompt-based methods typically require significant\\neffort to analyze and summarize the predicted data to extract aspect-specific\\nexpert knowledge, which is then fed into a general purpose large language model\\nto address specific decompilation tasks. End-to-end methods, on the other hand,\\ncarefully construct training datasets or neural networks to perform\\npost-training on general-purpose large language models, thereby obtaining\\ndomain-specific large language models for decompiling the predicted data.\\nHowever, both existing approaches still face significant challenges, including\\nthe absence of rich semantic representations of the input code and the neglect\\nof control flow information, which is crucial for accurate decompilation.\\nFurthermore, most current decompilation techniques are specifically tailored\\nfor the x86 architecture, making it difficult to efficiently adapt and\\ngeneralize them to other bit width or instruction architectures. To address\\nthese limitations, we propose a nov', 'link': 'http://arxiv.org/abs/2503.07215v1', 'similarity': np.float32(0.6829603)}, {'title': 'Large Language Model Guided Progressive Feature Alignment for Multimodal\\n  UAV Object Detection', 'snippet': '  Existing multimodal UAV object detection methods often overlook the impact of\\nsemantic gaps between modalities, which makes it difficult to achieve accurate\\nsemantic and spatial alignments, limiting detection performance. To address\\nthis problem, we propose a Large Language Model (LLM) guided Progressive\\nfeature Alignment Network called LPANet, which leverages the semantic features\\nextracted from a large language model to guide the progressive semantic and\\nspatial alignment between modalities for multimodal UAV object detection. To\\nemploy the powerful semantic representation of LLM, we generate the\\nfine-grained text descriptions of each object category by ChatGPT and then\\nextract the semantic features using the large language model MPNet. Based on\\nthe semantic features, we guide the semantic and spatial alignments in a\\nprogressive manner as follows. First, we design the Semantic Alignment Module\\n(SAM) to pull the semantic features and multimodal visual features of each\\nobject closer, alleviating the semantic differences of objects between\\nmodalities. Second, we design the Explicit Spatial alignment Module (ESM) by\\nintegrating the semantic relations into the estimation of feature-level\\noffsets, alleviating the coarse spatial misalignment between modalities.\\nFinally, we design the Implicit Spatial alignment Module (ISM), which leverages\\nthe cross-modal correlations to aggregate', 'link': 'http://arxiv.org/abs/2503.06948v1', 'similarity': np.float32(0.6738837)}, {'title': 'Maximum Hallucination Standards for Domain-Specific Large Language\\n  Models', 'snippet': '  Large language models (LLMs) often generate inaccurate yet credible-sounding\\ncontent, known as hallucinations. This inherent feature of LLMs poses\\nsignificant risks, especially in critical domains. I analyze LLMs as a new\\nclass of engineering products, treating hallucinations as a product attribute.\\nI demonstrate that, in the presence of imperfect awareness of LLM\\nhallucinations and misinformation externalities, net welfare improves when the\\nmaximum acceptable level of LLM hallucinations is designed to vary with two\\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\\nand the marginal damage associated with misinformation.\\n', 'link': 'http://arxiv.org/abs/2503.05481v1', 'similarity': np.float32(0.6386293)}, {'title': 'Can Large Language Models Predict Antimicrobial Resistance Gene?', 'snippet': '  This study demonstrates that generative large language models can be utilized\\nin a more flexible manner for DNA sequence analysis and classification tasks\\ncompared to traditional transformer encoder-based models. While recent\\nencoder-based models such as DNABERT and Nucleotide Transformer have shown\\nsignificant performance in DNA sequence classification, transformer\\ndecoder-based generative models have not yet been extensively explored in this\\nfield. This study evaluates how effectively generative Large Language Models\\nhandle DNA sequences with various labels and analyzes performance changes when\\nadditional textual information is provided. Experiments were conducted on\\nantimicrobial resistance genes, and the results show that generative Large\\nLanguage Models can offer comparable or potentially better predictions,\\ndemonstrating flexibility and accuracy when incorporating both sequence and\\ntextual information. The code and data used in this work are available at the\\nfollowing GitHub repository: https://github.com/biocomgit/llm4dna.\\n', 'link': 'http://arxiv.org/abs/2503.04413v1', 'similarity': np.float32(0.45512992)}]}}\n",
      "2025-03-12 16:37:20,928 - INFO - 生成最终的搜索报告...\n",
      "2025-03-12 16:37:20,928 - INFO - 调用大语言模型进行整合搜索结果...\n",
      "2025-03-12 16:37:20,930 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '你是一个内容整理助手，负责根据用户提供的信息整合搜索结果并生成报告。使用中文。不要md格式。将报告中的内容，以：\"来源：...（Google/arXiv/Google_arXiv等平台） \\n 标题：... \\n 摘要：... \\n 原文网址：...（只使用搜索结果中提供的真实链接）\\n BERT嵌入的余弦相似度:... \"的形式呈现出来。如果搜索结果中没有提供原文网址，则写\\'原文网址：未提供\\'。不要编造或猜测网址。报告使用用户交谈时的语言，如果原文不是，则准确的转化为用户使用的语言。目前的用户使用的是中文，将结果也转化为中文！如果无法完成就直接翻译用snippet的内容回答将\"摘要：\"改为\"片段：\"。并且回答严格按照规范来，就算无法完成任务也不要说别的不符合规范的话。'}, {'role': 'user', 'content': '请整合以下搜索结果并生成最终报告：{\"google_arxiv\": {\"results\": [{\"title\": \"PLMM: Personal Large Language Models on Mobile Devices\", \"snippet\": \"  Inspired by Federated Learning, in this paper, we propose personal large\\\\nmodels that are distilled from traditional large language models but more\\\\nadaptive to local users\\' personal information such as education background and\\\\nhobbies. We classify the large language models into three levels: the personal\\\\nlevel, expert level and traditional level. The personal level models are\\\\nadaptive to users\\' personal information. They encrypt the users\\' input and\\\\nprotect their privacy. The expert level models focus on merging specific\\\\nknowledge such as finance, IT and art. The traditional models focus on the\\\\nuniversal knowledge discovery and upgrading the expert models. In such\\\\nclassifications, the personal models directly interact with the user. For the\\\\nwhole system, the personal models have users\\' (encrypted) personal information.\\\\nMoreover, such models must be small enough to be performed on personal\\\\ncomputers or mobile devices. Finally, they also have to response in real-time\\\\nfor better user experience and produce high quality results. The proposed\\\\npersonal large models can be applied in a wide range of applications such as\\\\nlanguage and vision tasks.\\\\n\", \"link\": \"http://arxiv.org/abs/2309.14726v2\", \"similarity\": 0.8257602453231812}, {\"title\": \"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs\", \"snippet\": \"  This paper introduces an innovative approach to road network generation\\\\nthrough the utilization of a multi-modal Large Language Model (LLM). Our model\\\\nis specifically designed to process aerial images of road layouts and produce\\\\ndetailed, navigable road networks within the input images. The core innovation\\\\nof our system lies in the unique training methodology employed for the large\\\\nlanguage model to generate road networks as its output. This approach draws\\\\ninspiration from the BLIP-2 architecture arXiv:2301.12597, leveraging\\\\npre-trained frozen image encoders and large language models to create a\\\\nversatile multi-modal LLM.\\\\n  Our work also offers an alternative to the reasoning segmentation method\\\\nproposed in the LISA paper arXiv:2308.00692. By training the large language\\\\nmodel with our approach, the necessity for generating binary segmentation\\\\nmasks, as suggested in the LISA paper arXiv:2308.00692, is effectively\\\\neliminated. Experimental results underscore the efficacy of our multi-modal LLM\\\\nin providing precise and valuable navigational guidance. This research\\\\nrepresents a significant stride in bolstering autonomous navigation systems,\\\\nespecially in road network scenarios, where accurate guidance is of paramount\\\\nimportance.\\\\n\", \"link\": \"http://arxiv.org/abs/2310.09755v1\", \"similarity\": 0.7893250584602356}, {\"title\": \"Language Transfer of Audio Word2Vec: Learning Audio Segment\\\\n  Representations without Target Language Data\", \"snippet\": \"  Audio Word2Vec offers vector representations of fixed dimensionality for\\\\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\\\\nThese vector representations are shown to describe the sequential phonetic\\\\nstructures of the audio segments to a good degree, with real world applications\\\\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\\\\ncapability of language transfer of Audio Word2Vec. We train SA from one\\\\nlanguage (source language) and use it to extract the vector representation of\\\\nthe audio segments of another language (target language). We found that SA can\\\\nstill catch phonetic structure from the audio segments of the target language\\\\nif the source and target languages are similar. In query-by-example STD, we\\\\nobtain the vector representations from the SA learned from a large amount of\\\\nsource language data, and found them surpass the representations from naive\\\\nencoder and SA directly learned from a small amount of target language data.\\\\nThe result shows that it is possible to learn Audio Word2Vec model from\\\\nhigh-resource languages and use it on low-resource languages. This further\\\\nexpands the usability of Audio Word2Vec.\\\\n\", \"link\": \"http://arxiv.org/abs/1707.06519v1\", \"similarity\": 0.741592288017273}, {\"title\": \"Mathematical Language Models: A Survey\", \"snippet\": \"  In recent years, there has been remarkable progress in leveraging Language\\\\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\\\\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\\\\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\\\\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\\\\nlandscape reveals a large number of proposed mathematical LLMs, which are\\\\nfurther delineated into instruction learning, tool-based methods, fundamental\\\\nCoT techniques, advanced CoT methodologies and multi-modal methods. To\\\\ncomprehend the benefits of mathematical LMs more thoroughly, we carry out an\\\\nin-depth contrast of their characteristics and performance. In addition, our\\\\nsurvey entails the compilation of over 60 mathematical datasets, including\\\\ntraining datasets, benchmark datasets, and augmented datasets. Addressing the\\\\nprimary challenges and delineating future trajectories within the field of\\\\nmathematical LMs, this survey is poised to facilitate and inspire future\\\\ninnovation among researchers invested in advancing this domain.\\\\n\", \"link\": \"http://arxiv.org/abs/2312.07622v4\", \"similarity\": 0.7331765294075012}, {\"title\": \"Helping Language Models Learn More: Multi-dimensional Task Prompt for\\\\n  Few-shot Tuning\", \"snippet\": \"  Large language models (LLMs) can be used as accessible and intelligent\\\\nchatbots by constructing natural language queries and directly inputting the\\\\nprompt into the large language model. However, different prompt\\' constructions\\\\noften lead to uncertainty in the answers and thus make it hard to utilize the\\\\nspecific knowledge of LLMs (like ChatGPT). To alleviate this, we use an\\\\ninterpretable structure to explain the prompt learning principle in LLMs, which\\\\ncertificates that the effectiveness of language models is determined by\\\\nposition changes of the task\\'s related tokens. Therefore, we propose MTPrompt,\\\\na multi-dimensional task prompt learning method consisting based on\\\\ntask-related object, summary, and task description information. By\\\\nautomatically building and searching for appropriate prompts, our proposed\\\\nMTPrompt achieves the best results on few-shot samples setting and five\\\\ndifferent datasets. In addition, we demonstrate the effectiveness and stability\\\\nof our method in different experimental settings and ablation experiments. In\\\\ninteraction with large language models, embedding more task-related information\\\\ninto prompts will make it easier to stimulate knowledge embedded in large\\\\nlanguage models.\\\\n\", \"link\": \"http://arxiv.org/abs/2312.08027v1\", \"similarity\": 0.7234684228897095}, {\"title\": \"Using Large Language Models for (De-)Formalization and Natural\\\\n  Argumentation Exercises for Beginner\\'s Students\", \"snippet\": \"  We describe two systems currently being developed that use large language\\\\nmodels for the automatized correction of (i) exercises in translating back and\\\\nforth between natural language and the languages of propositional logic and\\\\nfirst-order predicate logic and (ii) exercises in writing simple arguments in\\\\nnatural language in non-mathematical scenarios.\\\\n\", \"link\": \"http://arxiv.org/abs/2304.06186v3\", \"similarity\": 0.637988805770874}, {\"title\": \"Symbolic and Language Agnostic Large Language Models\", \"snippet\": \"  We argue that the relative success of large language models (LLMs) is not a\\\\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\\\\nan appropriate strategy of bottom-up reverse engineering of language at scale.\\\\nHowever, due to the subsymbolic nature of these models whatever knowledge these\\\\nsystems acquire about language will always be buried in millions of\\\\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\\\\ndue to their stochastic nature, these models will often fail in capturing\\\\nvarious inferential aspects that are prevalent in natural language. What we\\\\nsuggest here is employing the successful bottom-up strategy in a symbolic\\\\nsetting, producing symbolic, language agnostic and ontologically grounded large\\\\nlanguage models.\\\\n\", \"link\": \"http://arxiv.org/abs/2308.14199v1\", \"similarity\": 0.6135643720626831}]}, \"arxiv\": {\"results\": [{\"title\": \"AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for\\\\n  Large Language Models using Data Chunking\", \"snippet\": \"  The Unlearning Sensitive Content from Large Language Models task aims to\\\\nremove targeted datapoints from trained models while minimally affecting their\\\\ngeneral knowledge. In our work, we leverage parameter-efficient, gradient-based\\\\nunlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To\\\\nfurther enhance unlearning effectiveness, we employ data chunking, splitting\\\\nforget data into disjoint partitions and merging them with cyclically sampled\\\\nretain samples at a pre-defined ratio. Our task-agnostic method achieves an\\\\noutstanding forget-retain balance, ranking first on leaderboards and\\\\nsignificantly outperforming baselines and competing systems.\\\\n\", \"link\": \"http://arxiv.org/abs/2503.02443v1\", \"similarity\": 0.8754001259803772}, {\"title\": \"Large Language Models in Bioinformatics: A Survey\", \"snippet\": \"  Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\\\\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\\\\nprovides a systematic review of recent advancements, focusing on genomic\\\\nsequence modeling, RNA structure prediction, protein function inference, and\\\\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\\\\nincluding data scarcity, computational complexity, and cross-omics integration,\\\\nand explore future directions such as multimodal learning, hybrid AI models,\\\\nand clinical applications. By offering a comprehensive perspective, this paper\\\\nunderscores the transformative potential of LLMs in driving innovations in\\\\nbioinformatics and precision medicine.\\\\n\", \"link\": \"http://arxiv.org/abs/2503.04490v1\", \"similarity\": 0.803259551525116}, {\"title\": \"A Survey of Large Language Model Empowered Agents for Recommendation and\\\\n  Search: Towards Next-Generation Information Retrieval\", \"snippet\": \"  Information technology has profoundly altered the way humans interact with\\\\ninformation. The vast amount of content created, shared, and disseminated\\\\nonline has made it increasingly difficult to access relevant information. Over\\\\nthe past two decades, search and recommendation systems (collectively referred\\\\nto as information retrieval systems) have evolved significantly to address\\\\nthese challenges. Recent advances in large language models (LLMs) have\\\\ndemonstrated capabilities that surpass human performance in various\\\\nlanguage-related tasks and exhibit general understanding, reasoning, and\\\\ndecision-making abilities. This paper explores the transformative potential of\\\\nlarge language model agents in enhancing search and recommendation systems. We\\\\ndiscuss the motivations and roles of LLM agents, and establish a classification\\\\nframework to elaborate on the existing research. We highlight the immense\\\\npotential of LLM agents in addressing current challenges in search and\\\\nrecommendation, providing insights into future research directions. This paper\\\\nis the first to systematically review and classify the research on LLM agents\\\\nin these domains, offering a novel perspective on leveraging this advanced AI\\\\ntechnology for information retrieval. To help understand the existing works, we\\\\nlist the existing papers on agent-based simulation with large language models\\\\nat this link:\\\\nhttps://github\", \"link\": \"http://arxiv.org/abs/2503.05659v1\", \"similarity\": 0.7374976873397827}, {\"title\": \"Control Flow-Augmented Decompiler based on Large Language Model\", \"snippet\": \"  Binary decompilation plays a crucial role in various tasks related to\\\\nsecurity threat analysis and software engineering, such as binary vulnerability\\\\ndetection and software supply chain analysis. Current prevalent binary\\\\ndecompilation methods primarily rely on large language models (LLMs) and can be\\\\nbroadly classified into two main approaches: prompt-based decompilation and\\\\nend-toend decompilation. Prompt-based methods typically require significant\\\\neffort to analyze and summarize the predicted data to extract aspect-specific\\\\nexpert knowledge, which is then fed into a general purpose large language model\\\\nto address specific decompilation tasks. End-to-end methods, on the other hand,\\\\ncarefully construct training datasets or neural networks to perform\\\\npost-training on general-purpose large language models, thereby obtaining\\\\ndomain-specific large language models for decompiling the predicted data.\\\\nHowever, both existing approaches still face significant challenges, including\\\\nthe absence of rich semantic representations of the input code and the neglect\\\\nof control flow information, which is crucial for accurate decompilation.\\\\nFurthermore, most current decompilation techniques are specifically tailored\\\\nfor the x86 architecture, making it difficult to efficiently adapt and\\\\ngeneralize them to other bit width or instruction architectures. To address\\\\nthese limitations, we propose a nov\", \"link\": \"http://arxiv.org/abs/2503.07215v1\", \"similarity\": 0.6829602718353271}, {\"title\": \"Large Language Model Guided Progressive Feature Alignment for Multimodal\\\\n  UAV Object Detection\", \"snippet\": \"  Existing multimodal UAV object detection methods often overlook the impact of\\\\nsemantic gaps between modalities, which makes it difficult to achieve accurate\\\\nsemantic and spatial alignments, limiting detection performance. To address\\\\nthis problem, we propose a Large Language Model (LLM) guided Progressive\\\\nfeature Alignment Network called LPANet, which leverages the semantic features\\\\nextracted from a large language model to guide the progressive semantic and\\\\nspatial alignment between modalities for multimodal UAV object detection. To\\\\nemploy the powerful semantic representation of LLM, we generate the\\\\nfine-grained text descriptions of each object category by ChatGPT and then\\\\nextract the semantic features using the large language model MPNet. Based on\\\\nthe semantic features, we guide the semantic and spatial alignments in a\\\\nprogressive manner as follows. First, we design the Semantic Alignment Module\\\\n(SAM) to pull the semantic features and multimodal visual features of each\\\\nobject closer, alleviating the semantic differences of objects between\\\\nmodalities. Second, we design the Explicit Spatial alignment Module (ESM) by\\\\nintegrating the semantic relations into the estimation of feature-level\\\\noffsets, alleviating the coarse spatial misalignment between modalities.\\\\nFinally, we design the Implicit Spatial alignment Module (ISM), which leverages\\\\nthe cross-modal correlations to aggregate\", \"link\": \"http://arxiv.org/abs/2503.06948v1\", \"similarity\": 0.6738836765289307}, {\"title\": \"Maximum Hallucination Standards for Domain-Specific Large Language\\\\n  Models\", \"snippet\": \"  Large language models (LLMs) often generate inaccurate yet credible-sounding\\\\ncontent, known as hallucinations. This inherent feature of LLMs poses\\\\nsignificant risks, especially in critical domains. I analyze LLMs as a new\\\\nclass of engineering products, treating hallucinations as a product attribute.\\\\nI demonstrate that, in the presence of imperfect awareness of LLM\\\\nhallucinations and misinformation externalities, net welfare improves when the\\\\nmaximum acceptable level of LLM hallucinations is designed to vary with two\\\\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\\\\nand the marginal damage associated with misinformation.\\\\n\", \"link\": \"http://arxiv.org/abs/2503.05481v1\", \"similarity\": 0.6386293172836304}, {\"title\": \"Can Large Language Models Predict Antimicrobial Resistance Gene?\", \"snippet\": \"  This study demonstrates that generative large language models can be utilized\\\\nin a more flexible manner for DNA sequence analysis and classification tasks\\\\ncompared to traditional transformer encoder-based models. While recent\\\\nencoder-based models such as DNABERT and Nucleotide Transformer have shown\\\\nsignificant performance in DNA sequence classification, transformer\\\\ndecoder-based generative models have not yet been extensively explored in this\\\\nfield. This study evaluates how effectively generative Large Language Models\\\\nhandle DNA sequences with various labels and analyzes performance changes when\\\\nadditional textual information is provided. Experiments were conducted on\\\\nantimicrobial resistance genes, and the results show that generative Large\\\\nLanguage Models can offer comparable or potentially better predictions,\\\\ndemonstrating flexibility and accuracy when incorporating both sequence and\\\\ntextual information. The code and data used in this work are available at the\\\\nfollowing GitHub repository: https://github.com/biocomgit/llm4dna.\\\\n\", \"link\": \"http://arxiv.org/abs/2503.04413v1\", \"similarity\": 0.4551299214363098}]}}'}], 'model': 'deepseek-chat', 'stream': False}}\n",
      "2025-03-12 16:37:20,932 - DEBUG - Sending HTTP Request: POST https://api.deepseek.com/chat/completions\n",
      "2025-03-12 16:37:20,934 - DEBUG - connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=5.0 socket_options=None\n",
      "2025-03-12 16:37:20,935 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000225A61BEE10>\n",
      "2025-03-12 16:37:20,936 - DEBUG - send_request_headers.started request=<Request [b'CONNECT']>\n",
      "2025-03-12 16:37:20,936 - DEBUG - send_request_headers.complete\n",
      "2025-03-12 16:37:20,937 - DEBUG - send_request_body.started request=<Request [b'CONNECT']>\n",
      "2025-03-12 16:37:20,937 - DEBUG - send_request_body.complete\n",
      "2025-03-12 16:37:20,937 - DEBUG - receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "2025-03-12 16:37:21,135 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索完成，共找到 14 条相关信息\n",
      "\n",
      "步骤 5/6: 生成报告\n",
      "正在整合搜索结果并生成报告...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:37:21,135 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000225F4547D50> server_hostname='api.deepseek.com' timeout=5.0\n",
      "2025-03-12 16:37:21,999 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002260573C140>\n",
      "2025-03-12 16:37:22,001 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-03-12 16:37:22,002 - DEBUG - send_request_headers.complete\n",
      "2025-03-12 16:37:22,003 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-03-12 16:37:22,003 - DEBUG - send_request_body.complete\n",
      "2025-03-12 16:37:22,003 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-03-12 16:37:22,581 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Mar 2025 08:37:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'vary', b'origin, access-control-request-method, access-control-request-headers'), (b'access-control-allow-credentials', b'true'), (b'x-ds-trace-id', b'032c46cdb769fd4b7b6e029bdb7d6a35'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Content-Encoding', b'gzip'), (b'Server', b'elb')])\n",
      "2025-03-12 16:37:22,581 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:37:22,582 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-03-12 16:39:56,720 - DEBUG - receive_response_body.complete\n",
      "2025-03-12 16:39:56,723 - DEBUG - response_closed.started\n",
      "2025-03-12 16:39:56,724 - DEBUG - response_closed.complete\n",
      "2025-03-12 16:39:56,725 - DEBUG - HTTP Response: POST https://api.deepseek.com/chat/completions \"200 OK\" Headers({'date': 'Wed, 12 Mar 2025 08:37:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-allow-credentials': 'true', 'x-ds-trace-id': '032c46cdb769fd4b7b6e029bdb7d6a35', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'content-encoding': 'gzip', 'server': 'elb'})\n",
      "2025-03-12 16:39:56,725 - DEBUG - request_id: None\n",
      "2025-03-12 16:39:56,728 - DEBUG - 大语言模型整合结果: 来源：Google_arXiv  \n",
      "标题：PLMM: Personal Large Language Models on Mobile Devices  \n",
      "摘要：受联邦学习的启发，本文提出了个人大模型，这些模型从传统的大语言模型中蒸馏出来，但更能适应用户的个人信息，如教育背景和兴趣爱好。我们将大语言模型分为三个层次：个人层次、专家层次和传统层次。个人层次模型适应用户的个人信息，加密用户输入并保护隐私。专家层次模型专注于融合特定领域的知识，如金融、IT和艺术。传统模型则专注于通用知识的发现和专家模型的升级。在这种分类中，个人模型直接与用户交互，拥有用户的（加密）个人信息。此外，这些模型必须足够小，以便在个人计算机或移动设备上运行，并且需要实时响应以提供更好的用户体验和高质量的结果。所提出的个人大模型可以广泛应用于语言和视觉任务中。  \n",
      "原文网址：http://arxiv.org/abs/2309.14726v2  \n",
      "BERT嵌入的余弦相似度: 0.8257602453231812  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Beyond Segmentation: Road Network Generation with Multi-Modal LLMs  \n",
      "摘要：本文介绍了一种通过多模态大语言模型（LLM）生成道路网络的创新方法。该模型专门设计用于处理道路布局的航拍图像，并在输入图像中生成详细的、可导航的道路网络。该系统的核心创新在于用于训练大语言模型以生成道路网络的独特训练方法。该方法借鉴了BLIP-2架构，利用预训练的冻结图像编码器和大语言模型创建了一个多功能的LLM。我们的工作还提供了LISA论文中提出的推理分割方法的替代方案。通过我们的方法训练大语言模型，有效消除了生成二进制分割掩码的需求。实验结果证明了我们的多模态LLM在提供精确和有价值的导航指导方面的有效性。这项研究代表了在增强自主导航系统方面的重要进展，特别是在道路网络场景中，准确的导航指导至关重要。  \n",
      "原文网址：http://arxiv.org/abs/2310.09755v1  \n",
      "BERT嵌入的余弦相似度: 0.7893250584602356  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data  \n",
      "摘要：Audio Word2Vec通过序列到序列自编码器（SA）为可变长度的音频片段提供固定维度的向量表示。这些向量表示能够很好地描述音频片段的顺序语音结构，并在实际应用中如基于示例的语音术语检测（STD）中表现出色。本文探讨了Audio Word2Vec的语言迁移能力。我们从一个语言（源语言）训练SA，并使用它提取另一个语言（目标语言）的音频片段的向量表示。我们发现，如果源语言和目标语言相似，SA仍然可以从目标语言的音频片段中捕捉到语音结构。在基于示例的STD中，我们从大量源语言数据中学习的SA中获取向量表示，并发现这些表示优于从少量目标语言数据中直接学习的朴素编码器和SA的表示。结果表明，可以从高资源语言中学习Audio Word2Vec模型并将其用于低资源语言，这进一步扩展了Audio Word2Vec的可用性。  \n",
      "原文网址：http://arxiv.org/abs/1707.06519v1  \n",
      "BERT嵌入的余弦相似度: 0.741592288017273  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Mathematical Language Models: A Survey  \n",
      "摘要：近年来，在数学领域利用语言模型（LMs），包括预训练语言模型（PLMs）和大规模语言模型（LLMs），取得了显著进展。本文对数学LMs进行了全面调查，从任务和方法两个角度系统分类了关键研究工作。该领域揭示了大量提出的数学LLMs，进一步细分为指令学习、基于工具的方法、基础CoT技术、高级CoT方法和多模态方法。为了更深入地理解数学LMs的优势，我们对其特征和性能进行了深入对比。此外，我们的调查还包括了60多个数学数据集的汇编，包括训练数据集、基准数据集和增强数据集。针对数学LMs领域的主要挑战和未来发展方向，本调查旨在促进和启发未来研究者的创新。  \n",
      "原文网址：http://arxiv.org/abs/2312.07622v4  \n",
      "BERT嵌入的余弦相似度: 0.7331765294075012  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning  \n",
      "摘要：大语言模型（LLMs）可以通过构建自然语言查询并直接将提示输入到大语言模型中，用作可访问且智能的聊天机器人。然而，不同的提示构建通常会导致答案的不确定性，从而难以利用LLMs（如ChatGPT）的特定知识。为了缓解这一问题，我们使用可解释的结构来解释LLMs中的提示学习原理，该结构证明了语言模型的有效性由任务相关标记的位置变化决定。因此，我们提出了MTPrompt，一种基于任务相关对象、摘要和任务描述信息的多维任务提示学习方法。通过自动构建和搜索适当的提示，我们的MTPrompt在少样本设置和五个不同数据集上取得了最佳结果。此外，我们在不同的实验设置和消融实验中证明了该方法的有效性和稳定性。在与大语言模型的交互中，将更多任务相关信息嵌入提示中，将更容易激发大语言模型中嵌入的知识。  \n",
      "原文网址：http://arxiv.org/abs/2312.08027v1  \n",
      "BERT嵌入的余弦相似度: 0.7234684228897095  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner's Students  \n",
      "摘要：我们描述了两个正在开发的系统，它们使用大语言模型来自动纠正（i）在自然语言与命题逻辑和一阶谓词逻辑之间来回翻译的练习，以及（ii）在非数学场景中撰写简单自然语言论证的练习。  \n",
      "原文网址：http://arxiv.org/abs/2304.06186v3  \n",
      "BERT嵌入的余弦相似度: 0.637988805770874  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Symbolic and Language Agnostic Large Language Models  \n",
      "摘要：我们认为，大语言模型（LLMs）的相对成功并不是符号与子符号辩论的反映，而是反映了在规模上采用适当的自下而上逆向工程语言的策略。然而，由于这些模型的子符号性质，这些系统获得的关于语言的知识将始终隐藏在数百万个微特征（权重）中，没有一个是有意义的。此外，由于其随机性，这些模型经常无法捕捉自然语言中普遍存在的各种推理方面。我们建议在符号设置中采用成功的自下而上策略，生成符号化、语言无关且基于本体的大语言模型。  \n",
      "原文网址：http://arxiv.org/abs/2308.14199v1  \n",
      "BERT嵌入的余弦相似度: 0.6135643720626831  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking  \n",
      "摘要：从大语言模型中删除敏感内容的任务旨在从训练好的模型中删除目标数据点，同时最小化对其通用知识的影响。在我们的工作中，我们利用基于梯度的参数高效遗忘方法，使用低秩（LoRA）适应和分层微调。为了进一步增强遗忘效果，我们采用数据分块，将遗忘数据分割成不相交的分区，并以预定义的比率与循环采样的保留样本合并。我们的任务无关方法在遗忘-保留平衡方面表现出色，在排行榜上排名第一，显著优于基线和竞争系统。  \n",
      "原文网址：http://arxiv.org/abs/2503.02443v1  \n",
      "BERT嵌入的余弦相似度: 0.8754001259803772  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Large Language Models in Bioinformatics: A Survey  \n",
      "摘要：大语言模型（LLMs）正在革命化生物信息学，使得DNA、RNA、蛋白质和单细胞数据的高级分析成为可能。本调查系统回顾了最近的进展，重点关注基因组序列建模、RNA结构预测、蛋白质功能推断和单细胞转录组学。同时，我们还讨论了几个关键挑战，包括数据稀缺性、计算复杂性和跨组学整合，并探讨了未来方向，如多模态学习、混合AI模型和临床应用。通过提供全面的视角，本文强调了LLMs在推动生物信息学和精准医学创新方面的变革潜力。  \n",
      "原文网址：http://arxiv.org/abs/2503.04490v1  \n",
      "BERT嵌入的余弦相似度: 0.803259551525116  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval  \n",
      "摘要：信息技术深刻改变了人类与信息交互的方式。在线创建、共享和传播的大量内容使得访问相关信息变得越来越困难。在过去的二十年中，搜索和推荐系统（统称为信息检索系统）已经显著发展以应对这些挑战。最近的大语言模型（LLMs）进展展示了在各种语言相关任务中超越人类表现的能力，并表现出一般的理解、推理和决策能力。本文探讨了大语言模型代理在增强搜索和推荐系统中的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来阐述现有研究。我们强调了LLM代理在解决当前搜索和推荐挑战中的巨大潜力，为未来研究方向提供了见解。本文是第一个系统回顾和分类LLM代理在这些领域的研究，提供了利用这一先进AI技术进行信息检索的新视角。  \n",
      "原文网址：http://arxiv.org/abs/2503.05659v1  \n",
      "BERT嵌入的余弦相似度: 0.7374976873397827  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Control Flow-Augmented Decompiler based on Large Language Model  \n",
      "摘要：二进制反编译在安全威胁分析和软件工程的各种任务中起着至关重要的作用，如二进制漏洞检测和软件供应链分析。当前流行的二进制反编译方法主要依赖于大语言模型（LLMs），并可以大致分为两种主要方法：基于提示的反编译和端到端反编译。基于提示的方法通常需要大量努力来分析和总结预测数据以提取特定方面的专家知识，然后将其输入通用大语言模型以解决特定的反编译任务。端到端方法则精心构建训练数据集或神经网络，对通用大语言模型进行后训练，从而获得领域特定的大语言模型以反编译预测数据。然而，现有的两种方法仍面临重大挑战，包括缺乏输入代码的丰富语义表示以及忽略控制流信息，这对准确反编译至关重要。此外，大多数当前的反编译技术专门针对x86架构，难以高效适应和推广到其他位宽或指令架构。为了解决这些限制，我们提出了一种新的方法。  \n",
      "原文网址：http://arxiv.org/abs/2503.07215v1  \n",
      "BERT嵌入的余弦相似度: 0.6829602718353271  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Large Language Model Guided Progressive Feature Alignment for Multimodal UAV Object Detection  \n",
      "摘要：现有的多模态无人机目标检测方法通常忽略了模态之间语义差距的影响，这使得难以实现准确的语义和空间对齐，限制了检测性能。为了解决这个问题，我们提出了LPANet，一种由大语言模型（LLM）引导的渐进特征对齐网络，利用从大语言模型中提取的语义特征来指导模态之间的渐进语义和空间对齐。为了利用LLM的强大语义表示，我们通过ChatGPT生成每个对象类别的细粒度文本描述，然后使用大语言模型MPNet提取语义特征。基于这些语义特征，我们以渐进的方式指导语义和空间对齐。首先，我们设计了语义对齐模块（SAM），将每个对象的语义特征和多模态视觉特征拉近，缓解模态之间的语义差异。其次，我们设计了显式空间对齐模块（ESM），通过将语义关系整合到特征级偏移的估计中，缓解模态之间的粗空间不对齐。最后，我们设计了隐式空间对齐模块（ISM），利用跨模态相关性来聚合特征。  \n",
      "原文网址：http://arxiv.org/abs/2503.06948v1  \n",
      "BERT嵌入的余弦相似度: 0.6738836765289307  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Maximum Hallucination Standards for Domain-Specific Large Language Models  \n",
      "摘要：大语言模型（LLMs）经常生成不准确但听起来可信的内容，称为幻觉。LLMs的这一固有特征带来了重大风险，特别是在关键领域。我将LLMs分析为一类新的工程产品，将幻觉视为产品属性。我证明，在LLM幻觉的不完美意识和错误信息外部性的存在下，当LLM幻觉的最大可接受水平设计为随两个领域特定因素变化时，净福利会提高：减少LLM幻觉的支付意愿和与错误信息相关的边际损害。  \n",
      "原文网址：http://arxiv.org/abs/2503.05481v1  \n",
      "BERT嵌入的余弦相似度: 0.6386293172836304  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Can Large Language Models Predict Antimicrobial Resistance Gene?  \n",
      "摘要：本研究表明，生成式大语言模型可以以比传统的基于Transformer编码器的模型更灵活的方式用于DNA序列分析和分类任务。虽然最近的基于编码器的模型如DNABERT和Nucleotide Transformer在DNA序列分类中表现出显著的性能，但基于Transformer解码器的生成模型在这一领域尚未得到广泛探索。本研究评估了生成式大语言模型如何处理带有各种标签的DNA序列，并分析了在提供额外文本信息时的性能变化。实验在抗菌素耐药性基因上进行，结果表明生成式大语言模型可以提供可比或可能更好的预测，展示了在结合序列和文本信息时的灵活性和准确性。  \n",
      "原文网址：http://arxiv.org/abs/2503.04413v1  \n",
      "BERT嵌入的余弦相似度: 0.4551299214363098\n",
      "2025-03-12 16:39:56,730 - DEBUG - 生成的报告内容: ['\\n\\n--- 根据您选择的【学术期刊】平台，近几日的行业内最新进展已整理好，请查收！ ---\\n', \"来源：Google_arXiv  \\n标题：PLMM: Personal Large Language Models on Mobile Devices  \\n摘要：受联邦学习的启发，本文提出了个人大模型，这些模型从传统的大语言模型中蒸馏出来，但更能适应用户的个人信息，如教育背景和兴趣爱好。我们将大语言模型分为三个层次：个人层次、专家层次和传统层次。个人层次模型适应用户的个人信息，加密用户输入并保护隐私。专家层次模型专注于融合特定领域的知识，如金融、IT和艺术。传统模型则专注于通用知识的发现和专家模型的升级。在这种分类中，个人模型直接与用户交互，拥有用户的（加密）个人信息。此外，这些模型必须足够小，以便在个人计算机或移动设备上运行，并且需要实时响应以提供更好的用户体验和高质量的结果。所提出的个人大模型可以广泛应用于语言和视觉任务中。  \\n原文网址：http://arxiv.org/abs/2309.14726v2  \\nBERT嵌入的余弦相似度: 0.8257602453231812  \\n\\n来源：Google_arXiv  \\n标题：Beyond Segmentation: Road Network Generation with Multi-Modal LLMs  \\n摘要：本文介绍了一种通过多模态大语言模型（LLM）生成道路网络的创新方法。该模型专门设计用于处理道路布局的航拍图像，并在输入图像中生成详细的、可导航的道路网络。该系统的核心创新在于用于训练大语言模型以生成道路网络的独特训练方法。该方法借鉴了BLIP-2架构，利用预训练的冻结图像编码器和大语言模型创建了一个多功能的LLM。我们的工作还提供了LISA论文中提出的推理分割方法的替代方案。通过我们的方法训练大语言模型，有效消除了生成二进制分割掩码的需求。实验结果证明了我们的多模态LLM在提供精确和有价值的导航指导方面的有效性。这项研究代表了在增强自主导航系统方面的重要进展，特别是在道路网络场景中，准确的导航指导至关重要。  \\n原文网址：http://arxiv.org/abs/2310.09755v1  \\nBERT嵌入的余弦相似度: 0.7893250584602356  \\n\\n来源：Google_arXiv  \\n标题：Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data  \\n摘要：Audio Word2Vec通过序列到序列自编码器（SA）为可变长度的音频片段提供固定维度的向量表示。这些向量表示能够很好地描述音频片段的顺序语音结构，并在实际应用中如基于示例的语音术语检测（STD）中表现出色。本文探讨了Audio Word2Vec的语言迁移能力。我们从一个语言（源语言）训练SA，并使用它提取另一个语言（目标语言）的音频片段的向量表示。我们发现，如果源语言和目标语言相似，SA仍然可以从目标语言的音频片段中捕捉到语音结构。在基于示例的STD中，我们从大量源语言数据中学习的SA中获取向量表示，并发现这些表示优于从少量目标语言数据中直接学习的朴素编码器和SA的表示。结果表明，可以从高资源语言中学习Audio Word2Vec模型并将其用于低资源语言，这进一步扩展了Audio Word2Vec的可用性。  \\n原文网址：http://arxiv.org/abs/1707.06519v1  \\nBERT嵌入的余弦相似度: 0.741592288017273  \\n\\n来源：Google_arXiv  \\n标题：Mathematical Language Models: A Survey  \\n摘要：近年来，在数学领域利用语言模型（LMs），包括预训练语言模型（PLMs）和大规模语言模型（LLMs），取得了显著进展。本文对数学LMs进行了全面调查，从任务和方法两个角度系统分类了关键研究工作。该领域揭示了大量提出的数学LLMs，进一步细分为指令学习、基于工具的方法、基础CoT技术、高级CoT方法和多模态方法。为了更深入地理解数学LMs的优势，我们对其特征和性能进行了深入对比。此外，我们的调查还包括了60多个数学数据集的汇编，包括训练数据集、基准数据集和增强数据集。针对数学LMs领域的主要挑战和未来发展方向，本调查旨在促进和启发未来研究者的创新。  \\n原文网址：http://arxiv.org/abs/2312.07622v4  \\nBERT嵌入的余弦相似度: 0.7331765294075012  \\n\\n来源：Google_arXiv  \\n标题：Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning  \\n摘要：大语言模型（LLMs）可以通过构建自然语言查询并直接将提示输入到大语言模型中，用作可访问且智能的聊天机器人。然而，不同的提示构建通常会导致答案的不确定性，从而难以利用LLMs（如ChatGPT）的特定知识。为了缓解这一问题，我们使用可解释的结构来解释LLMs中的提示学习原理，该结构证明了语言模型的有效性由任务相关标记的位置变化决定。因此，我们提出了MTPrompt，一种基于任务相关对象、摘要和任务描述信息的多维任务提示学习方法。通过自动构建和搜索适当的提示，我们的MTPrompt在少样本设置和五个不同数据集上取得了最佳结果。此外，我们在不同的实验设置和消融实验中证明了该方法的有效性和稳定性。在与大语言模型的交互中，将更多任务相关信息嵌入提示中，将更容易激发大语言模型中嵌入的知识。  \\n原文网址：http://arxiv.org/abs/2312.08027v1  \\nBERT嵌入的余弦相似度: 0.7234684228897095  \\n\\n来源：Google_arXiv  \\n标题：Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner's Students  \\n摘要：我们描述了两个正在开发的系统，它们使用大语言模型来自动纠正（i）在自然语言与命题逻辑和一阶谓词逻辑之间来回翻译的练习，以及（ii）在非数学场景中撰写简单自然语言论证的练习。  \\n原文网址：http://arxiv.org/abs/2304.06186v3  \\nBERT嵌入的余弦相似度: 0.637988805770874  \\n\\n来源：Google_arXiv  \\n标题：Symbolic and Language Agnostic Large Language Models  \\n摘要：我们认为，大语言模型（LLMs）的相对成功并不是符号与子符号辩论的反映，而是反映了在规模上采用适当的自下而上逆向工程语言的策略。然而，由于这些模型的子符号性质，这些系统获得的关于语言的知识将始终隐藏在数百万个微特征（权重）中，没有一个是有意义的。此外，由于其随机性，这些模型经常无法捕捉自然语言中普遍存在的各种推理方面。我们建议在符号设置中采用成功的自下而上策略，生成符号化、语言无关且基于本体的大语言模型。  \\n原文网址：http://arxiv.org/abs/2308.14199v1  \\nBERT嵌入的余弦相似度: 0.6135643720626831  \\n\\n来源：arXiv  \\n标题：AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking  \\n摘要：从大语言模型中删除敏感内容的任务旨在从训练好的模型中删除目标数据点，同时最小化对其通用知识的影响。在我们的工作中，我们利用基于梯度的参数高效遗忘方法，使用低秩（LoRA）适应和分层微调。为了进一步增强遗忘效果，我们采用数据分块，将遗忘数据分割成不相交的分区，并以预定义的比率与循环采样的保留样本合并。我们的任务无关方法在遗忘-保留平衡方面表现出色，在排行榜上排名第一，显著优于基线和竞争系统。  \\n原文网址：http://arxiv.org/abs/2503.02443v1  \\nBERT嵌入的余弦相似度: 0.8754001259803772  \\n\\n来源：arXiv  \\n标题：Large Language Models in Bioinformatics: A Survey  \\n摘要：大语言模型（LLMs）正在革命化生物信息学，使得DNA、RNA、蛋白质和单细胞数据的高级分析成为可能。本调查系统回顾了最近的进展，重点关注基因组序列建模、RNA结构预测、蛋白质功能推断和单细胞转录组学。同时，我们还讨论了几个关键挑战，包括数据稀缺性、计算复杂性和跨组学整合，并探讨了未来方向，如多模态学习、混合AI模型和临床应用。通过提供全面的视角，本文强调了LLMs在推动生物信息学和精准医学创新方面的变革潜力。  \\n原文网址：http://arxiv.org/abs/2503.04490v1  \\nBERT嵌入的余弦相似度: 0.803259551525116  \\n\\n来源：arXiv  \\n标题：A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval  \\n摘要：信息技术深刻改变了人类与信息交互的方式。在线创建、共享和传播的大量内容使得访问相关信息变得越来越困难。在过去的二十年中，搜索和推荐系统（统称为信息检索系统）已经显著发展以应对这些挑战。最近的大语言模型（LLMs）进展展示了在各种语言相关任务中超越人类表现的能力，并表现出一般的理解、推理和决策能力。本文探讨了大语言模型代理在增强搜索和推荐系统中的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来阐述现有研究。我们强调了LLM代理在解决当前搜索和推荐挑战中的巨大潜力，为未来研究方向提供了见解。本文是第一个系统回顾和分类LLM代理在这些领域的研究，提供了利用这一先进AI技术进行信息检索的新视角。  \\n原文网址：http://arxiv.org/abs/2503.05659v1  \\nBERT嵌入的余弦相似度: 0.7374976873397827  \\n\\n来源：arXiv  \\n标题：Control Flow-Augmented Decompiler based on Large Language Model  \\n摘要：二进制反编译在安全威胁分析和软件工程的各种任务中起着至关重要的作用，如二进制漏洞检测和软件供应链分析。当前流行的二进制反编译方法主要依赖于大语言模型（LLMs），并可以大致分为两种主要方法：基于提示的反编译和端到端反编译。基于提示的方法通常需要大量努力来分析和总结预测数据以提取特定方面的专家知识，然后将其输入通用大语言模型以解决特定的反编译任务。端到端方法则精心构建训练数据集或神经网络，对通用大语言模型进行后训练，从而获得领域特定的大语言模型以反编译预测数据。然而，现有的两种方法仍面临重大挑战，包括缺乏输入代码的丰富语义表示以及忽略控制流信息，这对准确反编译至关重要。此外，大多数当前的反编译技术专门针对x86架构，难以高效适应和推广到其他位宽或指令架构。为了解决这些限制，我们提出了一种新的方法。  \\n原文网址：http://arxiv.org/abs/2503.07215v1  \\nBERT嵌入的余弦相似度: 0.6829602718353271  \\n\\n来源：arXiv  \\n标题：Large Language Model Guided Progressive Feature Alignment for Multimodal UAV Object Detection  \\n摘要：现有的多模态无人机目标检测方法通常忽略了模态之间语义差距的影响，这使得难以实现准确的语义和空间对齐，限制了检测性能。为了解决这个问题，我们提出了LPANet，一种由大语言模型（LLM）引导的渐进特征对齐网络，利用从大语言模型中提取的语义特征来指导模态之间的渐进语义和空间对齐。为了利用LLM的强大语义表示，我们通过ChatGPT生成每个对象类别的细粒度文本描述，然后使用大语言模型MPNet提取语义特征。基于这些语义特征，我们以渐进的方式指导语义和空间对齐。首先，我们设计了语义对齐模块（SAM），将每个对象的语义特征和多模态视觉特征拉近，缓解模态之间的语义差异。其次，我们设计了显式空间对齐模块（ESM），通过将语义关系整合到特征级偏移的估计中，缓解模态之间的粗空间不对齐。最后，我们设计了隐式空间对齐模块（ISM），利用跨模态相关性来聚合特征。  \\n原文网址：http://arxiv.org/abs/2503.06948v1  \\nBERT嵌入的余弦相似度: 0.6738836765289307  \\n\\n来源：arXiv  \\n标题：Maximum Hallucination Standards for Domain-Specific Large Language Models  \\n摘要：大语言模型（LLMs）经常生成不准确但听起来可信的内容，称为幻觉。LLMs的这一固有特征带来了重大风险，特别是在关键领域。我将LLMs分析为一类新的工程产品，将幻觉视为产品属性。我证明，在LLM幻觉的不完美意识和错误信息外部性的存在下，当LLM幻觉的最大可接受水平设计为随两个领域特定因素变化时，净福利会提高：减少LLM幻觉的支付意愿和与错误信息相关的边际损害。  \\n原文网址：http://arxiv.org/abs/2503.05481v1  \\nBERT嵌入的余弦相似度: 0.6386293172836304  \\n\\n来源：arXiv  \\n标题：Can Large Language Models Predict Antimicrobial Resistance Gene?  \\n摘要：本研究表明，生成式大语言模型可以以比传统的基于Transformer编码器的模型更灵活的方式用于DNA序列分析和分类任务。虽然最近的基于编码器的模型如DNABERT和Nucleotide Transformer在DNA序列分类中表现出显著的性能，但基于Transformer解码器的生成模型在这一领域尚未得到广泛探索。本研究评估了生成式大语言模型如何处理带有各种标签的DNA序列，并分析了在提供额外文本信息时的性能变化。实验在抗菌素耐药性基因上进行，结果表明生成式大语言模型可以提供可比或可能更好的预测，展示了在结合序列和文本信息时的灵活性和准确性。  \\n原文网址：http://arxiv.org/abs/2503.04413v1  \\nBERT嵌入的余弦相似度: 0.4551299214363098\"]\n",
      "2025-03-12 16:39:56,732 - INFO - 准备发送邮件...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "报告生成完成\n",
      "\n",
      "步骤 6/6: 发送报告\n",
      "未找到邮件地址或报告为空，未发送邮件。\n",
      "流程执行完成\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from db_utils import get_db_connection, initialize_database\n",
    "import datetime\n",
    "import logging\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from xml.etree import ElementTree\n",
    "from deep_translator import GoogleTranslator, BaiduTranslator\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# 配置日志\n",
    "# logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 常量配置\n",
    "CONFIG = {\n",
    "    \"API_KEYS\": {\n",
    "        \"deepseek\": os.environ.get(\"DEEPSEEK_API_KEY\", \"YOUR_KEY\"),\n",
    "        \"serper\": os.environ.get(\"SERPER_API_KEY\", \"YOUR_KEY\"),\n",
    "        \"baidu_translate\": os.environ.get(\"BAIDU_API_KEY\", \"YOUR_KEY\"),\n",
    "    },\n",
    "    \"MODELS\": {\n",
    "        \"BERT\": \"bert-base-multilingual-cased\",\n",
    "        \"LLM\": \"deepseek-chat\",\n",
    "    },\n",
    "    \"DATA_DIR\": \"./user_data\",\n",
    "    \"DB_PATH\": \"./user_data/user_profiles.db\"\n",
    "}\n",
    "\n",
    "# 确保数据目录存在\n",
    "os.makedirs(CONFIG[\"DATA_DIR\"], exist_ok=True)\n",
    "\n",
    "# 初始化BERT模型\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG[\"MODELS\"][\"BERT\"])\n",
    "model = BertModel.from_pretrained(CONFIG[\"MODELS\"][\"BERT\"])\n",
    "\n",
    "# 创建数据库连接\n",
    "def verify_database():\n",
    "    \"\"\"验证数据库是否正确创建和可写入\"\"\"\n",
    "    print(\"\\n验证数据库...\")\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if conn is None:\n",
    "        print(\"无法连接到数据库，请检查路径和权限\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 尝试写入测试数据\n",
    "        test_id = f\"test_{int(time.time())}\"\n",
    "        conn.execute(\n",
    "            \"INSERT INTO users (id, name, occupation, email) VALUES (?, ?, ?, ?)\",\n",
    "            (test_id, \"测试用户\", \"测试职业\", \"test@example.com\")\n",
    "        )\n",
    "        conn.commit()\n",
    "        \n",
    "        # 验证是否写入成功\n",
    "        user = conn.execute(\"SELECT * FROM users WHERE id = ?\", (test_id,)).fetchone()\n",
    "        if user:\n",
    "            print(\"数据库验证成功：可以正常写入和读取数据\")\n",
    "            \n",
    "            # 清理测试数据\n",
    "            conn.execute(\"DELETE FROM users WHERE id = ?\", (test_id,))\n",
    "            conn.commit()\n",
    "            \n",
    "            # 显示数据库信息\n",
    "            tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "            print(f\"数据库包含以下表: {', '.join([t['name'] for t in tables])}\")\n",
    "            \n",
    "            for table in [t['name'] for t in tables]:\n",
    "                count = conn.execute(f\"SELECT COUNT(*) as count FROM {table}\").fetchone()['count']\n",
    "                print(f\"表 {table}: {count} 条记录\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"数据库验证失败：无法读取写入的测试数据\")\n",
    "            return False\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"数据库验证失败: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 创建一个自定义的 NumpyEncoder 类来处理 NumPy 数据类型\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        import numpy as np\n",
    "        if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def get_bert_embeddings(text: str) -> torch.Tensor:\n",
    "    \"\"\" 获取文本的BERT嵌入表示 \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "def compute_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\" 计算两个文本之间的余弦相似度 \"\"\"\n",
    "    embedding1 = get_bert_embeddings(text1)\n",
    "    embedding2 = get_bert_embeddings(text2)\n",
    "    similarity = cosine_similarity(embedding1.numpy(), embedding2.numpy())\n",
    "    return similarity[0][0]\n",
    "\n",
    "class UserProfileManager:\n",
    "    \"\"\"用户画像管理器，负责创建、更新和存储用户画像\"\"\"\n",
    "\n",
    "    def __init__(self, client=None):\n",
    "        \"\"\"初始化用户画像管理器\"\"\"\n",
    "        self.client = client or OpenAI(\n",
    "            api_key=CONFIG[\"API_KEYS\"][\"deepseek\"],\n",
    "            base_url=\"https://api.deepseek.com\"\n",
    "        )\n",
    "        self.interest_categories = self._load_interest_categories()\n",
    "        logging.info(\"用户画像管理器初始化完成\")\n",
    "\n",
    "    def _load_interest_categories(self):\n",
    "        \"\"\"加载预定义的兴趣分类体系\"\"\"\n",
    "        categories_file = os.path.join(CONFIG[\"DATA_DIR\"], \"interest_categories.json\")\n",
    "\n",
    "        if os.path.exists(categories_file):\n",
    "            try:\n",
    "                with open(categories_file, 'r', encoding='utf-8') as f:\n",
    "                    self.interest_categories = json.load(f)\n",
    "                logging.info(f\"已加载兴趣分类体系，共 {len(self.interest_categories)} 个类别\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"加载兴趣分类体系出错: {e}\")\n",
    "                self._create_default_categories()\n",
    "        else:\n",
    "            logging.warning(f\"兴趣分类文件不存在: {categories_file}，将创建默认分类\")\n",
    "            self._create_default_categories()\n",
    "\n",
    "    def _create_default_categories(self):\n",
    "        \"\"\"创建默认的兴趣分类体系\"\"\"\n",
    "        self.interest_categories = {\n",
    "            \"技术\": [\"人工智能\", \"机器学习\", \"深度学习\", \"自然语言处理\", \"计算机视觉\", \"大语言模型\",\n",
    "                   \"大数据\", \"云计算\", \"区块链\", \"物联网\", \"网络安全\", \"数据库\"],\n",
    "            \"科学\": [\"物理学\", \"化学\", \"生物学\", \"天文学\", \"数学\", \"医学\", \"地质学\", \"环境科学\"],\n",
    "            \"商业\": [\"管理\", \"市场营销\", \"金融\", \"创业\", \"投资\", \"电子商务\", \"人力资源\"],\n",
    "            \"艺术\": [\"绘画\", \"音乐\", \"电影\", \"文学\", \"设计\", \"摄影\", \"建筑\"],\n",
    "            \"教育\": [\"教学方法\", \"学习理论\", \"教育技术\", \"高等教育\", \"职业教育\"],\n",
    "            \"健康\": [\"营养\", \"健身\", \"心理健康\", \"医疗技术\", \"公共卫生\"]\n",
    "        }\n",
    "\n",
    "        # 保存到文件\n",
    "        categories_file = os.path.join(CONFIG[\"DATA_DIR\"], \"interest_categories.json\")\n",
    "        try:\n",
    "            with open(categories_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.interest_categories, f, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"已创建默认兴趣分类体系并保存到: {categories_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存默认兴趣分类体系出错: {e}\")\n",
    "\n",
    "\n",
    "    def _generate_user_id(self, user_info: Dict) -> str:\n",
    "        \"\"\"根据用户信息生成唯一ID\"\"\"\n",
    "        key_string = f\"{user_info.get('name', '')}-{user_info.get('email', '')}-{user_info.get('occupation', '')}\"\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "\n",
    "    def create_user(self, user_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        创建新用户并存储基本信息\n",
    "\n",
    "        Args:\n",
    "            user_info: 用户基本信息，包含姓名、职业、邮箱等\n",
    "\n",
    "        Returns:\n",
    "            用户ID\n",
    "        \"\"\"\n",
    "        user_id = self._generate_user_id(user_info)\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 检查用户是否已存在\n",
    "            existing_user = conn.execute(\"SELECT id FROM users WHERE id = ?\", (user_id,)).fetchone()\n",
    "\n",
    "            if not existing_user:\n",
    "                conn.execute(\n",
    "                    \"INSERT INTO users (id, name, occupation, email) VALUES (?, ?, ?, ?)\",\n",
    "                    (user_id, user_info.get(\"name\", \"\"), user_info.get(\"occupation\", \"\"), user_info.get(\"email\", \"\"))\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"创建新用户: {user_id}\")\n",
    "            else:\n",
    "                logging.info(f\"用户已存在: {user_id}\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return user_id\n",
    "\n",
    "    def extract_skills_from_resume(self, user_id: str, resume_text: str, max_skills: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从简历文本中提取技能信息\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            resume_text: 简历文本内容\n",
    "            max_skills: 最多提取的技能数量\n",
    "\n",
    "        Returns:\n",
    "            技能列表，每个技能包含名称、级别和分类\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始从简历中提取最重要的{max_skills}项技能...\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请从以下简历文本中提取最重要的{max_skills}项技能，并为每个技能提供以下信息：\n",
    "        1. 技能名称\n",
    "        2. 熟练程度（初级/中级/高级/专家）\n",
    "        3. 技能类别（技术技能、软技能、语言技能、管理技能等）\n",
    "        \n",
    "        请按照技能的重要性和熟练程度排序，最重要和最熟练的技能排在前面。\n",
    "        \n",
    "        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\n",
    "        [\n",
    "            {{\"skill\": \"技能名称\", \"level\": \"熟练程度\", \"category\": \"技能类别\"}},\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        简历文本：\n",
    "        {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"正在分析简历中的技能...\")\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的简历分析助手，擅长提取简历中的技能信息并进行分类和评估。请只返回JSON格式的结果，不要添加任何其他标记。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            skills_text = response.choices[0].message.content\n",
    "            \n",
    "            # 清理可能的格式标记\n",
    "            skills_text = skills_text.strip()\n",
    "            if skills_text.startswith(\"```json\"):\n",
    "                skills_text = skills_text[7:]\n",
    "            if skills_text.startswith(\"```\"):\n",
    "                skills_text = skills_text[3:]\n",
    "            if skills_text.endswith(\"```\"):\n",
    "                skills_text = skills_text[:-3]\n",
    "            skills_text = skills_text.strip()\n",
    "            \n",
    "            logging.debug(f\"清理后的技能JSON文本: {skills_text}\")\n",
    "    \n",
    "            # 尝试解析JSON\n",
    "            try:\n",
    "                skills = json.loads(skills_text)\n",
    "                print(f\"成功提取 {len(skills)} 项技能\")\n",
    "                \n",
    "                # 确保skills是列表类型\n",
    "                if not isinstance(skills, list):\n",
    "                    logging.error(f\"解析的技能不是列表类型: {type(skills)}\")\n",
    "                    skills = []\n",
    "                    print(\"解析的技能格式不正确，将使用空列表\")\n",
    "                \n",
    "                # 保存到数据库\n",
    "                if skills:  # 只有当skills非空时才尝试保存\n",
    "                    conn = get_db_connection()\n",
    "                    try:\n",
    "                        for i, skill in enumerate(skills):\n",
    "                            if isinstance(skill, dict):  # 确保每个技能是字典类型\n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_skills (user_id, skill, level, category) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, skill.get(\"skill\", \"\"), skill.get(\"level\", \"\"), skill.get(\"category\", \"\"))\n",
    "                                )\n",
    "                                # 显示进度\n",
    "                                print(f\"保存技能 {i+1}/{len(skills)}: {skill.get('skill', '')}\")\n",
    "                        conn.commit()\n",
    "                        print(\"所有技能已保存到数据库\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"保存技能到数据库时出错: {e}\")\n",
    "                        print(f\"保存技能时出错: {e}\")\n",
    "                    finally:\n",
    "                        conn.close()\n",
    "                \n",
    "                return skills if skills else []\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"无法解析技能JSON: {skills_text}, 错误: {e}\")\n",
    "                print(f\"解析技能信息失败: {e}\")\n",
    "                # 尝试手动解析简单格式\n",
    "                if \"[\" in skills_text and \"]\" in skills_text:\n",
    "                    try:\n",
    "                        # 尝试修复常见的JSON格式问题\n",
    "                        fixed_text = skills_text.replace(\"'\", \"\\\"\")\n",
    "                        skills = json.loads(fixed_text)\n",
    "                        print(f\"修复后成功解析，提取了 {len(skills)} 项技能\")\n",
    "                        return skills\n",
    "                    except:\n",
    "                        pass\n",
    "                return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取技能时出错: {e}\")\n",
    "            print(f\"提取技能时出错: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_interests_from_resume(self, user_id: str, resume_text: str, max_interests: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从简历中提取用户兴趣并分类\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            resume_text: 简历文本\n",
    "            max_interests: 最多提取的兴趣数量\n",
    "\n",
    "        Returns:\n",
    "            兴趣列表，每个兴趣包含主题、分类和初始权重\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始从简历中提取最重要的{max_interests}项兴趣...\")\n",
    "    \n",
    "        # 构建兴趣分类提示\n",
    "        categories_text = \"\\n\".join([f\"{cat}: {', '.join(topics)}\" for cat, topics in self.interest_categories.items()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请分析以下简历文本，提取用户最重要的{max_interests}项兴趣领域和专业方向。\n",
    "        将提取的兴趣根据以下分类系统进行归类：\n",
    "        \n",
    "        {categories_text}\n",
    "        \n",
    "        如果发现的兴趣不在上述分类中，请归入最相近的类别。\n",
    "        对于每个识别的兴趣，根据在简历中的明显程度，给出一个0到1之间的权重。\n",
    "        请按照兴趣的重要性排序，最重要的兴趣排在前面。\n",
    "        \n",
    "        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\n",
    "        [\n",
    "            {{\"topic\": \"兴趣主题\", \"category\": \"所属类别\", \"weight\": 权重值}},\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        简历文本：\n",
    "        {resume_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"正在分析简历中的兴趣...\")\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长从文本中提取人们的兴趣爱好并进行分类。请只返回JSON格式的结果，不要添加任何其他标记。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            interests_text = response.choices[0].message.content\n",
    "\n",
    "            # 记录原始响应以便调试\n",
    "            logging.debug(f\"原始兴趣响应: {interests_text}\")\n",
    "            \n",
    "            # 清理可能的格式标记\n",
    "            interests_text = interests_text.strip()\n",
    "            if interests_text.startswith(\"```json\"):\n",
    "                interests_text = interests_text[7:]\n",
    "            if interests_text.startswith(\"```\"):\n",
    "                interests_text = interests_text[3:]\n",
    "            if interests_text.endswith(\"```\"):\n",
    "                interests_text = interests_text[:-3]\n",
    "            interests_text = interests_text.strip()\n",
    "            \n",
    "            logging.debug(f\"清理后的兴趣JSON文本: {interests_text}\")\n",
    "        \n",
    "            # 尝试解析JSON\n",
    "            try:\n",
    "                interests = json.loads(interests_text)\n",
    "                print(f\"成功提取 {len(interests)} 项兴趣\")\n",
    "                \n",
    "                # 确保interests是列表类型\n",
    "                if not isinstance(interests, list):\n",
    "                    logging.error(f\"解析的兴趣不是列表类型: {type(interests)}\")\n",
    "                    interests = []\n",
    "                    print(\"解析的兴趣格式不正确，将使用空列表\")\n",
    "                \n",
    "                # 保存到数据库\n",
    "                if interests:  # 只有当interests非空时才尝试保存\n",
    "                    conn = get_db_connection()\n",
    "                    if conn is None:\n",
    "                        logging.error(\"无法连接到数据库\")\n",
    "                        print(\"无法连接到数据库，兴趣数据未保存\")\n",
    "                        return interests\n",
    "                    \n",
    "                    try:\n",
    "                        for i, interest in enumerate(interests):\n",
    "                            if isinstance(interest, dict):  # 确保每个兴趣是字典类型\n",
    "                                # 确保所有必要的键都存在\n",
    "                                topic = interest.get(\"topic\", \"未知兴趣\")\n",
    "                                category = interest.get(\"category\", \"未分类\")\n",
    "                                weight = interest.get(\"weight\", 0.5)\n",
    "                                \n",
    "                                # 确保weight是浮点数\n",
    "                                try:\n",
    "                                    weight = float(weight)\n",
    "                                except (ValueError, TypeError):\n",
    "                                    weight = 0.5\n",
    "                                \n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, topic, category, weight)\n",
    "                                )\n",
    "                                # 显示进度\n",
    "                                print(f\"保存兴趣 {i+1}/{len(interests)}: {topic} (权重: {weight:.2f})\")\n",
    "                        conn.commit()\n",
    "                        print(\"所有兴趣已保存到数据库\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"保存兴趣到数据库时出错: {e}\")\n",
    "                        print(f\"保存兴趣时出错: {e}\")\n",
    "                        conn.rollback()  # 回滚事务\n",
    "                    finally:\n",
    "                        conn.close()\n",
    "                \n",
    "                return interests if interests else []\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"无法解析兴趣JSON: {interests_text}, 错误: {e}\")\n",
    "                print(f\"解析兴趣信息失败: {e}\")\n",
    "                \n",
    "                # 尝试手动解析简单格式\n",
    "                if \"[\" in interests_text and \"]\" in interests_text:\n",
    "                    try:\n",
    "                        # 尝试修复常见的JSON格式问题\n",
    "                        fixed_text = interests_text.replace(\"'\", \"\\\"\").replace(\"None\", \"null\")\n",
    "                        interests = json.loads(fixed_text)\n",
    "                        print(f\"修复后成功解析，提取了 {len(interests)} 项兴趣\")\n",
    "                        return interests\n",
    "                    except Exception as parse_e:\n",
    "                        logging.error(f\"尝试修复JSON后仍然失败: {parse_e}\")\n",
    "                \n",
    "                # 如果无法解析，创建一些基本兴趣\n",
    "                print(\"无法解析兴趣，将创建基本兴趣\")\n",
    "                basic_interests = []\n",
    "                \n",
    "                # 从简历文本中提取一些关键词作为基本兴趣\n",
    "                keywords = [\"人工智能\", \"机器学习\", \"数据分析\", \"编程\", \"算法\"]\n",
    "                for i, keyword in enumerate(keywords):\n",
    "                    if keyword.lower() in resume_text.lower():\n",
    "                        interest = {\n",
    "                            \"topic\": keyword,\n",
    "                            \"category\": \"技术\",\n",
    "                            \"weight\": 0.7\n",
    "                        }\n",
    "                        basic_interests.append(interest)\n",
    "                        \n",
    "                        # 保存到数据库\n",
    "                        try:\n",
    "                            conn = get_db_connection()\n",
    "                            if conn:\n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, keyword, \"技术\", 0.7)\n",
    "                                )\n",
    "                                conn.commit()\n",
    "                                print(f\"保存基本兴趣: {keyword}\")\n",
    "                                conn.close()\n",
    "                        except Exception as db_e:\n",
    "                            logging.error(f\"保存基本兴趣到数据库时出错: {db_e}\")\n",
    "                \n",
    "                return basic_interests\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取兴趣时出错: {e}\")\n",
    "            print(f\"提取兴趣时出错: {e}\")\n",
    "            return []\n",
    "\n",
    "    def record_search(self, user_id: str, query: str, platform: str):\n",
    "        \"\"\"\n",
    "        记录用户搜索行为\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            query: 搜索查询\n",
    "            platform: 搜索平台\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO user_searches (user_id, query, platform) VALUES (?, ?, ?)\",\n",
    "                (user_id, query, platform)\n",
    "            )\n",
    "            conn.commit()\n",
    "            logging.info(f\"记录用户搜索: {user_id}, 查询: {query}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def record_interaction(self, user_id: str, content_id: str, action_type: str):\n",
    "        \"\"\"\n",
    "        记录用户与内容的交互\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            content_id: 内容ID（如文章URL）\n",
    "            action_type: 交互类型（如\"点击\"、\"收藏\"、\"分享\"）\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO user_interactions (user_id, content_id, action_type) VALUES (?, ?, ?)\",\n",
    "                (user_id, content_id, action_type)\n",
    "            )\n",
    "            conn.commit()\n",
    "            logging.info(f\"记录用户交互: {user_id}, 内容: {content_id}, 行为: {action_type}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def update_interest_weights(self, user_id: str, topic: str, adjustment: float):\n",
    "        \"\"\"\n",
    "        更新用户兴趣权重\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            topic: 兴趣主题\n",
    "            adjustment: 权重调整值，正数表示增加，负数表示减少\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 查找现有兴趣\n",
    "            interest = conn.execute(\n",
    "                \"SELECT id, weight FROM user_interests WHERE user_id = ? AND topic = ? ORDER BY timestamp DESC LIMIT 1\",\n",
    "                (user_id, topic)\n",
    "            ).fetchone()\n",
    "\n",
    "            if interest:\n",
    "                # 更新权重，确保在0-1范围内\n",
    "                new_weight = max(0, min(1, interest[\"weight\"] + adjustment))\n",
    "\n",
    "                conn.execute(\n",
    "                    \"UPDATE user_interests SET weight = ?, timestamp = CURRENT_TIMESTAMP WHERE id = ?\",\n",
    "                    (new_weight, interest[\"id\"])\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"更新用户兴趣权重: {user_id}, 主题: {topic}, 新权重: {new_weight}\")\n",
    "            else:\n",
    "                # 如果不存在，创建新的兴趣项\n",
    "                weight = max(0, min(1, 0.5 + adjustment))  # 默认权重0.5加上调整值\n",
    "\n",
    "                # 尝试确定类别\n",
    "                category = \"未分类\"\n",
    "                for cat, topics in self.interest_categories.items():\n",
    "                    if any(compute_similarity(topic, t) > 0.7 for t in topics):\n",
    "                        category = cat\n",
    "                        break\n",
    "\n",
    "                conn.execute(\n",
    "                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                    (user_id, topic, category, weight)\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"创建新用户兴趣: {user_id}, 主题: {topic}, 权重: {weight}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def apply_time_decay(self, user_id: str, decay_factor: float = 0.9, days_threshold: int = 30):\n",
    "        \"\"\"\n",
    "        应用时间衰减模型，降低旧兴趣的权重\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            decay_factor: 衰减因子(0-1)\n",
    "            days_threshold: 多少天前的兴趣开始衰减\n",
    "        \"\"\"\n",
    "        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days_threshold)\n",
    "        threshold_str = threshold_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            old_interests = conn.execute(\n",
    "                \"SELECT id, topic, category, weight, timestamp FROM user_interests WHERE user_id = ? AND timestamp < ?\",\n",
    "                (user_id, threshold_str)\n",
    "            ).fetchall()\n",
    "\n",
    "            for interest in old_interests:\n",
    "                # 计算时间差（天数）\n",
    "                interest_date = datetime.datetime.strptime(interest[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "                days_diff = (datetime.datetime.now() - interest_date).days\n",
    "\n",
    "                # 计算衰减倍数（随时间增加而增加衰减）\n",
    "                decay_multiplier = days_diff // days_threshold\n",
    "\n",
    "                # 计算新权重\n",
    "                new_weight = interest[\"weight\"] * (decay_factor ** decay_multiplier)\n",
    "\n",
    "                # 更新权重\n",
    "                conn.execute(\n",
    "                    \"UPDATE user_interests SET weight = ? WHERE id = ?\",\n",
    "                    (new_weight, interest[\"id\"])\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            logging.info(f\"应用时间衰减模型: {user_id}, 处理 {len(old_interests)} 条旧兴趣\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_top_interests(self, user_id: str, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        获取用户的顶级兴趣\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            limit: 返回的兴趣数量\n",
    "\n",
    "        Returns:\n",
    "            兴趣列表，按权重排序\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 对于每个主题，只取最新的一条记录\n",
    "            interests = conn.execute(\"\"\"\n",
    "                SELECT i1.topic, i1.category, i1.weight, i1.timestamp\n",
    "                FROM user_interests i1\n",
    "                INNER JOIN (\n",
    "                    SELECT topic, MAX(timestamp) as max_time\n",
    "                    FROM user_interests\n",
    "                    WHERE user_id = ?\n",
    "                    GROUP BY topic\n",
    "                ) i2 ON i1.topic = i2.topic AND i1.timestamp = i2.max_time\n",
    "                WHERE user_id = ?\n",
    "                ORDER BY i1.weight DESC\n",
    "                LIMIT ?\n",
    "            \"\"\", (user_id, user_id, limit)).fetchall()\n",
    "\n",
    "            return [dict(i) for i in interests]\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def analyze_search_patterns(self, user_id: str, days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        分析用户搜索模式\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            days: 分析的天数范围\n",
    "\n",
    "        Returns:\n",
    "            分析结果，包含常用平台、热门查询等\n",
    "        \"\"\"\n",
    "        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days)\n",
    "        threshold_str = threshold_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 获取搜索记录\n",
    "            searches = conn.execute(\n",
    "                \"SELECT query, platform, timestamp FROM user_searches WHERE user_id = ? AND timestamp > ?\",\n",
    "                (user_id, threshold_str)\n",
    "            ).fetchall()\n",
    "\n",
    "            if not searches:\n",
    "                return {\"status\": \"无搜索记录\"}\n",
    "\n",
    "            # 统计平台使用情况\n",
    "            platforms = {}\n",
    "            for search in searches:\n",
    "                platform = search[\"platform\"]\n",
    "                platforms[platform] = platforms.get(platform, 0) + 1\n",
    "\n",
    "            # 提取查询内容用于语义分析\n",
    "            queries = [search[\"query\"] for search in searches]\n",
    "\n",
    "            # 使用LLM分析查询主题\n",
    "            prompt = f\"\"\"\n",
    "            请分析以下搜索查询列表，识别主要的搜索主题和模式。\n",
    "            将分析结果以JSON格式返回，包含以下字段：\n",
    "            1. dominant_topics: 主导主题列表，按重要性排序\n",
    "            2. search_patterns: 搜索模式描述\n",
    "\n",
    "            搜索查询列表：\n",
    "            {queries}\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的搜索行为分析助手。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                analysis_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    analysis = json.loads(analysis_text)\n",
    "                except json.JSONDecodeError:\n",
    "                    analysis = {\"dominant_topics\": [], \"search_patterns\": \"无法解析分析结果\"}\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"分析搜索模式时出错: {e}\")\n",
    "                analysis = {\"dominant_topics\": [], \"search_patterns\": f\"分析出错: {str(e)}\"}\n",
    "\n",
    "            # 整合结果\n",
    "            return {\n",
    "                \"platform_stats\": platforms,\n",
    "                \"search_count\": len(searches),\n",
    "                \"analysis\": analysis,\n",
    "                \"timeframe\": f\"过去{days}天\"\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def generate_recommendations(self, user_id: str, count: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        基于用户画像生成内容推荐\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            count: 推荐数量\n",
    "\n",
    "        Returns:\n",
    "            推荐的主题列表\n",
    "        \"\"\"\n",
    "        # 获取用户顶级兴趣\n",
    "        top_interests = self.get_top_interests(user_id, limit=5)\n",
    "\n",
    "        if not top_interests:\n",
    "            return [\"未找到用户兴趣数据\"]\n",
    "\n",
    "        # 提取兴趣主题\n",
    "        interest_topics = [i[\"topic\"] for i in top_interests]\n",
    "\n",
    "        # 使用LLM生成推荐\n",
    "        prompt = f\"\"\"\n",
    "        基于以下用户兴趣主题，推荐{count}个具体的、精细的研究或学习主题，这些主题应该是前沿的、有深度的，并与用户的兴趣紧密相关。\n",
    "\n",
    "        用户兴趣主题：{\", \".join(interest_topics)}\n",
    "\n",
    "        请列出具体的推荐主题，每个主题应包含足够的细节和专业性，以便能够直接用于学术研究或专业学习。\n",
    "        以JSON数组格式返回，每个元素包含'topic'和'reason'字段。\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的学习内容推荐助手，擅长为用户提供高质量、前沿的学习主题推荐。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            recs_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                recommendations = json.loads(recs_text)\n",
    "                return recommendations\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析推荐JSON: {recs_text}\")\n",
    "                return [{\"topic\": \"解析推荐失败\", \"reason\": \"请稍后再试\"}]\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"生成推荐时出错: {e}\")\n",
    "            return [{\"topic\": \"生成推荐出错\", \"reason\": str(e)}]\n",
    "\n",
    "    def get_user_profile_summary(self, user_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        获取用户画像摘要\n",
    "        \n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            \n",
    "        Returns:\n",
    "            用户画像摘要信息\n",
    "        \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = get_db_connection()\n",
    "            if not conn:\n",
    "                return {\"error\": \"无法连接到数据库\"}\n",
    "                \n",
    "            # 获取基本信息\n",
    "            user = conn.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,)).fetchone()\n",
    "            \n",
    "            if not user:\n",
    "                return {\"error\": \"用户不存在\"}\n",
    "            \n",
    "            # 获取顶级兴趣\n",
    "            interests = []\n",
    "            try:\n",
    "                interests_query = conn.execute(\"\"\"\n",
    "                    SELECT i1.topic, i1.category, i1.weight, i1.timestamp \n",
    "                    FROM user_interests i1\n",
    "                    INNER JOIN (\n",
    "                        SELECT topic, MAX(timestamp) as max_time\n",
    "                        FROM user_interests\n",
    "                        WHERE user_id = ?\n",
    "                        GROUP BY topic\n",
    "                    ) i2 ON i1.topic = i2.topic AND i1.timestamp = i2.max_time\n",
    "                    WHERE user_id = ?\n",
    "                    ORDER BY i1.weight DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\", (user_id, user_id))\n",
    "                \n",
    "                interests = [dict(i) for i in interests_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取用户兴趣时出错: {e}\")\n",
    "                interests = []\n",
    "            \n",
    "            # 获取技能\n",
    "            skills = []\n",
    "            try:\n",
    "                skills_query = conn.execute(\n",
    "                    \"SELECT skill, level, category FROM user_skills WHERE user_id = ? ORDER BY level DESC\",\n",
    "                    (user_id,)\n",
    "                )\n",
    "                skills = [dict(s) for s in skills_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取用户技能时出错: {e}\")\n",
    "                skills = []\n",
    "            \n",
    "            # 获取搜索统计\n",
    "            search_count = 0\n",
    "            try:\n",
    "                search_count_query = conn.execute(\n",
    "                    \"SELECT COUNT(*) as count FROM user_searches WHERE user_id = ?\", \n",
    "                    (user_id,)\n",
    "                ).fetchone()\n",
    "                if search_count_query:\n",
    "                    search_count = search_count_query[\"count\"]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取搜索统计时出错: {e}\")\n",
    "            \n",
    "            # 获取交互统计\n",
    "            interaction_count = 0\n",
    "            try:\n",
    "                interaction_count_query = conn.execute(\n",
    "                    \"SELECT COUNT(*) as count FROM user_interactions WHERE user_id = ?\", \n",
    "                    (user_id,)\n",
    "                ).fetchone()\n",
    "                if interaction_count_query:\n",
    "                    interaction_count = interaction_count_query[\"count\"]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取交互统计时出错: {e}\")\n",
    "            \n",
    "            # 获取最近5次搜索\n",
    "            recent_searches = []\n",
    "            try:\n",
    "                searches_query = conn.execute(\n",
    "                    \"SELECT query, platform, timestamp FROM user_searches WHERE user_id = ? ORDER BY timestamp DESC LIMIT 5\",\n",
    "                    (user_id,)\n",
    "                )\n",
    "                recent_searches = [dict(s) for s in searches_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取最近搜索时出错: {e}\")\n",
    "            \n",
    "            # 整合数据\n",
    "            profile = {\n",
    "                \"basic_info\": dict(user),\n",
    "                \"top_interests\": interests,\n",
    "                \"skills\": skills,\n",
    "                \"activity\": {\n",
    "                    \"search_count\": search_count,\n",
    "                    \"interaction_count\": interaction_count,\n",
    "                    \"recent_searches\": recent_searches\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return profile\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"获取用户画像摘要时出错: {e}\")\n",
    "            return {\"error\": f\"获取用户画像摘要时出错: {str(e)}\"}\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "\n",
    "class KnowledgeFlow:\n",
    "    def __init__(self):\n",
    "        self.context = {}\n",
    "        self.client = OpenAI(api_key=CONFIG[\"API_KEYS\"][\"deepseek\"], base_url=\"https://api.deepseek.com\")\n",
    "        self.serper_api_key = CONFIG[\"API_KEYS\"][\"serper\"]\n",
    "        self.user_profile_manager = UserProfileManager(client=self.client)\n",
    "\n",
    "\n",
    "    def start_node(self, user_input: Dict[str, Any]) -> Dict:\n",
    "        \"\"\" 收集用户初始信息 \"\"\"\n",
    "        logging.info(\"开始收集用户输入信息...\")  \n",
    "        required_fields = ['occupation', 'day', 'platform']\n",
    "        for field in required_fields:\n",
    "            if field not in user_input:\n",
    "                raise ValueError(f\"缺少必要字段: {field}\")\n",
    "\n",
    "        logging.debug(f\"用户输入信息: {user_input}\") \n",
    "        self.context.update(user_input)\n",
    "\n",
    "        # 创建或获取用户ID\n",
    "        if 'user_id' not in self.context and 'email' in user_input:\n",
    "            user_info = {\n",
    "                'name': user_input.get('name', '未知用户'),\n",
    "                'occupation': user_input.get('occupation', ''),\n",
    "                'email': user_input.get('email', '')\n",
    "            }\n",
    "            self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "\n",
    "        # platform_type = self.context.get('platform')\n",
    "        self.context['update_cycle'] = self.calculate_update_cycle(user_input['day'])\n",
    "        return self.context\n",
    "    \n",
    "    def calculate_update_cycle(self, days: int) -> Dict:\n",
    "        \"\"\" 计算时间范围 \"\"\"\n",
    "        logging.info(\"计算时间范围...\") \n",
    "        end_date = datetime.datetime.now(datetime.timezone.utc)\n",
    "        start_date = end_date - datetime.timedelta(days=days)\n",
    "        logging.debug(f\"时间范围计算结果: 起始日期={start_date}, 结束日期={end_date}\")\n",
    "        return {\n",
    "            \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": end_date.strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "\n",
    "    def get_user_profile(self, cv_text: str, task: str) -> Dict:\n",
    "        \"\"\"使用大语言模型API初步分析用户画像，并根据不同任务执行不同的prompt\"\"\"\n",
    "        logging.info(f\"执行任务：{task}，分析简历内容...\")\n",
    "        print(f\"\\n===== 开始执行用户画像分析任务：{task} =====\")\n",
    "\n",
    "        # 初始化返回值，确保始终返回一个字典\n",
    "        profile_analysis = {\"skills\": [], \"interests\": []}\n",
    "\n",
    "        # 如果有用户ID，先调用用户画像管理器处理简历\n",
    "        if 'user_id' in self.context:\n",
    "            user_id = self.context['user_id']\n",
    "            print(f\"用户ID: {user_id}\")\n",
    "\n",
    "            # 提取技能\n",
    "            print(\"\\n第1步：提取用户技能\")\n",
    "            skills = self.user_profile_manager.extract_skills_from_resume(user_id, cv_text)\n",
    "            logging.info(f\"从简历中提取了 {len(skills)} 项技能\")\n",
    "            print(f\"技能提取完成，共 {len(skills)} 项\")\n",
    "\n",
    "            # 提取兴趣\n",
    "            print(\"\\n第2步：提取用户兴趣\")\n",
    "            interests = self.user_profile_manager.extract_interests_from_resume(user_id, cv_text)\n",
    "            logging.info(f\"从简历中提取了 {len(interests)} 项兴趣\")\n",
    "            print(f\"兴趣提取完成，共 {len(interests)} 项\")\n",
    "\n",
    "            # 合并技能和兴趣信息到分析结果\n",
    "            profile_analysis = {\n",
    "                \"skills\": skills,\n",
    "                \"interests\": interests\n",
    "            }\n",
    "\n",
    "            # 定义不同任务的prompt模板\n",
    "            print(\"\\n第3步：生成综合分析报告\")\n",
    "            prompt_templates = {\n",
    "                \"analyze_resume\": f\"分析以下简历内容，提供全面的职业画像分析：{cv_text}\",\n",
    "                \"user_interest\": f\"根据以下简历内容，识别用户的职业兴趣和专注领域：{cv_text}\",\n",
    "                \"skill_assessment\": f\"根据以下简历内容，评估用户的技能并提出改进建议：{cv_text}\",\n",
    "                \"career_development\": f\"根据以下简历内容，分析用户的职业发展路径并提供建议：{cv_text}\"\n",
    "            }\n",
    "\n",
    "            # 确保指定的任务在模板中存在\n",
    "            if task not in prompt_templates:\n",
    "                logging.warning(f\"未知任务: {task}，将使用默认分析\")\n",
    "                task = \"analyze_resume\"\n",
    "            \n",
    "            prompt = prompt_templates[task]\n",
    "            logging.debug(f\"任务的prompt: {prompt}\") \n",
    "\n",
    "            try:\n",
    "                # 调用大语言模型API获取任务的处理结果\n",
    "                print(\"正在生成综合分析报告...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个帮助助手，负责根据用户提供的信息分析并生成建议。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                # 从大语言模型响应中提取分析结果\n",
    "                llm_analysis = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型返回的分析结果: {llm_analysis}\") \n",
    "                \n",
    "                # 合并LLM分析结果到profile_analysis\n",
    "                profile_analysis[\"llm_analysis\"] = llm_analysis\n",
    "            except Exception as e:\n",
    "                logging.error(f\"生成综合分析报告时出错: {e}\")\n",
    "                print(f\"生成综合分析报告时出错: {e}\")\n",
    "                profile_analysis[\"llm_analysis\"] = \"无法生成分析报告\"\n",
    "\n",
    "            # 更新上下文，保存用户画像分析结果\n",
    "            self.context.update({\"profile_analysis\": profile_analysis})\n",
    "            print(\"\\n用户画像分析完成！\")\n",
    "            return {\"profile_analysis\": profile_analysis}\n",
    "        else:\n",
    "            # 没有用户ID，回退到原来的方法\n",
    "            print(\"未找到用户ID，将使用简化版用户画像分析\")\n",
    "            # 定义不同任务的prompt模板\n",
    "            prompt_templates = {\n",
    "                \"analyze_resume\": f\"分析以下简历内容：{cv_text}\",\n",
    "                \"user_interest\": f\"根据以下简历内容，识别用户的职业兴趣和专注领域：{cv_text}\",\n",
    "                \"skill_assessment\": f\"根据以下简历内容，评估用户的技能并提出改进建议：{cv_text}\",\n",
    "            }\n",
    "\n",
    "            # 确保指定的任务在模板中存在\n",
    "            if task not in prompt_templates:\n",
    "                raise ValueError(f\"未知任务: {task}. 请定义一个有效的任务。\")\n",
    "                task = \"analyze_resume\"\n",
    "\n",
    "            prompt = prompt_templates[task]\n",
    "            logging.debug(f\"任务的prompt: {prompt}\")\n",
    "\n",
    "            try:\n",
    "                # 调用大语言模型API获取任务的处理结果\n",
    "                print(\"正在生成简化版分析报告...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个帮助助手，负责根据用户提供的信息分析并生成建议。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                # 从大语言模型响应中提取分析结果\n",
    "                profile_data = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型返回的分析结果: {profile_data}\") \n",
    "                \n",
    "                # 更新上下文，保存用户画像分析结果\n",
    "                profile_analysis = {\"text_analysis\": profile_data}\n",
    "                self.context.update({\"profile_analysis\": profile_analysis})\n",
    "            except Exception as e:\n",
    "                logging.error(f\"生成简化版分析报告时出错: {e}\")\n",
    "                print(f\"生成简化版分析报告时出错: {e}\")\n",
    "                profile_analysis = {\"text_analysis\": \"无法生成分析报告\"}\n",
    "                self.context.update({\"profile_analysis\": profile_analysis})\n",
    "                \n",
    "            print(\"\\n简化版用户画像分析完成！\")\n",
    "            return {\"profile_analysis\": profile_analysis}\n",
    "\n",
    "    def build_user_profile(self, user_input: Dict, cv_text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        构建用户画像，确保用户ID存在并分析用户简历\n",
    "        \n",
    "        Args:\n",
    "            user_input: 用户输入的基本信息\n",
    "            cv_text: 用户简历文本\n",
    "            \n",
    "        Returns:\n",
    "            用户画像信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if cv_text.strip():\n",
    "                print(\"已收到简历，开始分析...\")\n",
    "\n",
    "                # 确保用户有ID - 如果没有，创建一个新用户\n",
    "                if 'user_id' not in self.context and 'email' in user_input:\n",
    "                    print(\"检测到新用户，正在创建用户档案...\")\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '未知用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': user_input.get('email', '')\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建新用户，ID: {self.context['user_id']}\")\n",
    "\n",
    "                # 如果仍然没有用户ID（可能是因为没有提供邮箱），创建一个临时ID\n",
    "                if 'user_id' not in self.context:\n",
    "                    import hashlib\n",
    "                    import time\n",
    "                    temp_id = hashlib.md5(f\"{time.time()}-{user_input.get('occupation', '')}-temp\".encode()).hexdigest()\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '临时用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': f\"temp_{temp_id[:8]}@example.com\"  # 创建临时邮箱\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建临时用户，ID: {self.context['user_id']}\")\n",
    "                    print(\"注意：由于未提供邮箱，此用户为临时用户，数据可能不会长期保存\")\n",
    "\n",
    "                # 现在可以确保有用户ID了，继续进行用户画像分析\n",
    "                try:\n",
    "                    task = \"analyze_resume\"  # 或根据需要选择其他任务\n",
    "                    user_profile = self.get_user_profile(cv_text, task)\n",
    "                    # 更新用户画像信息\n",
    "                    if user_profile:\n",
    "                        self.context.update(user_profile)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"分析用户画像时出错: {e}\")\n",
    "                    print(f\"分析用户画像时出错: {e}\")\n",
    "                    print(\"将继续使用基本用户信息\")\n",
    "                    user_profile = {\"basic_profile\": True}\n",
    "\n",
    "                # 显示用户画像摘要\n",
    "                try:\n",
    "                    self.display_profile_summary()\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"显示用户画像摘要时出错: {e}\")\n",
    "                    print(f\"显示用户画像摘要时出错: {e}\")\n",
    "                \n",
    "                return user_profile or {\"basic_profile\": True}\n",
    "            else:\n",
    "                print(\"未提供简历，但仍将创建基本用户档案\")\n",
    "\n",
    "                # 即使没有简历，也要确保用户有ID\n",
    "                if 'user_id' not in self.context and 'email' in user_input:\n",
    "                    print(\"创建基本用户档案...\")\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '未知用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': user_input.get('email', '')\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建新用户，ID: {self.context['user_id']}\")\n",
    "\n",
    "                    # 从用户输入中提取基本兴趣\n",
    "                    if 'content_type' in user_input and user_input['content_type']:\n",
    "                        print(f\"基于您提供的关注领域'{user_input['content_type']}'添加初始兴趣\")\n",
    "                        try:\n",
    "                            self.user_profile_manager.update_interest_weights(\n",
    "                                self.context['user_id'],\n",
    "                                user_input['content_type'],\n",
    "                                0.8  # 较高的初始权重\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"添加初始兴趣时出错: {e}\")\n",
    "                            print(f\"添加初始兴趣时出错: {e}\")\n",
    "                \n",
    "                return {\"basic_profile\": True}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"构建用户画像时出错: {e}\")\n",
    "            print(f\"构建用户画像时出错: {e}\")\n",
    "            print(\"将继续使用基本用户信息\")\n",
    "            return {\"basic_profile\": True}\n",
    "\n",
    "    def display_profile_summary(self):\n",
    "        \"\"\"显示用户画像摘要\"\"\"\n",
    "        if 'user_id' in self.context:\n",
    "            try:\n",
    "                profile_summary = self.user_profile_manager.get_user_profile_summary(self.context['user_id'])\n",
    "                if not profile_summary or 'error' in profile_summary:\n",
    "                    print(\"\\n--- 用户画像摘要 ---\")\n",
    "                    print(f\"用户ID: {self.context['user_id']}\")\n",
    "                    print(\"无法获取完整的用户画像信息\")\n",
    "                    print(\"-----------------\\n\")\n",
    "                    return\n",
    "                    \n",
    "                print(\"\\n--- 用户画像摘要 ---\")\n",
    "                print(f\"用户ID: {self.context['user_id']}\")\n",
    "                print(f\"职业: {profile_summary.get('basic_info', {}).get('occupation', '未知')}\")\n",
    "                \n",
    "                # 安全地获取技能数量\n",
    "                skills = profile_summary.get('skills', [])\n",
    "                print(f\"技能数量: {len(skills)}\")\n",
    "                \n",
    "                # 安全地获取顶级兴趣\n",
    "                interests = profile_summary.get('top_interests', [])\n",
    "                print(\"顶级兴趣:\")\n",
    "                if interests:\n",
    "                    for interest in interests[:3]:\n",
    "                        print(f\"  - {interest.get('topic', '未知')} (权重: {interest.get('weight', 0):.2f})\")\n",
    "                else:\n",
    "                    print(\"  暂无兴趣数据\")\n",
    "                print(\"-----------------\\n\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"显示用户画像摘要时出错: {e}\")\n",
    "                print(f\"\\n显示用户画像摘要时出错: {e}\")\n",
    "                print(\"将继续执行后续步骤\")\n",
    "\n",
    "    def _update_interests_from_query(self, query: str, weight_adjustment: float = 0.05):\n",
    "        \"\"\"\n",
    "        从用户查询中提取可能的兴趣点并更新用户画像\n",
    "\n",
    "        Args:\n",
    "            query: 用户查询\n",
    "            weight_adjustment: 权重调整幅度\n",
    "        \"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "\n",
    "        # 使用LLM提取查询中的兴趣点\n",
    "        prompt = f\"\"\"\n",
    "        请从以下搜索查询中提取最多3个主要的兴趣领域或关键主题。\n",
    "        只返回提取的主题列表，格式为JSON数组，例如：[\"人工智能\", \"机器学习\", \"自然语言处理\"]\n",
    "\n",
    "        搜索查询: {query}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的主题提取助手，擅长从文本中识别核心主题。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            topics_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                topics = json.loads(topics_text)\n",
    "\n",
    "                # 更新每个主题的权重\n",
    "                for topic in topics:\n",
    "                    self.user_profile_manager.update_interest_weights(user_id, topic, weight_adjustment)\n",
    "                    logging.info(f\"从查询中更新用户兴趣: {topic}, 调整: +{weight_adjustment}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取查询主题时出错: {e}\")\n",
    "\n",
    "    async def translate_query(self, query):\n",
    "        \"\"\"使用多引擎翻译并比较结果\"\"\"\n",
    "        translations = {}\n",
    "\n",
    "        try:\n",
    "            translations['谷歌'] = GoogleTranslator(source='auto', target='en').translate(query)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"谷歌翻译失败: {e}\")\n",
    "\n",
    "        # try:\n",
    "        #     translations['百度'] = BaiduTranslator(appid='YOUR_ID', appkey='YOUR_KEY').translate(query, dst='en')\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"百度翻译失败: {e}\")\n",
    "\n",
    "        # 其他翻译API\n",
    "        # try:\n",
    "        #     translations['DeepL'] = DeepLTranslator(source='ZH', target='EN').translate(query)\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"DeepL翻译失败: {e}\")\n",
    "\n",
    "        if not translations:\n",
    "            logging.error(\"所有翻译引擎均失败\")\n",
    "            return query\n",
    "\n",
    "        translations_text = \"\\n\".join([f\"{engine}翻译：{result}\" for engine, result in translations.items()])\n",
    "\n",
    "        try:\n",
    "            validation = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是专业翻译验证助手。回答只用翻译后的内容本身！以下是不同引擎翻译的结果，请你思考它们从中文到英文翻译的准确性，并提供最准确的翻译。最终结果只用翻译后的内容本身。\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"原文：{query}\\n{translations_text}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            translated_query = validation.choices[0].message.content\n",
    "            translations['大模型'] = translated_query\n",
    "\n",
    "            # 如果有用户ID，记录这次翻译\n",
    "            if 'user_id' in self.context:\n",
    "                # 更新用户兴趣 - 从查询中提取可能的兴趣点\n",
    "                self._update_interests_from_query(query)\n",
    "\n",
    "            best_translation = None\n",
    "            best_similarity = -1\n",
    "\n",
    "            for engine, translated_text in translations.items():\n",
    "                similarity = compute_similarity(query, translated_text)\n",
    "                logging.debug(f\"{engine}翻译为：{translated_text}，相似度: {similarity}\")\n",
    "\n",
    "                if similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_translation = translated_text\n",
    "\n",
    "            logging.info(f\"最终翻译结果:{best_translation} ，相似度是：{best_similarity}\")\n",
    "            return translated_query # 目前匹配效果不佳，暂用大模型结果\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"大语言模型验证失败: {e}\")\n",
    "            # 如果验证失败，返回第一个可用的翻译结果\n",
    "            return next(iter(translations.values()))\n",
    "            \n",
    "    async def build_search_query(self) -> Dict:\n",
    "        \"\"\"根据用户需求构建搜索查询，并将查询词转换为英文\"\"\"\n",
    "        logging.info(\"构建搜索查询并进行翻译...\")  \n",
    "\n",
    "        query_base = self.context.get('content_type', \" \".join(self.context.get('content_focus', [])))\n",
    "\n",
    "        # 如果有用户ID，基于用户兴趣增强查询\n",
    "        if 'user_id' in self.context:\n",
    "            user_id = self.context['user_id']\n",
    "            top_interests = self.user_profile_manager.get_top_interests(user_id, limit=3)\n",
    "\n",
    "            if top_interests:\n",
    "                # 获取顶部兴趣与当前查询的关系\n",
    "                prompt = f\"\"\"\n",
    "                以下是用户的当前搜索请求和顶级兴趣。\n",
    "                请判断这些兴趣是否可以用于增强当前搜索，如果可以，请提供一个更优化的搜索查询。\n",
    "                如果不应该增强，只返回原始查询。\n",
    "\n",
    "                当前搜索请求: {query_base}\n",
    "                用户顶级兴趣: {\", \".join([interest[\"topic\"] for interest in top_interests])}\n",
    "                \"\"\"\n",
    "\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"你是一个专业的搜索优化助手，擅长根据用户兴趣优化搜索查询。\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    enhanced_query = response.choices[0].message.content\n",
    "                    if enhanced_query and enhanced_query != query_base and \"不应该增强\" not in enhanced_query:\n",
    "                        logging.info(f\"基于用户兴趣增强查询: {query_base} -> {enhanced_query}\")\n",
    "                        query_base = enhanced_query\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"增强查询时出错: {e}\")\n",
    "\n",
    "        # 翻译查询\n",
    "        translated_query_base = await self.translate_query(query_base)\n",
    "        if translated_query_base == query_base:\n",
    "            logging.warning(\"翻译内容与原查询一致，可能未成功翻译。\")\n",
    "    \n",
    "        # 构建基于时间范围的搜索查询\n",
    "        time_range = f\"after:{self.context['update_cycle']['start_date']} before:{self.context['update_cycle']['end_date']}\"\n",
    "        logging.debug(f\"构建的搜索查询：query_google={translated_query_base} {time_range} site:google.com\")  \n",
    "        \n",
    "        # 如果有用户ID，记录这次搜索\n",
    "        if 'user_id' in self.context:\n",
    "            self.user_profile_manager.record_search(\n",
    "                self.context['user_id'],\n",
    "                query_base,\n",
    "                self.context.get('platform', '未指定')\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"query_google\": f\"{translated_query_base} {time_range} site:google.com\",\n",
    "            \"query_arxiv\": f'\\\"{translated_query_base}\\\" AND submittedDate:[{self.context[\"update_cycle\"][\"start_date\"].replace(\"-\", \"\")} TO {self.context[\"update_cycle\"][\"end_date\"].replace(\"-\", \"\")}]',\n",
    "            \"query_google_arxiv\": f\"{translated_query_base} {time_range} arXiv site:arxiv.org\"\n",
    "        }\n",
    "\n",
    "    def execute_search(self, queries: Dict) -> Dict:\n",
    "        \"\"\"执行实际的搜索操作\"\"\"\n",
    "        logging.info(\"执行搜索操作...\")  \n",
    "        results = {}\n",
    "\n",
    "        # 获取用户选择的平台类型\n",
    "        platform_type = self.context.get('platform', '新闻类')\n",
    "\n",
    "        # 根据不同的平台类型调整搜索策略和结果数量\n",
    "        if platform_type == \"学术期刊\":\n",
    "            # 学术期刊类：只使用ArXiv搜索，最多返回7条结果\n",
    "            if queries.get('query_google_arxiv'):\n",
    "                logging.info(\"用户选择学术期刊类，执行Google_ArXiv搜索...\")\n",
    "                logging.debug(f\"执行Google_ArXiv搜索: {queries['query_google_arxiv']}\")\n",
    "                results['google_arxiv'] = self.arxiv_search(queries['query_google_arxiv'], max_results=7)\n",
    "            if queries.get('query_arxiv'):\n",
    "                logging.info(\"用户选择学术期刊类，执行ArXiv搜索...\")\n",
    "                logging.debug(f\"执行ArXiv搜索: {queries['query_arxiv']}\") \n",
    "                results['arxiv'] = self.arxiv_search(queries['query_arxiv'], max_results=7)\n",
    "\n",
    "                \n",
    "        elif platform_type == \"新闻类\":\n",
    "            # 新闻类：只使用Google搜索，最多返回7条结果\n",
    "            logging.info(\"用户选择新闻类，执行Google搜索...\")\n",
    "            if queries.get('query_google'):\n",
    "                logging.debug(f\"执行Google搜索: {queries['query_google']}\")  \n",
    "                results['google'] = self.google_search(queries['query_google'], max_results=7)\n",
    "                \n",
    "        else:  # 综合类或其他类型\n",
    "            # 综合类：同时使用Google和ArXiv搜索，各返回最多4条结果\n",
    "            logging.info(\"用户选择综合类，执行Google和ArXiv搜索...\")\n",
    "            if queries.get('query_google'):\n",
    "                logging.debug(f\"执行Google搜索: {queries['query_google']}\")  \n",
    "                results['google'] = self.google_search(queries['query_google'], max_results=4)\n",
    "            if queries.get('query_arxiv'):\n",
    "                logging.debug(f\"执行ArXiv搜索: {queries['query_arxiv']}\") \n",
    "                results['arxiv'] = self.arxiv_search(queries['query_arxiv'], max_results=4)\n",
    "            if queries.get('query_google_arxiv'):\n",
    "                logging.debug(f\"执行Google_ArXiv搜索: {queries['query_google_arxiv']}\") \n",
    "                results['google_arxiv'] = self.arxiv_search(queries['query_google_arxiv'], max_results=4)\n",
    "\n",
    "        logging.debug(f\"搜索结果: {results}\")  \n",
    "        return results\n",
    "\n",
    "\n",
    "    def google_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行Google搜索，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行Google搜索，限制结果数量为{max_results}...\")  \n",
    "        api_url = f\"https://google.serper.dev/search?q={query}&num={max_results}\"\n",
    "        headers = {'X-API-KEY': self.serper_api_key}\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            logging.debug(f\"Google搜索返回结果数量: {len(search_results.get('organic', []))}\")\n",
    "            logging.debug(f\"Google搜索结果: {search_results}\")\n",
    "            # 限制结果数量\n",
    "            if 'organic' in search_results:\n",
    "                search_results['organic'] = search_results['organic'][:max_results]\n",
    "            return self.parse_google_results(search_results, query)\n",
    "        else:\n",
    "            logging.error(f\"Google搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"Google搜索失败\"}\n",
    "    \n",
    "    def parse_google_results(self, data: Dict, query: str) -> Dict:\n",
    "        \"\"\"解析Google搜索结果\"\"\"\n",
    "        results = []\n",
    "        for item in data.get('organic', []):\n",
    "            title = item.get('title', '')\n",
    "            snippet = item.get('snippet', '')[:1400]  # 限制摘要长度\n",
    "            link = item.get('link', '')\n",
    "            date = item.get('date', '')\n",
    "            position = item.get('position', '')\n",
    "\n",
    "            # 计算标题和摘要与查询的相似度\n",
    "            title_similarity = compute_similarity(query, title)\n",
    "            snippet_similarity = compute_similarity(query, snippet)\n",
    "            overall_similarity = 0.3 * title_similarity + 0.7 * snippet_similarity\n",
    "\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'snippet': snippet,\n",
    "                'link': link,\n",
    "                'date': date,\n",
    "                'position': position,\n",
    "                'similarity': overall_similarity,\n",
    "            })\n",
    "\n",
    "        # 按相似度排序\n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "        return {'results': results}\n",
    "\n",
    "    def arxiv_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行ArXiv搜索，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行ArXiv搜索，限制结果数量为{max_results}...\")\n",
    "        api_url = f'http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}'\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            #logging.debug(f\"ArXiv API response (first 500 chars): {response.text[:500]}...\")\n",
    "            arxiv_results = self.parse_arxiv_response(response.text, query)\n",
    "            logging.debug(f\"ArXiv搜索返回结果数量: {len(arxiv_results.get('results', []))}\")\n",
    "            return arxiv_results\n",
    "        else:\n",
    "            logging.error(f\"ArXiv搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"ArXiv搜索失败\"}\n",
    "\n",
    "    def parse_arxiv_response(self, xml_data: str, query: str) -> Dict:\n",
    "        \"\"\"解析ArXiv的响应数据\"\"\"\n",
    "        tree = ElementTree.fromstring(xml_data)\n",
    "        results = []\n",
    "        for entry in tree.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "\n",
    "            link = \"\"\n",
    "            for link_element in entry.findall(\"{http://www.w3.org/2005/Atom}link\"):\n",
    "                if link_element.get(\"rel\") == \"alternate\":\n",
    "                    link = link_element.get(\"href\")\n",
    "                    break\n",
    "\n",
    "            if not link:\n",
    "                link_element = entry.find(\"{http://www.w3.org/2005/Atom}link\")\n",
    "                if link_element is not None:\n",
    "                    link = link_element.get(\"href\", \"\")\n",
    "\n",
    "            # 计算标题和摘要与查询的相似度\n",
    "            title_similarity = compute_similarity(query, title)\n",
    "            summary_similarity = compute_similarity(query, summary)\n",
    "            overall_similarity = 0.7 * title_similarity + 0.3 * summary_similarity\n",
    "\n",
    "            # 提取发布日期\n",
    "            # date = entry.find(\"{http://www.w3.org/2005/Atom}published\").text if entry.find(\"{http://www.w3.org/2005/Atom}published\") is not None else \"\"\n",
    "\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'snippet': summary[:1400],\n",
    "                'link': link,\n",
    "                #'date': date,\n",
    "                'similarity': overall_similarity\n",
    "            })\n",
    "\n",
    "        # 按照相似度进行排序\n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "        return {\"results\": results}\n",
    "\n",
    "    def google_arxiv_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行Google搜索，用于搜索ArXiv文献，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行Google搜索用于查找ArXiv文献，限制结果数量为{max_results}...\")  \n",
    "        api_url = f\"https://google.serper.dev/search?q={query}&num={max_results}\"\n",
    "        headers = {'X-API-KEY': self.serper_api_key}  \n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            logging.debug(f\"Google_ArXiv搜索返回结果数量: {len(search_results.get('organic', []))}\")\n",
    "            if 'organic' in search_results:\n",
    "                search_results['organic'] = search_results['organic'][:max_results]\n",
    "            #logging.debug(f\"Google ArXiv搜索结果: {search_results}\") \n",
    "            return self.parse_google_results(search_results, query)\n",
    "        else:\n",
    "            logging.error(f\"Google ArXiv搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"Google ArXiv搜索失败\"}\n",
    "    \n",
    "    def integrate_with_large_model(self, search_results: Dict) -> str:\n",
    "        \"\"\"调用大语言模型进行整合\"\"\"\n",
    "        logging.info(\"调用大语言模型进行整合搜索结果...\")  \n",
    "    \n",
    "        # 将搜索结果转换为大语言模型所需的格式\n",
    "        search_results_str = json.dumps(search_results, ensure_ascii=False, cls=NumpyEncoder)\n",
    "\n",
    "        try:\n",
    "            # 调用大语言模型的API进行结果整合\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                # model=\"qwen-plus\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"你是一个内容整理助手，负责根据用户提供的信息整合搜索结果并生成报告。使用中文。不要md格式。\"\n",
    "                            \"将报告中的内容，以：\\\"来源：...（Google/arXiv/Google_arXiv等平台） \\n 标题：... \\n 摘要：... \\n 原文网址：...（只使用搜索结果中提供的真实链接）\\n BERT嵌入的余弦相似度:... \\\"的形式呈现出来。\"\n",
    "                            \"如果搜索结果中没有提供原文网址，则写'原文网址：未提供'。不要编造或猜测网址。\"\n",
    "                            \"报告使用用户交谈时的语言，如果原文不是，则准确的转化为用户使用的语言。目前的用户使用的是中文，将结果也转化为中文！\"\n",
    "                            \"如果无法完成就直接翻译用snippet的内容回答将\\\"摘要：\\\"改为\\\"片段：\\\"。\"\n",
    "                            \"并且回答严格按照规范来，就算无法完成任务也不要说别的不符合规范的话。\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"请整合以下搜索结果并生成最终报告：{search_results_str}\"\n",
    "                    }\n",
    "                ],\n",
    "                stream=False\n",
    "            )\n",
    "            if response and hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "                integration_result = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型整合结果: {integration_result}\")\n",
    "            else:\n",
    "                raise ValueError(\"API响应没有有效的choices字段\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"调用大语言模型整合时发生错误: {e}\")\n",
    "            integration_result = \"由于API错误，无法生成整合结果。\"\n",
    "        \n",
    "        return integration_result\n",
    "    \n",
    "    def _extract_interest_from_content(self, title: str, snippet: str, weight_adjustment: float = 0.03):\n",
    "        \"\"\"从内容中提取兴趣点并更新用户模型\"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "        combined_text = f\"{title}\\n{snippet}\"\n",
    "\n",
    "        # 提取主题\n",
    "        prompt = f\"\"\"\n",
    "        请从以下文本中提取最多3个核心学术或专业主题。\n",
    "        只返回主题列表，格式为JSON数组，例如：[\"强化学习\", \"计算机视觉\", \"神经网络\"]\n",
    "\n",
    "        文本:\n",
    "        {combined_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的主题提取助手，擅长从文本中识别核心学术主题。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            topics_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                topics = json.loads(topics_text)\n",
    "\n",
    "                # 更新每个主题的权重\n",
    "                for topic in topics:\n",
    "                    self.user_profile_manager.update_interest_weights(user_id, topic, weight_adjustment)\n",
    "                    logging.info(f\"从内容中提取用户兴趣: {topic}, 调整: +{weight_adjustment}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取内容主题时出错: {e}\")\n",
    "\n",
    "    def generate_report(self, search_results: Dict) -> str:\n",
    "        \"\"\"生成最终的搜索报告\"\"\"\n",
    "        logging.info(\"生成最终的搜索报告...\") \n",
    "        report = []\n",
    "        platform_type = self.context.get('platform')\n",
    "        \n",
    "        # 先构建Google、ArXiv等来源的报告内容\n",
    "        for source, data in search_results.items():\n",
    "            if 'error' in data:\n",
    "                report.append(f\"来源：{source}\\n错误：{data['error']}\")\n",
    "            else:\n",
    "                for item in data.get('results', [])[:3]: \n",
    "                    if isinstance(item, dict): \n",
    "                        title = item.get('title', '')\n",
    "                        snippet = item.get('snippet', '')\n",
    "                        link = item.get('link', '')\n",
    "                        report.append(f\"来源：{source}\\n标题：{title}\\n摘要：{snippet}...\\n链接：{link}\")\n",
    "\n",
    "                    # 如果有用户ID，记录这个内容为潜在兴趣点\n",
    "                    # if 'user_id' in self.context and link:\n",
    "                    #     # 从标题中提取可能的兴趣点\n",
    "                    #     self._extract_interest_from_content(title, snippet, weight_adjustment=0.03)\n",
    "\n",
    "                    #     # 记录内容交互\n",
    "                    #     self.user_profile_manager.record_interaction(\n",
    "                    #         self.context['user_id'],\n",
    "                    #         link,\n",
    "                    #         \"search_result\"\n",
    "                    #     )\n",
    "\n",
    "        # 再调用大语言模型整合结果\n",
    "        final_report = self.integrate_with_large_model(search_results)\n",
    "    \n",
    "        # 最终替换为大语言模型整合后的内容\n",
    "        report.clear()\n",
    "\n",
    "        # 如果有用户ID，记录这个内容为潜在兴趣点\n",
    "        # if 'user_id' in self.context and link:\n",
    "        #     # 从标题中提取可能的兴趣点\n",
    "        #     self._extract_interest_from_content(title, snippet, weight_adjustment=0.03)\n",
    "\n",
    "        #     # 记录内容交互\n",
    "        #     self.user_profile_manager.record_interaction(\n",
    "        #         self.context['user_id'],\n",
    "        #         link,\n",
    "        #         \"search_result\"\n",
    "        #     )\n",
    "\n",
    "        # 如果有用户ID，添加个性化推荐\n",
    "        # if 'user_id' in self.context:\n",
    "        #     # 应用时间衰减模型\n",
    "        #     self.user_profile_manager.apply_time_decay(self.context['user_id'])\n",
    "\n",
    "        #     # 获取推荐内容\n",
    "        #     try:\n",
    "        #         recommendations = self.user_profile_manager.generate_recommendations(self.context['user_id'], count=3)\n",
    "\n",
    "        #         # 添加推荐内容到报告\n",
    "        #         if recommendations and len(recommendations) > 0:\n",
    "        #             rec_text = \"\\n\\n--- 基于您的兴趣，我们还为您推荐以下主题 ---\\n\\n\"\n",
    "        #             for rec in recommendations:\n",
    "        #                 rec_text += f\"主题：{rec.get('topic', '')}\\n\"\n",
    "        #                 rec_text += f\"原因：{rec.get('reason', '')}\\n\\n\"\n",
    "\n",
    "        #             report.append(rec_text)\n",
    "        #     except Exception as e:\n",
    "        #         logging.error(f\"生成推荐时出错: {e}\")\n",
    "\n",
    "        report.append(f\"\\n\\n--- 根据您选择的【{platform_type}】平台，近几日的行业内最新进展已整理好，请查收！ ---\\n\")\n",
    "        report.append(final_report)\n",
    "\n",
    "        logging.debug(f\"生成的报告内容: {report}\")\n",
    "        return \"\\n\\n\".join(report)\n",
    "\n",
    "    def send_email(self, report: str):\n",
    "        \"\"\"发送邮件（示例）\"\"\"\n",
    "        logging.info(\"准备发送邮件...\")\n",
    "        if 'Mail_number' in self.context and report:\n",
    "            print(f\"已发送邮件到 {self.context['Mail_number']}:\\n{report}\")\n",
    "        else:\n",
    "            print(\"未找到邮件地址或报告为空，未发送邮件。\")\n",
    "\n",
    "    def process_user_feedback(self, content_id: str, feedback_type: str, feedback_text: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        处理用户对内容的反馈\n",
    "\n",
    "        Args:\n",
    "            content_id: 内容ID（URL或其他标识）\n",
    "            feedback_type: 反馈类型（\"like\"、\"dislike\"、\"save\"、\"share\"等）\n",
    "            feedback_text: 反馈文本（可选）\n",
    "\n",
    "        Returns:\n",
    "            处理结果描述\n",
    "        \"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return \"无用户信息，无法处理反馈\"\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "\n",
    "        # 记录交互\n",
    "        self.user_profile_manager.record_interaction(user_id, content_id, feedback_type)\n",
    "\n",
    "        # 根据反馈类型调整兴趣权重\n",
    "        if feedback_type in (\"like\", \"save\", \"share\"):\n",
    "            # 提取内容关联主题并增加权重\n",
    "            try:\n",
    "                # 获取内容摘要（实际应用中可能需要从数据库或通过URL获取）\n",
    "                content_summary = feedback_text if feedback_text else \"用户喜欢的内容\"\n",
    "\n",
    "                # 提取主题\n",
    "                prompt = f\"\"\"\n",
    "                请从以下用户反馈中提取最多2个可能的兴趣主题。\n",
    "                只返回主题列表，格式为JSON数组。\n",
    "\n",
    "                用户反馈类型: {feedback_type}\n",
    "                反馈内容: {content_summary}\n",
    "                \"\"\"\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长分析用户反馈。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                topics_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    topics = json.loads(topics_text)\n",
    "\n",
    "                    # 增加各主题权重\n",
    "                    for topic in topics:\n",
    "                        self.user_profile_manager.update_interest_weights(user_id, topic, 0.1)\n",
    "                        logging.info(f\"基于正面反馈增加兴趣权重: {topic} +0.1\")\n",
    "\n",
    "                    return f\"已记录您对'{content_id}'的{feedback_type}反馈，并更新了您的兴趣模型\"\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "                    return \"已记录您的反馈，但分析主题时出错\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理反馈时出错: {e}\")\n",
    "                return f\"处理反馈时出错: {str(e)}\"\n",
    "\n",
    "        elif feedback_type == \"dislike\":\n",
    "            # 提取内容关联主题并减少权重\n",
    "            try:\n",
    "                content_summary = feedback_text if feedback_text else \"用户不喜欢的内容\"\n",
    "\n",
    "                # 提取主题\n",
    "                prompt = f\"\"\"\n",
    "                请从以下用户不喜欢的内容中提取最多2个可能的兴趣主题。\n",
    "                只返回主题列表，格式为JSON数组。\n",
    "\n",
    "                不喜欢的内容: {content_summary}\n",
    "                \"\"\"\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长分析用户反馈。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                topics_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    topics = json.loads(topics_text)\n",
    "\n",
    "                    # 减少各主题权重\n",
    "                    for topic in topics:\n",
    "                        self.user_profile_manager.update_interest_weights(user_id, topic, -0.1)\n",
    "                        logging.info(f\"基于负面反馈减少兴趣权重: {topic} -0.1\")\n",
    "\n",
    "                    return f\"已记录您对'{content_id}'的不喜欢反馈，并更新了您的兴趣模型\"\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "                    return \"已记录您的反馈，但分析主题时出错\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理反馈时出错: {e}\")\n",
    "                return f\"处理反馈时出错: {str(e)}\"\n",
    "\n",
    "        return f\"已记录您对'{content_id}'的{feedback_type}反馈\"\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import docx\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "class ResumeReader:\n",
    "    \"\"\"用于读取多种格式简历文件的类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.supported_formats = {\n",
    "            '.txt': self.read_txt,\n",
    "            '.pdf': self.read_pdf,\n",
    "            '.docx': self.read_docx,\n",
    "            '.doc': self.read_doc,\n",
    "            '.xlsx': self.read_excel,\n",
    "            '.xls': self.read_excel,\n",
    "            '.jpg': self.read_image,\n",
    "            '.jpeg': self.read_image,\n",
    "            '.png': self.read_image,\n",
    "        }\n",
    "        logging.info(\"初始化ResumeReader，支持的格式：%s\", list(self.supported_formats.keys()))\n",
    "\n",
    "    def read_resume(self, file_path=None):\n",
    "        \"\"\"读取简历文件或请求用户输入\"\"\"\n",
    "        if not file_path:\n",
    "            choice = input(\"请选择输入方式：1.直接输入文本 2.上传文件 (如有需要进行用户画像构建请输入数字，不需要可回车（或确认）跳过): \")\n",
    "\n",
    "            if choice == \"\":\n",
    "                return \"\"\n",
    "            elif choice == \"1\":\n",
    "                return input(\"请输入您的简历文本：\")\n",
    "            elif choice == \"2\":\n",
    "                file_path = input(\"请输入简历文件的完整路径：\").strip().strip('\"')\n",
    "            else:\n",
    "                logging.warning(\"无效的选择，默认使用文本输入方式\")\n",
    "                return input(\"请输入您的简历文本：\")\n",
    "\n",
    "        while not os.path.exists(file_path):\n",
    "            logging.error(f\"文件不存在: {file_path}\")\n",
    "            choice = input(f\"文件不存在: {file_path}\\n请输入有效的文件路径，或输入 'q' 退出：\").strip()\n",
    "\n",
    "            if choice.lower() == 'q':\n",
    "                logging.info(\"用户选择退出\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                file_path = choice.strip()\n",
    "\n",
    "        file_path = file_path.strip('\\\"')\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if file_ext not in self.supported_formats:\n",
    "            logging.warning(f\"不支持的文件格式: {file_ext}，支持的格式有: {list(self.supported_formats.keys())}\")\n",
    "            return self.ask_for_input()\n",
    "\n",
    "        # 调用相应的文件读取方法\n",
    "        try:\n",
    "            text = self.supported_formats[file_ext](file_path)\n",
    "            logging.info(f\"成功读取{file_ext}格式简历文件\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"读取文件时出错: {e}\")\n",
    "            return self.ask_for_input()\n",
    "\n",
    "    def ask_for_input(self):\n",
    "        \"\"\"帮助用户提供错误反馈并重新输入\"\"\"\n",
    "        return input(\"请输入您的简历文本：\")\n",
    "\n",
    "    def read_txt(self, file_path):\n",
    "        \"\"\"读取txt文本文件\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # 尝试其他编码\n",
    "            with open(file_path, 'r', encoding='gbk') as f:\n",
    "                return f.read()\n",
    "\n",
    "    def read_pdf(self, file_path):\n",
    "        \"\"\"读取PDF文件\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def read_docx(self, file_path):\n",
    "        \"\"\"读取Word docx文件\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    def read_doc(self, file_path):\n",
    "        \"\"\"读取旧版 Word doc文件 (需要转换)\"\"\"\n",
    "        logging.warning(\"直接读取.doc文件需要额外依赖，建议转换为.docx或.pdf格式\")\n",
    "        return f\"无法直接读取.doc文件: {file_path}，请转换为.docx或.pdf格式后重试。\"\n",
    "\n",
    "    def read_excel(self, file_path):\n",
    "        \"\"\"读取Excel文件\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    def read_image(self, file_path):\n",
    "        \"\"\"使用OCR读取图片中的文本\"\"\"\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(img, lang='chi_sim+eng')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"OCR处理图片时出错: {e}\")\n",
    "            return f\"OCR处理图片时出错: {e}\"\n",
    "\n",
    "def collect_user_input() -> Dict:\n",
    "    \"\"\"收集真实用户输入\"\"\"\n",
    "    print(\"\\n===== 欢迎使用KnowlEdge系统 =====\")\n",
    "    print(\"请提供以下信息，以便我们为您提供个性化的行业知识更新\")\n",
    "\n",
    "    # user_name = input(\"请输入您的用户名: \").strip()\n",
    "    # occupation = input(\"请输入您的职业: \").strip() or \"算法工程师\"\n",
    "    # days = int(input(\"请输入获取知识更新周期（天数，默认10）: \").strip() or \"10\")\n",
    "    # platform = input(\"请输入消息来源平台（学术期刊/新闻类/综合类，默认学术期刊）: \").strip() or \"学术期刊\"\n",
    "    # content_type = input(\"请输入关注领域（如：大语言模型，默认大语言模型）: \").strip() or \"大语言模型\"\n",
    "    # email = input(\"请输入您的邮箱（用于接收报告和识别用户）: \").strip() or \"example@example.com\"\n",
    "\n",
    "    print(\"\\n您的信息已收集完毕，系统将基于这些信息为您提供个性化服务\")\n",
    "\n",
    "    user_name = \"Tssword\"\n",
    "    occupation = \"算法工程师\"\n",
    "    days = 10\n",
    "    platform = \"学术期刊\"\n",
    "    content_type = \"大语言模型\"\n",
    "    email = \"114514@qq.com\"\n",
    "\n",
    "    return {\n",
    "        \"user_name\": user_name,\n",
    "        \"occupation\": occupation,\n",
    "        \"day\": days,\n",
    "        \"platform\": platform,\n",
    "        \"content_type\": content_type,\n",
    "        \"email\": email\n",
    "    }\n",
    "    \n",
    "async def main():\n",
    "    print(\"\\n===== KnowlEdge系统启动 =====\")\n",
    "\n",
    "    # 验证数据库\n",
    "    if not verify_database():\n",
    "        print(\"数据库验证失败，系统可能无法正常工作\")\n",
    "        choice = input(\"是否继续运行? (y/n): \").strip().lower()\n",
    "        if choice != 'y':\n",
    "            print(\"系统退出\")\n",
    "            return\n",
    "\n",
    "    # 检查系统初始化状态\n",
    "    if not os.path.exists(CONFIG[\"DATA_DIR\"]) or not os.path.exists(CONFIG[\"DB_PATH\"]):\n",
    "        print(\"系统尚未初始化，正在进行初始化...\")\n",
    "        # 导入并运行初始化脚本\n",
    "        try:\n",
    "            import init_system\n",
    "            init_result = init_system.main()\n",
    "            if not init_result:\n",
    "                print(\"系统初始化失败，请检查日志并解决问题后重试\")\n",
    "                return\n",
    "        except ImportError:\n",
    "            print(\"找不到初始化脚本，请确保init_system.py文件存在\")\n",
    "            return\n",
    "\n",
    "    workflow = KnowledgeFlow()\n",
    "    print(\"KnowledgeFlow引擎已初始化\")\n",
    "\n",
    "    # 步骤 1：收集用户输入\n",
    "    print(\"\\n步骤 1/6: 收集用户信息\")\n",
    "    user_input = collect_user_input()\n",
    "    context = workflow.start_node(user_input)\n",
    "    print(\"用户信息已收集并处理\")\n",
    "\n",
    "    # 步骤 2：分析用户画像（可选）\n",
    "    print(\"\\n步骤 2/6: 用户画像分析\")\n",
    "    resume_reader = ResumeReader()\n",
    "    print(\"请提供您的简历以进行更精确的用户画像分析（可选）\")\n",
    "    cv_text = resume_reader.read_resume()\n",
    "\n",
    "    # 构建用户画像\n",
    "    workflow.build_user_profile(user_input, cv_text)\n",
    "\n",
    "    # 步骤 3：构建搜索参数\n",
    "    print(\"\\n步骤 3/6: 构建搜索参数\")\n",
    "    print(\"正在根据您的需求和兴趣构建搜索参数...\")\n",
    "    queries = await workflow.build_search_query()\n",
    "    print(f\"搜索参数构建完成，将在以下平台搜索: {', '.join(queries.keys())}\")\n",
    "\n",
    "\n",
    "    # 步骤 4：执行Google搜索、ArXiv搜索、和Google搜索ArXiv文献\n",
    "    print(\"\\n步骤 4/6: 执行搜索\")\n",
    "    print(f\"正在搜索与{user_input['content_type']}相关的最新信息...\")\n",
    "    search_results = workflow.execute_search(queries)\n",
    "    result_count = sum(len(data.get('results', [])) for source, data in search_results.items() if 'results' in data)\n",
    "    print(f\"搜索完成，共找到 {result_count} 条相关信息\")\n",
    "\n",
    "    # 步骤 5：生成报告\n",
    "    print(\"\\n步骤 5/6: 生成报告\")\n",
    "    print(\"正在整合搜索结果并生成报告...\")\n",
    "    report = workflow.generate_report(search_results)\n",
    "    print(\"报告生成完成\")\n",
    "\n",
    "    # 步骤 6：发送邮件\n",
    "    print(\"\\n步骤 6/6: 发送报告\")\n",
    "    workflow.send_email(report)\n",
    "\n",
    "    print(\"流程执行完成\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0940e9efdc9d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
