{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ef6044-3020-4ec5-b314-07833bdd811f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T10:21:42.663558Z",
     "start_time": "2025-03-16T10:18:28.016350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== KnowlEdge系统启动 =====\n",
      "\n",
      "验证数据库...\n",
      "数据库验证成功：可以正常写入和读取数据\n",
      "数据库包含以下表: users, user_interests, sqlite_sequence, user_searches, user_interactions, user_skills\n",
      "表 users: 2 条记录\n",
      "表 user_interests: 0 条记录\n",
      "表 sqlite_sequence: 3 条记录\n",
      "表 user_searches: 1 条记录\n",
      "表 user_interactions: 0 条记录\n",
      "表 user_skills: 112 条记录\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:18:32,266 - INFO - 已加载兴趣分类体系，共 6 个类别\n",
      "2025-03-16 18:18:32,268 - INFO - 用户画像管理器初始化完成\n",
      "2025-03-16 18:18:32,269 - INFO - 开始收集用户输入信息...\n",
      "2025-03-16 18:18:32,272 - INFO - 用户已存在: d9ab58acfe0accd14133c23ba5e7d8f5\n",
      "2025-03-16 18:18:32,272 - INFO - 计算时间范围...\n",
      "2025-03-16 18:18:32,273 - INFO - 初始化ResumeReader，支持的格式：['.txt', '.pdf', '.docx', '.doc', '.xlsx', '.xls', '.jpg', '.jpeg', '.png']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeFlow引擎已初始化\n",
      "\n",
      "步骤 1/6: 收集用户信息\n",
      "\n",
      "===== 欢迎使用KnowlEdge系统 =====\n",
      "请提供以下信息，以便我们为您提供个性化的行业知识更新\n",
      "\n",
      "您的信息已收集完毕，系统将基于这些信息为您提供个性化服务\n",
      "用户信息已收集并处理\n",
      "\n",
      "步骤 2/6: 用户画像分析\n",
      "请提供您的简历以进行更精确的用户画像分析（可选）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:18:44,142 - INFO - 成功读取.pdf格式简历文件\n",
      "2025-03-16 18:18:44,145 - INFO - 执行任务：analyze_resume，分析简历内容...\n",
      "2025-03-16 18:18:44,316 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已收到简历，开始分析...\n",
      "\n",
      "===== 开始执行用户画像分析任务：analyze_resume =====\n",
      "用户ID: d9ab58acfe0accd14133c23ba5e7d8f5\n",
      "\n",
      "第1步：提取用户技能\n",
      "\n",
      "开始从简历中提取最重要的8项技能...\n",
      "正在分析简历中的技能...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:19:02,414 - INFO - 从简历中提取了 8 项技能\n",
      "2025-03-16 18:19:02,416 - ERROR - 分析用户画像时出错: 'NoneType' object has no attribute 'items'\n",
      "2025-03-16 18:19:02,420 - INFO - 构建搜索查询并进行翻译...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取 8 项技能\n",
      "保存技能 1/8: 大模型Agent开发\n",
      "保存技能 2/8: 自然语言处理\n",
      "保存技能 3/8: Prompt Engineering\n",
      "保存技能 4/8: C++\n",
      "保存技能 5/8: Python\n",
      "保存技能 6/8: 数学建模\n",
      "保存技能 7/8: 机器学习\n",
      "保存技能 8/8: 团队合作与管理\n",
      "所有技能已保存到数据库\n",
      "技能提取完成，共 8 项\n",
      "\n",
      "第2步：提取用户兴趣\n",
      "\n",
      "开始从简历中提取最重要的8项兴趣...\n",
      "分析用户画像时出错: 'NoneType' object has no attribute 'items'\n",
      "将继续使用基本用户信息\n",
      "\n",
      "--- 用户画像摘要 ---\n",
      "用户ID: d9ab58acfe0accd14133c23ba5e7d8f5\n",
      "用户名: Tssword1\n",
      "职业: 算法工程师\n",
      "技能数量: 24\n",
      "顶级兴趣:\n",
      "  暂无兴趣数据\n",
      "-----------------\n",
      "\n",
      "\n",
      "步骤 3/6: 构建搜索参数\n",
      "正在根据您的需求和兴趣构建搜索参数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:19:02,979 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-16 18:19:12,708 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-16 18:19:25,339 - ERROR - 提取查询主题时出错: 'NoneType' object has no attribute 'items'\n",
      "2025-03-16 18:19:25,499 - INFO - 谷歌翻译为：Big language model，相似度: 0.7646453380584717\n",
      "2025-03-16 18:19:25,556 - INFO - 大模型翻译为：Large language model，相似度: 0.5579333901405334\n",
      "2025-03-16 18:19:25,556 - INFO - 最终翻译结果:Big language model ，相似度是：0.7646453380584717\n",
      "2025-03-16 18:19:25,589 - INFO - 记录用户搜索: d9ab58acfe0accd14133c23ba5e7d8f5, 查询: 大语言模型\n",
      "2025-03-16 18:19:25,590 - INFO - 执行搜索操作...\n",
      "2025-03-16 18:19:25,591 - INFO - 用户选择学术期刊类，执行Google_ArXiv搜索...\n",
      "2025-03-16 18:19:25,592 - INFO - 执行ArXiv搜索，限制结果数量为7...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索参数构建完成，将在以下平台搜索: query_google, query_arxiv, query_google_arxiv\n",
      "\n",
      "步骤 4/6: 执行搜索\n",
      "正在搜索与大语言模型相关的最新信息...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:19:28,630 - INFO - 用户选择学术期刊类，执行ArXiv搜索...\n",
      "2025-03-16 18:19:28,631 - INFO - 执行ArXiv搜索，限制结果数量为7...\n",
      "2025-03-16 18:19:31,543 - INFO - 生成最终的搜索报告...\n",
      "2025-03-16 18:19:31,544 - INFO - 调用大语言模型进行整合搜索结果...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索完成，共找到 14 条相关信息\n",
      "\n",
      "步骤 5/6: 生成报告\n",
      "正在整合搜索结果并生成报告...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 18:19:31,777 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-16 18:21:42,657 - INFO - 准备发送邮件...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "报告生成完成\n",
      "\n",
      "步骤 6/6: 发送报告\n",
      "已发送邮件到 1145141@qq.com:\n",
      "\n",
      "\n",
      "--- 根据您选择的【学术期刊】平台，近几日的行业内最新进展已整理好，请查收！ ---\n",
      "\n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：PLMM: Personal Large Language Models on Mobile Devices  \n",
      "摘要：受联邦学习的启发，本文提出了个人大模型，这些模型从传统的大语言模型中蒸馏出来，但更适应用户的个人信息，如教育背景和兴趣爱好。我们将大语言模型分为三个层次：个人层次、专家层次和传统层次。个人层次模型适应用户的个人信息，加密用户输入并保护隐私。专家层次模型专注于融合特定领域的知识，如金融、IT和艺术。传统模型则专注于通用知识的发现和专家模型的升级。在这种分类中，个人模型直接与用户交互，拥有用户的（加密）个人信息。此外，这些模型必须足够小，以便在个人计算机或移动设备上运行，并且需要实时响应以提供更好的用户体验和高质量的结果。所提出的个人大模型可以广泛应用于语言和视觉任务。  \n",
      "原文网址：http://arxiv.org/abs/2309.14726v2  \n",
      "BERT嵌入的余弦相似度: 0.8269748687744141  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Beyond Segmentation: Road Network Generation with Multi-Modal LLMs  \n",
      "摘要：本文介绍了一种通过多模态大语言模型（LLM）生成道路网络的创新方法。我们的模型专门设计用于处理道路布局的航拍图像，并在输入图像中生成详细的、可导航的道路网络。该系统的核心创新在于用于训练大语言模型的独特方法，以生成道路网络作为输出。该方法借鉴了BLIP-2架构，利用预训练的冻结图像编码器和大语言模型创建了一个多功能的多模态LLM。我们的工作还提供了对LISA论文中提出的推理分割方法的替代方案。通过我们的方法训练大语言模型，有效地消除了生成二进制分割掩码的必要性。实验结果强调了我们的多模态LLM在提供精确和有价值的导航指导方面的有效性。这项研究代表了在增强自主导航系统方面的重要进展，特别是在道路网络场景中，准确的导航至关重要。  \n",
      "原文网址：http://arxiv.org/abs/2310.09755v1  \n",
      "BERT嵌入的余弦相似度: 0.7902063727378845  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data  \n",
      "摘要：Audio Word2Vec通过序列到序列自编码器（SA）为可变长度的音频片段提供固定维度的向量表示。这些向量表示被证明能够很好地描述音频片段的顺序语音结构，并在实际应用中如基于示例的语音术语检测（STD）中发挥作用。本文探讨了Audio Word2Vec的语言迁移能力。我们从一个语言（源语言）训练SA，并使用它提取另一个语言（目标语言）的音频片段的向量表示。我们发现，如果源语言和目标语言相似，SA仍然可以从目标语言的音频片段中捕捉到语音结构。在基于示例的STD中，我们从大量源语言数据中学习的SA中获取向量表示，并发现它们优于从少量目标语言数据中直接学习的朴素编码器和SA的表示。结果表明，可以从高资源语言中学习Audio Word2Vec模型，并将其应用于低资源语言。这进一步扩展了Audio Word2Vec的可用性。  \n",
      "原文网址：http://arxiv.org/abs/1707.06519v1  \n",
      "BERT嵌入的余弦相似度: 0.7424719929695129  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Mathematical Language Models: A Survey  \n",
      "摘要：近年来，在数学领域利用语言模型（LMs），包括预训练语言模型（PLMs）和大规模语言模型（LLMs），取得了显著进展。本文对数学LMs进行了全面调查，从任务和方法两个角度系统分类了关键研究工作。研究领域揭示了大量提出的数学LLMs，这些模型进一步细分为指令学习、基于工具的方法、基础CoT技术、高级CoT方法和多模态方法。为了更深入地理解数学LMs的优势，我们对其特征和性能进行了深入对比。此外，我们的调查还汇编了60多个数学数据集，包括训练数据集、基准数据集和增强数据集。针对数学LMs领域的主要挑战和未来发展方向，本次调查旨在促进和启发未来研究者的创新。  \n",
      "原文网址：http://arxiv.org/abs/2312.07622v4  \n",
      "BERT嵌入的余弦相似度: 0.7338587045669556  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning  \n",
      "摘要：大语言模型（LLMs）可以通过构建自然语言查询并直接将提示输入大语言模型来用作可访问且智能的聊天机器人。然而，不同的提示构建通常会导致答案的不确定性，从而难以利用LLMs（如ChatGPT）的特定知识。为了缓解这一问题，我们使用可解释的结构来解释LLMs中的提示学习原理，该结构证明了语言模型的有效性取决于任务相关标记的位置变化。因此，我们提出了MTPrompt，一种基于任务相关对象、摘要和任务描述信息的多维任务提示学习方法。通过自动构建和搜索适当的提示，我们的MTPrompt在少样本设置和五个不同数据集上取得了最佳结果。此外，我们在不同的实验设置和消融实验中证明了该方法的有效性和稳定性。在与大语言模型的交互中，将更多任务相关信息嵌入提示中更容易激发大语言模型中的知识。  \n",
      "原文网址：http://arxiv.org/abs/2312.08027v1  \n",
      "BERT嵌入的余弦相似度: 0.7237551212310791  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner's Students  \n",
      "摘要：我们描述了两个正在开发的系统，它们使用大语言模型来自动化纠正（i）在自然语言与命题逻辑和一阶谓词逻辑之间来回翻译的练习，以及（ii）在非数学场景中用自然语言撰写简单论点的练习。  \n",
      "原文网址：http://arxiv.org/abs/2304.06186v3  \n",
      "BERT嵌入的余弦相似度: 0.6390053033828735  \n",
      "\n",
      "来源：Google_arXiv  \n",
      "标题：Symbolic and Language Agnostic Large Language Models  \n",
      "摘要：我们认为大语言模型（LLMs）的相对成功并不是符号与子符号辩论的反映，而是反映了在规模上采用适当的自下而上逆向工程语言的策略。然而，由于这些模型的子符号性质，这些系统获得的关于语言的知识将始终被埋在数百万个微特征（权重）中，没有一个是有意义的。此外，由于其随机性，这些模型通常无法捕捉自然语言中普遍存在的各种推理方面。我们建议在符号环境中采用成功的自下而上策略，生成符号化、语言无关且基于本体的大语言模型。  \n",
      "原文网址：http://arxiv.org/abs/2308.14199v1  \n",
      "BERT嵌入的余弦相似度: 0.6138080358505249  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：A Review on Proprietary Accelerators for Large Language Models  \n",
      "摘要：随着大语言模型（LLMs）的进步，高效处理LLM计算的加速器的重要性日益增加。本文讨论了LLM加速器的必要性，并对主要商业LLM加速器的硬件和软件特性进行了全面分析。基于此分析，我们提出了下一代LLM加速器开发的考虑因素，并建议了未来的研究方向。  \n",
      "原文网址：http://arxiv.org/abs/2503.09650v1  \n",
      "BERT嵌入的余弦相似度: 0.7848071455955505  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Numerical Error Analysis of Large Language Models  \n",
      "摘要：基于Transformer架构的大语言模型已成为最先进的自然语言处理应用的核心。然而，它们的训练仍然计算昂贵且表现出不稳定性，其中一些预计是由有限精度计算引起的。我们对Transformer架构前向传播中的舍入误差影响进行了理论分析，得出了这些影响的基本界限。此外，我们进行了一系列数值实验，证明了我们界限的实际相关性。我们的结果为选择减轻舍入误差的超参数提供了具体指导，从而实现了更稳健和稳定的推理。  \n",
      "原文网址：http://arxiv.org/abs/2503.10251v1  \n",
      "BERT嵌入的余弦相似度: 0.7195348739624023  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Control Flow-Augmented Decompiler based on Large Language Model  \n",
      "摘要：二进制反编译在安全威胁分析和软件工程的各种任务中起着至关重要的作用，如二进制漏洞检测和软件供应链分析。当前主流的二进制反编译方法主要依赖于大语言模型（LLMs），并大致分为两种主要方法：基于提示的反编译和端到端反编译。基于提示的方法通常需要大量努力来分析和总结预测数据，以提取特定方面的专家知识，然后将其输入通用大语言模型以解决特定的反编译任务。端到端方法则精心构建训练数据集或神经网络，对通用大语言模型进行后训练，从而获得用于反编译预测数据的领域特定大语言模型。然而，现有方法仍面临重大挑战，包括缺乏输入代码的丰富语义表示以及忽略控制流信息，这对准确反编译至关重要。此外，大多数当前的反编译技术专门针对x86架构，难以高效适应和推广到其他位宽或指令架构。为了解决这些限制，我们提出了一种新的方法。  \n",
      "原文网址：http://arxiv.org/abs/2503.07215v1  \n",
      "BERT嵌入的余弦相似度: 0.6816016435623169  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Large Language Model Guided Progressive Feature Alignment for Multimodal UAV Object Detection  \n",
      "摘要：现有的多模态无人机目标检测方法通常忽略了模态之间语义差距的影响，这使得难以实现准确的语义和空间对齐，限制了检测性能。为了解决这个问题，我们提出了一种大语言模型（LLM）引导的渐进特征对齐网络，称为LPANet，它利用从大语言模型中提取的语义特征来指导模态之间的渐进语义和空间对齐，以实现多模态无人机目标检测。为了利用LLM的强大语义表示，我们通过ChatGPT生成每个对象类别的细粒度文本描述，然后使用大语言模型MPNet提取语义特征。基于这些语义特征，我们以渐进的方式指导语义和空间对齐。首先，我们设计了语义对齐模块（SAM），将每个对象的语义特征和多模态视觉特征拉近，缓解模态之间的语义差异。其次，我们设计了显式空间对齐模块（ESM），通过将语义关系整合到特征级偏移的估计中，缓解模态之间的粗糙空间不对齐。最后，我们设计了隐式空间对齐模块（ISM），利用跨模态相关性来聚合。  \n",
      "原文网址：http://arxiv.org/abs/2503.06948v1  \n",
      "BERT嵌入的余弦相似度: 0.6727063059806824  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Position-Aware Depth Decay Decoding ($D^3$): Boosting Large Language Model Inference Efficiency  \n",
      "摘要：由于参数数量庞大，大语言模型（LLMs）的推理阶段资源密集。与传统模型压缩不同，后者需要重新训练，最近的动态计算方法表明并非所有组件都需要用于推理，从而实现了无需训练的流程。在本文中，我们专注于LLM生成的动态深度。提出了一个令牌位置感知的层跳过框架，以高效地节省1.5倍的操作，同时保持性能。我们首先观察到，预测较晚的令牌具有较低的困惑度，因此需要较少的计算。然后，我们提出了一种无需训练的算法，称为位置感知深度衰减解码（$D^3$），它利用幂律衰减函数$\\\\left\\\\lfloor L \\\\times (\\\\alpha^i) \\\\right\\\\rfloor$来确定生成令牌$T_i$时保留的层数。值得注意的是，无需任何重新训练，$D^3$首次在广泛的生成任务中取得了成功。在具有$7 \\\\sim 70$亿参数的大语言模型（如Llama）上的实验表明，$D^3$可以在保持与全推理管道相当性能的同时，平均实现1.5倍的加速，在GSM8K和BBH基准测试中几乎没有性能下降（$<1\\\\%$）。  \n",
      "原文网址：http://arxiv.org/abs/2503.08524v1  \n",
      "BERT嵌入的余弦相似度: 0.6501635313034058  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Crowdsourced Homophily Ties Based Graph Annotation Via Large Language Model  \n",
      "摘要：准确的图标注通常需要大量标记数据，这通常具有挑战性且资源密集。在本文中，我们提出了基于大语言模型的众包同质性图标注（CSA-LLM），这是一种将众包注释与大语言模型（LLMs）能力相结合的新方法，以增强图标注过程。CSA-LLM通过整合1跳和2跳邻居的信息来利用图数据的结构上下文。通过强调同质性联系——图中表示相似性的关键连接——CSA-LLM显著提高了标注的准确性。实验结果表明，该方法通过提供更精确和可靠的标注，增强了图神经网络（GNNs）的性能。  \n",
      "原文网址：http://arxiv.org/abs/2503.09281v1  \n",
      "BERT嵌入的余弦相似度: 0.6175556182861328  \n",
      "\n",
      "来源：arXiv  \n",
      "标题：Test Amplification for REST APIs Using \\\"Out-of-the-box\\\" Large Language Models  \n",
      "摘要：REST API是当今云原生应用中不可或缺的构建块，因此测试它们至关重要。然而，为这些REST API编写自动化测试具有挑战性，因为需要强大且可读的测试来覆盖嵌入在REST API中的协议的边界值。在本文中，我们报告了使用“开箱即用”的大语言模型（ChatGPT和GitHub的Copilot）来扩增REST API测试套件的经验。我们基于覆盖率和可理解性比较了生成的测试，并得出了一系列关于生成最强测试套件的提示的指南和经验教训。  \n",
      "原文网址：http://arxiv.org/abs/2503.10306v1  \n",
      "BERT嵌入的余弦相似度: 0.5805650353431702\n",
      "流程执行完成\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from db_utils import get_db_connection, initialize_database\n",
    "import datetime\n",
    "import logging\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from xml.etree import ElementTree\n",
    "from deep_translator import GoogleTranslator, BaiduTranslator\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# 配置日志\n",
    "# logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 常量配置\n",
    "CONFIG = {\n",
    "    \"API_KEYS\": {\n",
    "        \"deepseek\": os.environ.get(\"DEEPSEEK_API_KEY\", \"sk-58c6314fd9ae47a6a493d0d8499d2271\"),\n",
    "        \"serper\": os.environ.get(\"SERPER_API_KEY\", \"a196a1abc535244a7430523550c88adb422d893e\"),\n",
    "        \"baidu_translate\": os.environ.get(\"BAIDU_API_KEY\", \"\"),\n",
    "    },\n",
    "    \"MODELS\": {\n",
    "        \"BERT\": \"bert-base-multilingual-cased\",\n",
    "        \"LLM\": \"deepseek-chat\",\n",
    "    },\n",
    "    \"DATA_DIR\": \"./user_data\",\n",
    "    \"DB_PATH\": \"./user_data/user_profiles.db\"\n",
    "}\n",
    "\n",
    "# 确保数据目录存在\n",
    "os.makedirs(CONFIG[\"DATA_DIR\"], exist_ok=True)\n",
    "\n",
    "# 初始化BERT模型\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG[\"MODELS\"][\"BERT\"])\n",
    "model = BertModel.from_pretrained(CONFIG[\"MODELS\"][\"BERT\"])\n",
    "\n",
    "# 创建数据库连接\n",
    "def verify_database():\n",
    "    \"\"\"验证数据库是否正确创建和可写入\"\"\"\n",
    "    print(\"\\n验证数据库...\")\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if conn is None:\n",
    "        print(\"无法连接到数据库，请检查路径和权限\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 尝试写入测试数据\n",
    "        test_id = f\"test_{int(time.time())}\"\n",
    "        conn.execute(\n",
    "            \"INSERT INTO users (id, name, occupation, email) VALUES (?, ?, ?, ?)\",\n",
    "            (test_id, \"测试用户\", \"测试职业\", \"test@example.com\")\n",
    "        )\n",
    "        conn.commit()\n",
    "        \n",
    "        # 验证是否写入成功\n",
    "        user = conn.execute(\"SELECT * FROM users WHERE id = ?\", (test_id,)).fetchone()\n",
    "        if user:\n",
    "            print(\"数据库验证成功：可以正常写入和读取数据\")\n",
    "            \n",
    "            # 清理测试数据\n",
    "            conn.execute(\"DELETE FROM users WHERE id = ?\", (test_id,))\n",
    "            conn.commit()\n",
    "            \n",
    "            # 显示数据库信息\n",
    "            tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n",
    "            print(f\"数据库包含以下表: {', '.join([t['name'] for t in tables])}\")\n",
    "            \n",
    "            for table in [t['name'] for t in tables]:\n",
    "                count = conn.execute(f\"SELECT COUNT(*) as count FROM {table}\").fetchone()['count']\n",
    "                print(f\"表 {table}: {count} 条记录\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"数据库验证失败：无法读取写入的测试数据\")\n",
    "            return False\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"数据库验证失败: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 创建一个自定义的 NumpyEncoder 类来处理 NumPy 数据类型\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        import numpy as np\n",
    "        if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def get_bert_embeddings(text: str) -> torch.Tensor:\n",
    "    \"\"\" 获取文本的BERT嵌入表示 \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "def compute_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\" 计算两个文本之间的余弦相似度 \"\"\"\n",
    "    embedding1 = get_bert_embeddings(text1)\n",
    "    embedding2 = get_bert_embeddings(text2)\n",
    "    similarity = cosine_similarity(embedding1.numpy(), embedding2.numpy())\n",
    "    return similarity[0][0]\n",
    "\n",
    "class UserProfileManager:\n",
    "    \"\"\"用户画像管理器，负责创建、更新和存储用户画像\"\"\"\n",
    "\n",
    "    def __init__(self, client=None):\n",
    "        \"\"\"初始化用户画像管理器\"\"\"\n",
    "        self.client = client or OpenAI(\n",
    "            api_key=CONFIG[\"API_KEYS\"][\"deepseek\"],\n",
    "            base_url=\"https://api.deepseek.com\"\n",
    "        )\n",
    "        self.interest_categories = self._load_interest_categories()\n",
    "        logging.info(\"用户画像管理器初始化完成\")\n",
    "\n",
    "    def _load_interest_categories(self):\n",
    "        \"\"\"加载预定义的兴趣分类体系\"\"\"\n",
    "        categories_file = os.path.join(CONFIG[\"DATA_DIR\"], \"interest_categories.json\")\n",
    "\n",
    "        if os.path.exists(categories_file):\n",
    "            try:\n",
    "                with open(categories_file, 'r', encoding='utf-8') as f:\n",
    "                    self.interest_categories = json.load(f)\n",
    "                logging.info(f\"已加载兴趣分类体系，共 {len(self.interest_categories)} 个类别\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"加载兴趣分类体系出错: {e}\")\n",
    "                self._create_default_categories()\n",
    "        else:\n",
    "            logging.warning(f\"兴趣分类文件不存在: {categories_file}，将创建默认分类\")\n",
    "            self._create_default_categories()\n",
    "\n",
    "    def _create_default_categories(self):\n",
    "        \"\"\"创建默认的兴趣分类体系\"\"\"\n",
    "        self.interest_categories = {\n",
    "            \"技术\": [\"人工智能\", \"机器学习\", \"深度学习\", \"自然语言处理\", \"计算机视觉\", \"大语言模型\",\n",
    "                   \"大数据\", \"云计算\", \"区块链\", \"物联网\", \"网络安全\", \"数据库\"],\n",
    "            \"科学\": [\"物理学\", \"化学\", \"生物学\", \"天文学\", \"数学\", \"医学\", \"地质学\", \"环境科学\"],\n",
    "            \"商业\": [\"管理\", \"市场营销\", \"金融\", \"创业\", \"投资\", \"电子商务\", \"人力资源\"],\n",
    "            \"艺术\": [\"绘画\", \"音乐\", \"电影\", \"文学\", \"设计\", \"摄影\", \"建筑\"],\n",
    "            \"教育\": [\"教学方法\", \"学习理论\", \"教育技术\", \"高等教育\", \"职业教育\"],\n",
    "            \"健康\": [\"营养\", \"健身\", \"心理健康\", \"医疗技术\", \"公共卫生\"]\n",
    "        }\n",
    "\n",
    "        # 保存到文件\n",
    "        categories_file = os.path.join(CONFIG[\"DATA_DIR\"], \"interest_categories.json\")\n",
    "        try:\n",
    "            with open(categories_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.interest_categories, f, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"已创建默认兴趣分类体系并保存到: {categories_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存默认兴趣分类体系出错: {e}\")\n",
    "\n",
    "\n",
    "    def _generate_user_id(self, user_info: Dict) -> str:\n",
    "        \"\"\"根据用户信息生成唯一ID\"\"\"\n",
    "        key_string = f\"{user_info.get('name', '')}-{user_info.get('email', '')}-{user_info.get('occupation', '')}\"\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "\n",
    "    def create_user(self, user_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        创建新用户并存储基本信息\n",
    "\n",
    "        Args:\n",
    "            user_info: 用户基本信息，包含姓名、职业、邮箱等\n",
    "\n",
    "        Returns:\n",
    "            用户ID\n",
    "        \"\"\"\n",
    "        user_id = self._generate_user_id(user_info)\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 检查用户是否已存在\n",
    "            existing_user = conn.execute(\"SELECT id FROM users WHERE id = ?\", (user_id,)).fetchone()\n",
    "\n",
    "            if not existing_user:\n",
    "                conn.execute(\n",
    "                    \"INSERT INTO users (id, name, occupation, email) VALUES (?, ?, ?, ?)\",\n",
    "                    (user_id, user_info.get(\"name\", \"\"), user_info.get(\"occupation\", \"\"), user_info.get(\"email\", \"\"))\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"创建新用户: {user_id}\")\n",
    "            else:\n",
    "                logging.info(f\"用户已存在: {user_id}\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return user_id\n",
    "\n",
    "    def extract_skills_from_resume(self, user_id: str, resume_text: str, max_skills: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从简历文本中提取技能信息\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            resume_text: 简历文本内容\n",
    "            max_skills: 最多提取的技能数量\n",
    "\n",
    "        Returns:\n",
    "            技能列表，每个技能包含名称、级别和分类\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始从简历中提取最重要的{max_skills}项技能...\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请从以下简历文本中提取最重要的{max_skills}项技能，并为每个技能提供以下信息：\n",
    "        1. 技能名称\n",
    "        2. 熟练程度（初级/中级/高级/专家）\n",
    "        3. 技能类别（技术技能、软技能、语言技能、管理技能等）\n",
    "        \n",
    "        请按照技能的重要性和熟练程度排序，最重要和最熟练的技能排在前面。\n",
    "        \n",
    "        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\n",
    "        [\n",
    "            {{\"skill\": \"技能名称\", \"level\": \"熟练程度\", \"category\": \"技能类别\"}},\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        简历文本：\n",
    "        {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"正在分析简历中的技能...\")\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的简历分析助手，擅长提取简历中的技能信息并进行分类和评估。请只返回JSON格式的结果，不要添加任何其他标记。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            skills_text = response.choices[0].message.content\n",
    "            \n",
    "            # 清理可能的格式标记\n",
    "            skills_text = skills_text.strip()\n",
    "            if skills_text.startswith(\"```json\"):\n",
    "                skills_text = skills_text[7:]\n",
    "            if skills_text.startswith(\"```\"):\n",
    "                skills_text = skills_text[3:]\n",
    "            if skills_text.endswith(\"```\"):\n",
    "                skills_text = skills_text[:-3]\n",
    "            skills_text = skills_text.strip()\n",
    "            \n",
    "            logging.debug(f\"清理后的技能JSON文本: {skills_text}\")\n",
    "    \n",
    "            # 尝试解析JSON\n",
    "            try:\n",
    "                skills = json.loads(skills_text)\n",
    "                print(f\"成功提取 {len(skills)} 项技能\")\n",
    "                \n",
    "                # 确保skills是列表类型\n",
    "                if not isinstance(skills, list):\n",
    "                    logging.error(f\"解析的技能不是列表类型: {type(skills)}\")\n",
    "                    skills = []\n",
    "                    print(\"解析的技能格式不正确，将使用空列表\")\n",
    "                \n",
    "                # 保存到数据库\n",
    "                if skills:  # 只有当skills非空时才尝试保存\n",
    "                    conn = get_db_connection()\n",
    "                    try:\n",
    "                        for i, skill in enumerate(skills):\n",
    "                            if isinstance(skill, dict):  # 确保每个技能是字典类型\n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_skills (user_id, skill, level, category) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, skill.get(\"skill\", \"\"), skill.get(\"level\", \"\"), skill.get(\"category\", \"\"))\n",
    "                                )\n",
    "                                # 显示进度\n",
    "                                print(f\"保存技能 {i+1}/{len(skills)}: {skill.get('skill', '')}\")\n",
    "                        conn.commit()\n",
    "                        print(\"所有技能已保存到数据库\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"保存技能到数据库时出错: {e}\")\n",
    "                        print(f\"保存技能时出错: {e}\")\n",
    "                    finally:\n",
    "                        conn.close()\n",
    "                \n",
    "                return skills if skills else []\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"无法解析技能JSON: {skills_text}, 错误: {e}\")\n",
    "                print(f\"解析技能信息失败: {e}\")\n",
    "                # 尝试手动解析简单格式\n",
    "                if \"[\" in skills_text and \"]\" in skills_text:\n",
    "                    try:\n",
    "                        # 尝试修复常见的JSON格式问题\n",
    "                        fixed_text = skills_text.replace(\"'\", \"\\\"\")\n",
    "                        skills = json.loads(fixed_text)\n",
    "                        print(f\"修复后成功解析，提取了 {len(skills)} 项技能\")\n",
    "                        return skills\n",
    "                    except:\n",
    "                        pass\n",
    "                return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取技能时出错: {e}\")\n",
    "            print(f\"提取技能时出错: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_interests_from_resume(self, user_id: str, resume_text: str, max_interests: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从简历中提取用户兴趣并分类\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            resume_text: 简历文本\n",
    "            max_interests: 最多提取的兴趣数量\n",
    "\n",
    "        Returns:\n",
    "            兴趣列表，每个兴趣包含主题、分类和初始权重\n",
    "        \"\"\"\n",
    "        print(f\"\\n开始从简历中提取最重要的{max_interests}项兴趣...\")\n",
    "    \n",
    "        # 构建兴趣分类提示\n",
    "        categories_text = \"\\n\".join([f\"{cat}: {', '.join(topics)}\" for cat, topics in self.interest_categories.items()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请分析以下简历文本，提取用户最重要的{max_interests}项兴趣领域和专业方向。\n",
    "        将提取的兴趣根据以下分类系统进行归类：\n",
    "        \n",
    "        {categories_text}\n",
    "        \n",
    "        如果发现的兴趣不在上述分类中，请归入最相近的类别。\n",
    "        对于每个识别的兴趣，根据在简历中的明显程度，给出一个0到1之间的权重。\n",
    "        请按照兴趣的重要性排序，最重要的兴趣排在前面。\n",
    "        \n",
    "        请严格按照以下JSON格式返回，不要添加任何其他格式标记如```json或```：\n",
    "        [\n",
    "            {{\"topic\": \"兴趣主题\", \"category\": \"所属类别\", \"weight\": 权重值}},\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "        简历文本：\n",
    "        {resume_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"正在分析简历中的兴趣...\")\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长从文本中提取人们的兴趣爱好并进行分类。请只返回JSON格式的结果，不要添加任何其他标记。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            interests_text = response.choices[0].message.content\n",
    "\n",
    "            # 记录原始响应以便调试\n",
    "            logging.debug(f\"原始兴趣响应: {interests_text}\")\n",
    "            \n",
    "            # 清理可能的格式标记\n",
    "            interests_text = interests_text.strip()\n",
    "            if interests_text.startswith(\"```json\"):\n",
    "                interests_text = interests_text[7:]\n",
    "            if interests_text.startswith(\"```\"):\n",
    "                interests_text = interests_text[3:]\n",
    "            if interests_text.endswith(\"```\"):\n",
    "                interests_text = interests_text[:-3]\n",
    "            interests_text = interests_text.strip()\n",
    "            \n",
    "            logging.debug(f\"清理后的兴趣JSON文本: {interests_text}\")\n",
    "        \n",
    "            # 尝试解析JSON\n",
    "            try:\n",
    "                interests = json.loads(interests_text)\n",
    "                print(f\"成功提取 {len(interests)} 项兴趣\")\n",
    "                \n",
    "                # 确保interests是列表类型\n",
    "                if not isinstance(interests, list):\n",
    "                    logging.error(f\"解析的兴趣不是列表类型: {type(interests)}\")\n",
    "                    interests = []\n",
    "                    print(\"解析的兴趣格式不正确，将使用空列表\")\n",
    "                \n",
    "                # 保存到数据库\n",
    "                if interests:  # 只有当interests非空时才尝试保存\n",
    "                    conn = get_db_connection()\n",
    "                    try:\n",
    "                        for i, interest in enumerate(interests):\n",
    "                            if isinstance(interest, dict):  # 确保每个兴趣是字典类型\n",
    "                                # 确保所有必要的键都存在\n",
    "                                topic = interest.get(\"topic\", \"未知兴趣\")\n",
    "                                category = interest.get(\"category\", \"未分类\")\n",
    "                                weight = interest.get(\"weight\", 0.5)\n",
    "                                \n",
    "                                # 确保weight是浮点数\n",
    "                                try:\n",
    "                                    weight = float(weight)\n",
    "                                except (ValueError, TypeError):\n",
    "                                    weight = 0.5\n",
    "                                \n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, topic, category, weight)\n",
    "                                )\n",
    "                                # 显示进度\n",
    "                                print(f\"保存兴趣 {i+1}/{len(interests)}: {topic} (权重: {weight:.2f})\")\n",
    "                        conn.commit()\n",
    "                        print(\"所有兴趣已保存到数据库\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"保存兴趣到数据库时出错: {e}\")\n",
    "                        print(f\"保存兴趣时出错: {e}\")\n",
    "                        conn.rollback()  # 回滚事务\n",
    "                    finally:\n",
    "                        conn.close()\n",
    "                \n",
    "                return interests if interests else []\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"无法解析兴趣JSON: {interests_text}, 错误: {e}\")\n",
    "                print(f\"解析兴趣信息失败: {e}\")\n",
    "                \n",
    "                # 尝试手动解析简单格式\n",
    "                if \"[\" in interests_text and \"]\" in interests_text:\n",
    "                    try:\n",
    "                        # 尝试修复常见的JSON格式问题\n",
    "                        fixed_text = interests_text.replace(\"'\", \"\\\"\").replace(\"None\", \"null\")\n",
    "                        interests = json.loads(fixed_text)\n",
    "                        print(f\"修复后成功解析，提取了 {len(interests)} 项兴趣\")\n",
    "                        return interests\n",
    "                    except Exception as parse_e:\n",
    "                        logging.error(f\"尝试修复JSON后仍然失败: {parse_e}\")\n",
    "                \n",
    "                # 如果无法解析，创建一些基本兴趣\n",
    "                print(\"无法解析兴趣，将创建基本兴趣\")\n",
    "                basic_interests = []\n",
    "                \n",
    "                # 从简历文本中提取一些关键词作为基本兴趣\n",
    "                keywords = [\"人工智能\", \"机器学习\", \"数据分析\", \"编程\", \"算法\"]\n",
    "                for i, keyword in enumerate(keywords):\n",
    "                    if keyword.lower() in resume_text.lower():\n",
    "                        interest = {\n",
    "                            \"topic\": keyword,\n",
    "                            \"category\": \"技术\",\n",
    "                            \"weight\": 0.7\n",
    "                        }\n",
    "                        basic_interests.append(interest)\n",
    "                        \n",
    "                        # 保存到数据库\n",
    "                        try:\n",
    "                            conn = get_db_connection()\n",
    "                            if conn:\n",
    "                                conn.execute(\n",
    "                                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                                    (user_id, keyword, \"技术\", 0.7)\n",
    "                                )\n",
    "                                conn.commit()\n",
    "                                print(f\"保存基本兴趣: {keyword}\")\n",
    "                                conn.close()\n",
    "                        except Exception as db_e:\n",
    "                            logging.error(f\"保存基本兴趣到数据库时出错: {db_e}\")\n",
    "                \n",
    "                return basic_interests\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取兴趣时出错: {e}\")\n",
    "            print(f\"提取兴趣时出错: {e}\")\n",
    "            return []\n",
    "\n",
    "    def record_search(self, user_id: str, query: str, platform: str):\n",
    "        \"\"\"\n",
    "        记录用户搜索行为\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            query: 搜索查询\n",
    "            platform: 搜索平台\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO user_searches (user_id, query, platform) VALUES (?, ?, ?)\",\n",
    "                (user_id, query, platform)\n",
    "            )\n",
    "            conn.commit()\n",
    "            logging.info(f\"记录用户搜索: {user_id}, 查询: {query}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def record_interaction(self, user_id: str, content_id: str, action_type: str):\n",
    "        \"\"\"\n",
    "        记录用户与内容的交互\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            content_id: 内容ID（如文章URL）\n",
    "            action_type: 交互类型（如\"点击\"、\"收藏\"、\"分享\"）\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO user_interactions (user_id, content_id, action_type) VALUES (?, ?, ?)\",\n",
    "                (user_id, content_id, action_type)\n",
    "            )\n",
    "            conn.commit()\n",
    "            logging.info(f\"记录用户交互: {user_id}, 内容: {content_id}, 行为: {action_type}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def update_interest_weights(self, user_id: str, topic: str, adjustment: float):\n",
    "        \"\"\"\n",
    "        更新用户兴趣权重\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            topic: 兴趣主题\n",
    "            adjustment: 权重调整值，正数表示增加，负数表示减少\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 查找现有兴趣\n",
    "            interest = conn.execute(\n",
    "                \"SELECT id, weight FROM user_interests WHERE user_id = ? AND topic = ? ORDER BY timestamp DESC LIMIT 1\",\n",
    "                (user_id, topic)\n",
    "            ).fetchone()\n",
    "\n",
    "            if interest:\n",
    "                # 更新权重，确保在0-1范围内\n",
    "                new_weight = max(0, min(1, interest[\"weight\"] + adjustment))\n",
    "\n",
    "                conn.execute(\n",
    "                    \"UPDATE user_interests SET weight = ?, timestamp = CURRENT_TIMESTAMP WHERE id = ?\",\n",
    "                    (new_weight, interest[\"id\"])\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"更新用户兴趣权重: {user_id}, 主题: {topic}, 新权重: {new_weight}\")\n",
    "            else:\n",
    "                # 如果不存在，创建新的兴趣项\n",
    "                weight = max(0, min(1, 0.5 + adjustment))  # 默认权重0.5加上调整值\n",
    "\n",
    "                # 尝试确定类别\n",
    "                category = \"未分类\"\n",
    "                for cat, topics in self.interest_categories.items():\n",
    "                    if any(compute_similarity(topic, t) > 0.7 for t in topics):\n",
    "                        category = cat\n",
    "                        break\n",
    "\n",
    "                conn.execute(\n",
    "                    \"INSERT INTO user_interests (user_id, topic, category, weight) VALUES (?, ?, ?, ?)\",\n",
    "                    (user_id, topic, category, weight)\n",
    "                )\n",
    "                conn.commit()\n",
    "                logging.info(f\"创建新用户兴趣: {user_id}, 主题: {topic}, 权重: {weight}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def apply_time_decay(self, user_id: str, decay_factor: float = 0.9, days_threshold: int = 30):\n",
    "        \"\"\"\n",
    "        应用时间衰减模型，降低旧兴趣的权重\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            decay_factor: 衰减因子(0-1)\n",
    "            days_threshold: 多少天前的兴趣开始衰减\n",
    "        \"\"\"\n",
    "        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days_threshold)\n",
    "        threshold_str = threshold_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            old_interests = conn.execute(\n",
    "                \"SELECT id, topic, category, weight, timestamp FROM user_interests WHERE user_id = ? AND timestamp < ?\",\n",
    "                (user_id, threshold_str)\n",
    "            ).fetchall()\n",
    "\n",
    "            for interest in old_interests:\n",
    "                # 计算时间差（天数）\n",
    "                interest_date = datetime.datetime.strptime(interest[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "                days_diff = (datetime.datetime.now() - interest_date).days\n",
    "\n",
    "                # 计算衰减倍数（随时间增加而增加衰减）\n",
    "                decay_multiplier = days_diff // days_threshold\n",
    "\n",
    "                # 计算新权重\n",
    "                new_weight = interest[\"weight\"] * (decay_factor ** decay_multiplier)\n",
    "\n",
    "                # 更新权重\n",
    "                conn.execute(\n",
    "                    \"UPDATE user_interests SET weight = ? WHERE id = ?\",\n",
    "                    (new_weight, interest[\"id\"])\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            logging.info(f\"应用时间衰减模型: {user_id}, 处理 {len(old_interests)} 条旧兴趣\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_top_interests(self, user_id: str, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        获取用户的顶级兴趣\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            limit: 返回的兴趣数量\n",
    "\n",
    "        Returns:\n",
    "            兴趣列表，按权重排序\n",
    "        \"\"\"\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 对于每个主题，只取最新的一条记录\n",
    "            interests = conn.execute(\"\"\"\n",
    "                SELECT i1.topic, i1.category, i1.weight, i1.timestamp\n",
    "                FROM user_interests i1\n",
    "                INNER JOIN (\n",
    "                    SELECT topic, MAX(timestamp) as max_time\n",
    "                    FROM user_interests\n",
    "                    WHERE user_id = ?\n",
    "                    GROUP BY topic\n",
    "                ) i2 ON i1.topic = i2.topic AND i1.timestamp = i2.max_time\n",
    "                WHERE user_id = ?\n",
    "                ORDER BY i1.weight DESC\n",
    "                LIMIT ?\n",
    "            \"\"\", (user_id, user_id, limit)).fetchall()\n",
    "\n",
    "            return [dict(i) for i in interests]\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def analyze_search_patterns(self, user_id: str, days: int = 30) -> Dict:\n",
    "        \"\"\"\n",
    "        分析用户搜索模式\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            days: 分析的天数范围\n",
    "\n",
    "        Returns:\n",
    "            分析结果，包含常用平台、热门查询等\n",
    "        \"\"\"\n",
    "        threshold_date = datetime.datetime.now() - datetime.timedelta(days=days)\n",
    "        threshold_str = threshold_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        conn = get_db_connection()\n",
    "        try:\n",
    "            # 获取搜索记录\n",
    "            searches = conn.execute(\n",
    "                \"SELECT query, platform, timestamp FROM user_searches WHERE user_id = ? AND timestamp > ?\",\n",
    "                (user_id, threshold_str)\n",
    "            ).fetchall()\n",
    "\n",
    "            if not searches:\n",
    "                return {\"status\": \"无搜索记录\"}\n",
    "\n",
    "            # 统计平台使用情况\n",
    "            platforms = {}\n",
    "            for search in searches:\n",
    "                platform = search[\"platform\"]\n",
    "                platforms[platform] = platforms.get(platform, 0) + 1\n",
    "\n",
    "            # 提取查询内容用于语义分析\n",
    "            queries = [search[\"query\"] for search in searches]\n",
    "\n",
    "            # 使用LLM分析查询主题\n",
    "            prompt = f\"\"\"\n",
    "            请分析以下搜索查询列表，识别主要的搜索主题和模式。\n",
    "            将分析结果以JSON格式返回，包含以下字段：\n",
    "            1. dominant_topics: 主导主题列表，按重要性排序\n",
    "            2. search_patterns: 搜索模式描述\n",
    "\n",
    "            搜索查询列表：\n",
    "            {queries}\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的搜索行为分析助手。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                analysis_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    analysis = json.loads(analysis_text)\n",
    "                except json.JSONDecodeError:\n",
    "                    analysis = {\"dominant_topics\": [], \"search_patterns\": \"无法解析分析结果\"}\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"分析搜索模式时出错: {e}\")\n",
    "                analysis = {\"dominant_topics\": [], \"search_patterns\": f\"分析出错: {str(e)}\"}\n",
    "\n",
    "            # 整合结果\n",
    "            return {\n",
    "                \"platform_stats\": platforms,\n",
    "                \"search_count\": len(searches),\n",
    "                \"analysis\": analysis,\n",
    "                \"timeframe\": f\"过去{days}天\"\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def generate_recommendations(self, user_id: str, count: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        基于用户画像生成内容推荐\n",
    "\n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            count: 推荐数量\n",
    "\n",
    "        Returns:\n",
    "            推荐的主题列表\n",
    "        \"\"\"\n",
    "        # 获取用户顶级兴趣\n",
    "        top_interests = self.get_top_interests(user_id, limit=5)\n",
    "\n",
    "        if not top_interests:\n",
    "            return [\"未找到用户兴趣数据\"]\n",
    "\n",
    "        # 提取兴趣主题\n",
    "        interest_topics = [i[\"topic\"] for i in top_interests]\n",
    "\n",
    "        # 使用LLM生成推荐\n",
    "        prompt = f\"\"\"\n",
    "        基于以下用户兴趣主题，推荐{count}个具体的、精细的研究或学习主题，这些主题应该是前沿的、有深度的，并与用户的兴趣紧密相关。\n",
    "\n",
    "        用户兴趣主题：{\", \".join(interest_topics)}\n",
    "\n",
    "        请列出具体的推荐主题，每个主题应包含足够的细节和专业性，以便能够直接用于学术研究或专业学习。\n",
    "        以JSON数组格式返回，每个元素包含'topic'和'reason'字段。\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的学习内容推荐助手，擅长为用户提供高质量、前沿的学习主题推荐。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            recs_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                recommendations = json.loads(recs_text)\n",
    "                return recommendations\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析推荐JSON: {recs_text}\")\n",
    "                return [{\"topic\": \"解析推荐失败\", \"reason\": \"请稍后再试\"}]\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"生成推荐时出错: {e}\")\n",
    "            return [{\"topic\": \"生成推荐出错\", \"reason\": str(e)}]\n",
    "\n",
    "    def get_user_profile_summary(self, user_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        获取用户画像摘要\n",
    "        \n",
    "        Args:\n",
    "            user_id: 用户ID\n",
    "            \n",
    "        Returns:\n",
    "            用户画像摘要信息\n",
    "        \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = get_db_connection()\n",
    "            if not conn:\n",
    "                return {\"error\": \"无法连接到数据库\"}\n",
    "                \n",
    "            # 获取基本信息\n",
    "            user = conn.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,)).fetchone()\n",
    "            \n",
    "            if not user:\n",
    "                return {\"error\": \"用户不存在\"}\n",
    "            \n",
    "            # 获取顶级兴趣\n",
    "            interests = []\n",
    "            try:\n",
    "                interests_query = conn.execute(\"\"\"\n",
    "                    SELECT i1.topic, i1.category, i1.weight, i1.timestamp \n",
    "                    FROM user_interests i1\n",
    "                    INNER JOIN (\n",
    "                        SELECT topic, MAX(timestamp) as max_time\n",
    "                        FROM user_interests\n",
    "                        WHERE user_id = ?\n",
    "                        GROUP BY topic\n",
    "                    ) i2 ON i1.topic = i2.topic AND i1.timestamp = i2.max_time\n",
    "                    WHERE user_id = ?\n",
    "                    ORDER BY i1.weight DESC\n",
    "                    LIMIT 10\n",
    "                \"\"\", (user_id, user_id))\n",
    "                \n",
    "                interests = [dict(i) for i in interests_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取用户兴趣时出错: {e}\")\n",
    "                interests = []\n",
    "            \n",
    "            # 获取技能\n",
    "            skills = []\n",
    "            try:\n",
    "                skills_query = conn.execute(\n",
    "                    \"SELECT skill, level, category FROM user_skills WHERE user_id = ? ORDER BY level DESC\",\n",
    "                    (user_id,)\n",
    "                )\n",
    "                skills = [dict(s) for s in skills_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取用户技能时出错: {e}\")\n",
    "                skills = []\n",
    "            \n",
    "            # 获取搜索统计\n",
    "            search_count = 0\n",
    "            try:\n",
    "                search_count_query = conn.execute(\n",
    "                    \"SELECT COUNT(*) as count FROM user_searches WHERE user_id = ?\", \n",
    "                    (user_id,)\n",
    "                ).fetchone()\n",
    "                if search_count_query:\n",
    "                    search_count = search_count_query[\"count\"]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取搜索统计时出错: {e}\")\n",
    "            \n",
    "            # 获取交互统计\n",
    "            interaction_count = 0\n",
    "            try:\n",
    "                interaction_count_query = conn.execute(\n",
    "                    \"SELECT COUNT(*) as count FROM user_interactions WHERE user_id = ?\", \n",
    "                    (user_id,)\n",
    "                ).fetchone()\n",
    "                if interaction_count_query:\n",
    "                    interaction_count = interaction_count_query[\"count\"]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取交互统计时出错: {e}\")\n",
    "            \n",
    "            # 获取最近5次搜索\n",
    "            recent_searches = []\n",
    "            try:\n",
    "                searches_query = conn.execute(\n",
    "                    \"SELECT query, platform, timestamp FROM user_searches WHERE user_id = ? ORDER BY timestamp DESC LIMIT 5\",\n",
    "                    (user_id,)\n",
    "                )\n",
    "                recent_searches = [dict(s) for s in searches_query.fetchall()]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"获取最近搜索时出错: {e}\")\n",
    "            \n",
    "            # 整合数据\n",
    "            profile = {\n",
    "                \"basic_info\": dict(user),\n",
    "                \"top_interests\": interests,\n",
    "                \"skills\": skills,\n",
    "                \"activity\": {\n",
    "                    \"search_count\": search_count,\n",
    "                    \"interaction_count\": interaction_count,\n",
    "                    \"recent_searches\": recent_searches\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return profile\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"获取用户画像摘要时出错: {e}\")\n",
    "            return {\"error\": f\"获取用户画像摘要时出错: {str(e)}\"}\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "\n",
    "class KnowledgeFlow:\n",
    "    def __init__(self):\n",
    "        self.context = {}\n",
    "        self.client = OpenAI(api_key=CONFIG[\"API_KEYS\"][\"deepseek\"], base_url=\"https://api.deepseek.com\")\n",
    "        self.serper_api_key = CONFIG[\"API_KEYS\"][\"serper\"]\n",
    "        self.user_profile_manager = UserProfileManager(client=self.client)\n",
    "\n",
    "\n",
    "    def start_node(self, user_input: Dict[str, Any]) -> Dict:\n",
    "        \"\"\" 收集用户初始信息 \"\"\"\n",
    "        logging.info(\"开始收集用户输入信息...\")  \n",
    "        required_fields = ['occupation', 'day', 'platform']\n",
    "        for field in required_fields:\n",
    "            if field not in user_input:\n",
    "                raise ValueError(f\"缺少必要字段: {field}\")\n",
    "\n",
    "        logging.debug(f\"用户输入信息: {user_input}\") \n",
    "        self.context.update(user_input)\n",
    "\n",
    "        # 创建或获取用户ID\n",
    "        if 'user_id' not in self.context and 'email' in user_input:\n",
    "            user_info = {\n",
    "                'name': user_input.get('name', '未知用户'),\n",
    "                'occupation': user_input.get('occupation', ''),\n",
    "                'email': user_input.get('email', '')\n",
    "            }\n",
    "            self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "\n",
    "        # platform_type = self.context.get('platform')\n",
    "        self.context['update_cycle'] = self.calculate_update_cycle(user_input['day'])\n",
    "        return self.context\n",
    "    \n",
    "    def calculate_update_cycle(self, days: int) -> Dict:\n",
    "        \"\"\" 计算时间范围 \"\"\"\n",
    "        logging.info(\"计算时间范围...\") \n",
    "        end_date = datetime.datetime.now(datetime.timezone.utc)\n",
    "        start_date = end_date - datetime.timedelta(days=days)\n",
    "        logging.debug(f\"时间范围计算结果: 起始日期={start_date}, 结束日期={end_date}\")\n",
    "        return {\n",
    "            \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": end_date.strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "\n",
    "    def get_user_profile(self, cv_text: str, task: str) -> Dict:\n",
    "        \"\"\"使用大语言模型API初步分析用户画像，并根据不同任务执行不同的prompt\"\"\"\n",
    "        logging.info(f\"执行任务：{task}，分析简历内容...\")\n",
    "        print(f\"\\n===== 开始执行用户画像分析任务：{task} =====\")\n",
    "\n",
    "        # 初始化返回值，确保始终返回一个字典\n",
    "        profile_analysis = {\"skills\": [], \"interests\": []}\n",
    "\n",
    "        # 如果有用户ID，先调用用户画像管理器处理简历\n",
    "        if 'user_id' in self.context:\n",
    "            user_id = self.context['user_id']\n",
    "            print(f\"用户ID: {user_id}\")\n",
    "\n",
    "            # 提取技能\n",
    "            print(\"\\n第1步：提取用户技能\")\n",
    "            skills = self.user_profile_manager.extract_skills_from_resume(user_id, cv_text)\n",
    "            logging.info(f\"从简历中提取了 {len(skills)} 项技能\")\n",
    "            print(f\"技能提取完成，共 {len(skills)} 项\")\n",
    "\n",
    "            # 提取兴趣\n",
    "            print(\"\\n第2步：提取用户兴趣\")\n",
    "            interests = self.user_profile_manager.extract_interests_from_resume(user_id, cv_text)\n",
    "            logging.info(f\"从简历中提取了 {len(interests)} 项兴趣\")\n",
    "            print(f\"兴趣提取完成，共 {len(interests)} 项\")\n",
    "\n",
    "            # 合并技能和兴趣信息到分析结果\n",
    "            profile_analysis = {\n",
    "                \"skills\": skills,\n",
    "                \"interests\": interests\n",
    "            }\n",
    "\n",
    "            # 定义不同任务的prompt模板\n",
    "            print(\"\\n第3步：生成综合分析报告\")\n",
    "            prompt_templates = {\n",
    "                \"analyze_resume\": f\"分析以下简历内容，提供全面的职业画像分析：{cv_text}\",\n",
    "                \"user_interest\": f\"根据以下简历内容，识别用户的职业兴趣和专注领域：{cv_text}\",\n",
    "                \"skill_assessment\": f\"根据以下简历内容，评估用户的技能并提出改进建议：{cv_text}\",\n",
    "                \"career_development\": f\"根据以下简历内容，分析用户的职业发展路径并提供建议：{cv_text}\"\n",
    "            }\n",
    "\n",
    "            # 确保指定的任务在模板中存在\n",
    "            if task not in prompt_templates:\n",
    "                logging.warning(f\"未知任务: {task}，将使用默认分析\")\n",
    "                task = \"analyze_resume\"\n",
    "            \n",
    "            prompt = prompt_templates[task]\n",
    "            logging.debug(f\"任务的prompt: {prompt}\") \n",
    "\n",
    "            try:\n",
    "                # 调用大语言模型API获取任务的处理结果\n",
    "                print(\"正在生成综合分析报告...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个帮助助手，负责根据用户提供的信息分析并生成建议。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                # 从大语言模型响应中提取分析结果\n",
    "                llm_analysis = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型返回的分析结果: {llm_analysis}\") \n",
    "                \n",
    "                # 合并LLM分析结果到profile_analysis\n",
    "                profile_analysis[\"llm_analysis\"] = llm_analysis\n",
    "            except Exception as e:\n",
    "                logging.error(f\"生成综合分析报告时出错: {e}\")\n",
    "                print(f\"生成综合分析报告时出错: {e}\")\n",
    "                profile_analysis[\"llm_analysis\"] = \"无法生成分析报告\"\n",
    "\n",
    "            # 更新上下文，保存用户画像分析结果\n",
    "            self.context.update({\"profile_analysis\": profile_analysis})\n",
    "            print(\"\\n用户画像分析完成！\")\n",
    "            return {\"profile_analysis\": profile_analysis}\n",
    "        else:\n",
    "            # 没有用户ID，回退到原来的方法\n",
    "            print(\"未找到用户ID，将使用简化版用户画像分析\")\n",
    "            # 定义不同任务的prompt模板\n",
    "            prompt_templates = {\n",
    "                \"analyze_resume\": f\"分析以下简历内容：{cv_text}\",\n",
    "                \"user_interest\": f\"根据以下简历内容，识别用户的职业兴趣和专注领域：{cv_text}\",\n",
    "                \"skill_assessment\": f\"根据以下简历内容，评估用户的技能并提出改进建议：{cv_text}\",\n",
    "            }\n",
    "\n",
    "            # 确保指定的任务在模板中存在\n",
    "            if task not in prompt_templates:\n",
    "                tem_task = task\n",
    "                task = \"analyze_resume\"\n",
    "                raise ValueError(f\"未知任务: {tem_task}. 请定义一个有效的任务。\")\n",
    "\n",
    "            prompt = prompt_templates[task]\n",
    "            logging.debug(f\"任务的prompt: {prompt}\")\n",
    "\n",
    "            try:\n",
    "                # 调用大语言模型API获取任务的处理结果\n",
    "                print(\"正在生成简化版分析报告...\")\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个帮助助手，负责根据用户提供的信息分析并生成建议。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                # 从大语言模型响应中提取分析结果\n",
    "                profile_data = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型返回的分析结果: {profile_data}\") \n",
    "                \n",
    "                # 更新上下文，保存用户画像分析结果\n",
    "                profile_analysis = {\"text_analysis\": profile_data}\n",
    "                self.context.update({\"profile_analysis\": profile_analysis})\n",
    "            except Exception as e:\n",
    "                logging.error(f\"生成简化版分析报告时出错: {e}\")\n",
    "                print(f\"生成简化版分析报告时出错: {e}\")\n",
    "                profile_analysis = {\"text_analysis\": \"无法生成分析报告\"}\n",
    "                self.context.update({\"profile_analysis\": profile_analysis})\n",
    "                \n",
    "            print(\"\\n简化版用户画像分析完成！\")\n",
    "            return {\"profile_analysis\": profile_analysis}\n",
    "\n",
    "    def build_user_profile(self, user_input: Dict, cv_text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        构建用户画像，确保用户ID存在并分析用户简历\n",
    "        \n",
    "        Args:\n",
    "            user_input: 用户输入的基本信息\n",
    "            cv_text: 用户简历文本\n",
    "            \n",
    "        Returns:\n",
    "            用户画像信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if cv_text.strip():\n",
    "                print(\"已收到简历，开始分析...\")\n",
    "\n",
    "                # 确保用户有ID - 如果没有，创建一个新用户\n",
    "                if 'user_id' not in self.context and 'email' in user_input:\n",
    "                    print(\"检测到新用户，正在创建用户档案...\")\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', ''),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': user_input.get('email', '')\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建新用户，ID: {self.context['user_id']}\")\n",
    "\n",
    "                # 如果仍然没有用户ID（可能是因为没有提供邮箱），创建一个临时ID\n",
    "                if 'user_id' not in self.context:\n",
    "                    import hashlib\n",
    "                    import time\n",
    "                    temp_id = hashlib.md5(f\"{time.time()}-{user_input.get('occupation', '')}-temp\".encode()).hexdigest()\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '临时用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': f\"temp_{temp_id[:8]}@example.com\"  # 创建临时邮箱\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建临时用户，ID: {self.context['user_id']}\")\n",
    "                    print(\"注意：由于未提供邮箱，此用户为临时用户，数据可能不会长期保存\")\n",
    "\n",
    "                # 现在可以确保有用户ID了，继续进行用户画像分析\n",
    "                try:\n",
    "                    task = \"analyze_resume\"  # 或根据需要选择其他任务\n",
    "                    user_profile = self.get_user_profile(cv_text, task)\n",
    "                    # 更新用户画像信息\n",
    "                    if user_profile:\n",
    "                        self.context.update(user_profile)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"分析用户画像时出错: {e}\")\n",
    "                    print(f\"分析用户画像时出错: {e}\")\n",
    "                    print(\"将继续使用基本用户信息\")\n",
    "                    user_profile = {\"basic_profile\": True}\n",
    "\n",
    "                # 显示用户画像摘要\n",
    "                try:\n",
    "                    self.display_profile_summary()\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"显示用户画像摘要时出错: {e}\")\n",
    "                    print(f\"显示用户画像摘要时出错: {e}\")\n",
    "                \n",
    "                return user_profile or {\"basic_profile\": True}\n",
    "            else:\n",
    "                print(\"未提供简历，但仍将创建基本用户档案\")\n",
    "\n",
    "                # 即使没有简历，也要确保用户有ID\n",
    "                if 'user_id' not in self.context and 'email' in user_input:\n",
    "                    print(\"创建基本用户档案...\")\n",
    "                    user_info = {\n",
    "                        'name': user_input.get('name', '未知用户'),\n",
    "                        'occupation': user_input.get('occupation', ''),\n",
    "                        'email': user_input.get('email', '')\n",
    "                    }\n",
    "                    self.context['user_id'] = self.user_profile_manager.create_user(user_info)\n",
    "                    print(f\"已创建新用户，ID: {self.context['user_id']}\")\n",
    "\n",
    "                    # 从用户输入中提取基本兴趣\n",
    "                    if 'content_type' in user_input and user_input['content_type']:\n",
    "                        print(f\"基于您提供的关注领域'{user_input['content_type']}'添加初始兴趣\")\n",
    "                        try:\n",
    "                            self.user_profile_manager.update_interest_weights(\n",
    "                                self.context['user_id'],\n",
    "                                user_input['content_type'],\n",
    "                                0.8  # 较高的初始权重\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"添加初始兴趣时出错: {e}\")\n",
    "                            print(f\"添加初始兴趣时出错: {e}\")\n",
    "                \n",
    "                return {\"basic_profile\": True}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"构建用户画像时出错: {e}\")\n",
    "            print(f\"构建用户画像时出错: {e}\")\n",
    "            print(\"将继续使用基本用户信息\")\n",
    "            return {\"basic_profile\": True}\n",
    "\n",
    "    def display_profile_summary(self):\n",
    "        \"\"\"显示用户画像摘要\"\"\"\n",
    "        if 'user_id' in self.context:\n",
    "            try:\n",
    "                profile_summary = self.user_profile_manager.get_user_profile_summary(self.context['user_id'])\n",
    "                if not profile_summary or 'error' in profile_summary:\n",
    "                    print(\"\\n--- 用户画像摘要 ---\")\n",
    "                    print(f\"用户ID: {self.context['user_id']}\")\n",
    "                    print(\"无法获取完整的用户画像信息\")\n",
    "                    print(\"-----------------\\n\")\n",
    "                    return\n",
    "                    \n",
    "                print(\"\\n--- 用户画像摘要 ---\")\n",
    "                print(f\"用户ID: {self.context['user_id']}\")\n",
    "                print(f\"用户名: {self.context['user_name']}\")\n",
    "                print(f\"职业: {profile_summary.get('basic_info', {}).get('occupation', '未知')}\")\n",
    "                \n",
    "                # 安全地获取技能数量\n",
    "                skills = profile_summary.get('skills', [])\n",
    "                print(f\"技能数量: {len(skills)}\")\n",
    "                \n",
    "                # 安全地获取顶级兴趣\n",
    "                interests = profile_summary.get('top_interests', [])\n",
    "                print(\"顶级兴趣:\")\n",
    "                if interests:\n",
    "                    for interest in interests[:3]:\n",
    "                        print(f\"  - {interest.get('topic', '未知')} (权重: {interest.get('weight', 0):.2f})\")\n",
    "                else:\n",
    "                    print(\"  暂无兴趣数据\")\n",
    "                print(\"-----------------\\n\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"显示用户画像摘要时出错: {e}\")\n",
    "                print(f\"\\n显示用户画像摘要时出错: {e}\")\n",
    "                print(\"将继续执行后续步骤\")\n",
    "\n",
    "    def _update_interests_from_query(self, query: str, weight_adjustment: float = 0.05):\n",
    "        \"\"\"\n",
    "        从用户查询中提取可能的兴趣点并更新用户画像\n",
    "\n",
    "        Args:\n",
    "            query: 用户查询\n",
    "            weight_adjustment: 权重调整幅度\n",
    "        \"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "\n",
    "        # 使用LLM提取查询中的兴趣点\n",
    "        prompt = f\"\"\"\n",
    "        请从以下搜索查询中提取最多3个主要的兴趣领域或关键主题。\n",
    "        只返回提取的主题列表，格式为JSON数组，例如：[\"人工智能\", \"机器学习\", \"自然语言处理\"]\n",
    "\n",
    "        搜索查询: {query}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的主题提取助手，擅长从文本中识别核心主题。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            topics_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                topics = json.loads(topics_text)\n",
    "\n",
    "                # 更新每个主题的权重\n",
    "                for topic in topics:\n",
    "                    self.user_profile_manager.update_interest_weights(user_id, topic, weight_adjustment)\n",
    "                    logging.info(f\"从查询中更新用户兴趣: {topic}, 调整: +{weight_adjustment}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取查询主题时出错: {e}\")\n",
    "\n",
    "    async def translate_query(self, query):\n",
    "        \"\"\"使用多引擎翻译并比较结果\"\"\"\n",
    "        translations = {}\n",
    "\n",
    "        try:\n",
    "            translations['谷歌'] = GoogleTranslator(source='auto', target='en').translate(query)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"谷歌翻译失败: {e}\")\n",
    "\n",
    "        # try:\n",
    "        #     translations['百度'] = BaiduTranslator(appid='YOUR_ID', appkey='YOUR_KEY').translate(query, dst='en')\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"百度翻译失败: {e}\")\n",
    "\n",
    "        # 其他翻译API\n",
    "        # try:\n",
    "        #     translations['DeepL'] = DeepLTranslator(source='ZH', target='EN').translate(query)\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"DeepL翻译失败: {e}\")\n",
    "\n",
    "        if not translations:\n",
    "            logging.error(\"所有翻译引擎均失败\")\n",
    "            return query\n",
    "\n",
    "        translations_text = \"\\n\".join([f\"{engine}翻译：{result}\" for engine, result in translations.items()])\n",
    "\n",
    "        try:\n",
    "            validation = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是专业翻译验证助手。回答只用翻译后的内容本身！以下是不同引擎翻译的结果，请你思考它们从中文到英文翻译的准确性，并提供最准确的翻译。最终结果只用翻译后的内容本身。\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"原文：{query}\\n{translations_text}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            translated_query = validation.choices[0].message.content\n",
    "            translations['大模型'] = translated_query\n",
    "\n",
    "            # 如果有用户ID，记录这次翻译\n",
    "            if 'user_id' in self.context:\n",
    "                # 更新用户兴趣 - 从查询中提取可能的兴趣点\n",
    "                self._update_interests_from_query(query)\n",
    "\n",
    "            best_translation = None\n",
    "            best_similarity = -1\n",
    "\n",
    "            for engine, translated_text in translations.items():\n",
    "                similarity = compute_similarity(query, translated_text)\n",
    "                logging.info(f\"{engine}翻译为：{translated_text}，相似度: {similarity}\")\n",
    "\n",
    "                if similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_translation = translated_text\n",
    "\n",
    "            logging.info(f\"最终翻译结果:{translated_query} ，相似度是：{best_similarity}\")\n",
    "            return translated_query # 目前匹配效果不佳，暂用大模型结果\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"大语言模型验证失败: {e}\")\n",
    "            # 如果验证失败，返回第一个可用的翻译结果\n",
    "            return next(iter(translations.values()))\n",
    "            \n",
    "    async def build_search_query(self) -> Dict:\n",
    "        \"\"\"根据用户需求构建搜索查询，并将查询词转换为英文\"\"\"\n",
    "        logging.info(\"构建搜索查询并进行翻译...\")  \n",
    "\n",
    "        query_base = self.context.get('content_type', \" \".join(self.context.get('content_focus', [])))\n",
    "\n",
    "        # 如果有用户ID，基于用户兴趣增强查询\n",
    "        if 'user_id' in self.context:\n",
    "            user_id = self.context['user_id']\n",
    "            top_interests = self.user_profile_manager.get_top_interests(user_id, limit=3)\n",
    "\n",
    "            if top_interests:\n",
    "                # 获取顶部兴趣与当前查询的关系\n",
    "                prompt = f\"\"\"\n",
    "                以下是用户的当前搜索请求和顶级兴趣。\n",
    "                请判断这些兴趣是否可以用于增强当前搜索，如果可以，请提供一个更优化的搜索查询。\n",
    "                如果不应该增强，只返回原始查询。\n",
    "\n",
    "                当前搜索请求: {query_base}\n",
    "                用户顶级兴趣: {\", \".join([interest[\"topic\"] for interest in top_interests])}\n",
    "                \"\"\"\n",
    "\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"你是一个专业的搜索优化助手，擅长根据用户兴趣优化搜索查询。\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    enhanced_query = response.choices[0].message.content\n",
    "                    if enhanced_query and enhanced_query != query_base and \"不应该增强\" not in enhanced_query:\n",
    "                        logging.info(f\"基于用户兴趣增强查询: {query_base} -> {enhanced_query}\")\n",
    "                        query_base = enhanced_query\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"增强查询时出错: {e}\")\n",
    "\n",
    "        # 翻译查询\n",
    "        translated_query_base = await self.translate_query(query_base)\n",
    "        if translated_query_base == query_base:\n",
    "            logging.warning(\"翻译内容与原查询一致，可能未成功翻译。\")\n",
    "    \n",
    "        # 构建基于时间范围的搜索查询\n",
    "        time_range = f\"after:{self.context['update_cycle']['start_date']} before:{self.context['update_cycle']['end_date']}\"\n",
    "        logging.debug(f\"构建的搜索查询：query_google={translated_query_base} {time_range} site:google.com\")  \n",
    "        \n",
    "        # 如果有用户ID，记录这次搜索\n",
    "        if 'user_id' in self.context:\n",
    "            self.user_profile_manager.record_search(\n",
    "                self.context['user_id'],\n",
    "                query_base,\n",
    "                self.context.get('platform', '未指定')\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"query_google\": f\"{translated_query_base} {time_range} site:google.com\",\n",
    "            \"query_arxiv\": f'\\\"{translated_query_base}\\\" AND submittedDate:[{self.context[\"update_cycle\"][\"start_date\"].replace(\"-\", \"\")} TO {self.context[\"update_cycle\"][\"end_date\"].replace(\"-\", \"\")}]',\n",
    "            \"query_google_arxiv\": f\"{translated_query_base} {time_range} arXiv site:arxiv.org\"\n",
    "        }\n",
    "\n",
    "    def execute_search(self, queries: Dict) -> Dict:\n",
    "        \"\"\"执行实际的搜索操作\"\"\"\n",
    "        logging.info(\"执行搜索操作...\")  \n",
    "        results = {}\n",
    "\n",
    "        # 获取用户选择的平台类型\n",
    "        platform_type = self.context.get('platform', '新闻类')\n",
    "\n",
    "        # 根据不同的平台类型调整搜索策略和结果数量\n",
    "        if platform_type == \"学术期刊\":\n",
    "            # 学术期刊类：只使用ArXiv搜索，最多返回7条结果\n",
    "            if queries.get('query_google_arxiv'):\n",
    "                logging.info(\"用户选择学术期刊类，执行Google_ArXiv搜索...\")\n",
    "                logging.debug(f\"执行Google_ArXiv搜索: {queries['query_google_arxiv']}\")\n",
    "                results['google_arxiv'] = self.arxiv_search(queries['query_google_arxiv'], max_results=7)\n",
    "            if queries.get('query_arxiv'):\n",
    "                logging.info(\"用户选择学术期刊类，执行ArXiv搜索...\")\n",
    "                logging.debug(f\"执行ArXiv搜索: {queries['query_arxiv']}\") \n",
    "                results['arxiv'] = self.arxiv_search(queries['query_arxiv'], max_results=7)\n",
    "\n",
    "                \n",
    "        elif platform_type == \"新闻类\":\n",
    "            # 新闻类：只使用Google搜索，最多返回7条结果\n",
    "            logging.info(\"用户选择新闻类，执行Google搜索...\")\n",
    "            if queries.get('query_google'):\n",
    "                logging.debug(f\"执行Google搜索: {queries['query_google']}\")  \n",
    "                results['google'] = self.google_search(queries['query_google'], max_results=7)\n",
    "                \n",
    "        else:  # 综合类或其他类型\n",
    "            # 综合类：同时使用Google和ArXiv搜索，各返回最多4条结果\n",
    "            logging.info(\"用户选择综合类，执行Google和ArXiv搜索...\")\n",
    "            if queries.get('query_google'):\n",
    "                logging.debug(f\"执行Google搜索: {queries['query_google']}\")  \n",
    "                results['google'] = self.google_search(queries['query_google'], max_results=4)\n",
    "            if queries.get('query_arxiv'):\n",
    "                logging.debug(f\"执行ArXiv搜索: {queries['query_arxiv']}\") \n",
    "                results['arxiv'] = self.arxiv_search(queries['query_arxiv'], max_results=4)\n",
    "            if queries.get('query_google_arxiv'):\n",
    "                logging.debug(f\"执行Google_ArXiv搜索: {queries['query_google_arxiv']}\") \n",
    "                results['google_arxiv'] = self.arxiv_search(queries['query_google_arxiv'], max_results=4)\n",
    "\n",
    "        logging.debug(f\"搜索结果: {results}\")  \n",
    "        return results\n",
    "\n",
    "\n",
    "    def google_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行Google搜索，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行Google搜索，限制结果数量为{max_results}...\")  \n",
    "        api_url = f\"https://google.serper.dev/search?q={query}&num={max_results}\"\n",
    "        headers = {'X-API-KEY': self.serper_api_key}\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            logging.debug(f\"Google搜索返回结果数量: {len(search_results.get('organic', []))}\")\n",
    "            logging.debug(f\"Google搜索结果: {search_results}\")\n",
    "            # 限制结果数量\n",
    "            if 'organic' in search_results:\n",
    "                search_results['organic'] = search_results['organic'][:max_results]\n",
    "            return self.parse_google_results(search_results, query)\n",
    "        else:\n",
    "            logging.error(f\"Google搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"Google搜索失败\"}\n",
    "    \n",
    "    def parse_google_results(self, data: Dict, query: str) -> Dict:\n",
    "        \"\"\"解析Google搜索结果\"\"\"\n",
    "        results = []\n",
    "        for item in data.get('organic', []):\n",
    "            title = item.get('title', '')\n",
    "            snippet = item.get('snippet', '')[:1400]  # 限制摘要长度\n",
    "            link = item.get('link', '')\n",
    "            date = item.get('date', '')\n",
    "            position = item.get('position', '')\n",
    "\n",
    "            # 计算标题和摘要与查询的相似度\n",
    "            title_similarity = compute_similarity(query, title)\n",
    "            snippet_similarity = compute_similarity(query, snippet)\n",
    "            overall_similarity = 0.3 * title_similarity + 0.7 * snippet_similarity\n",
    "\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'snippet': snippet,\n",
    "                'link': link,\n",
    "                'date': date,\n",
    "                'position': position,\n",
    "                'similarity': overall_similarity,\n",
    "            })\n",
    "\n",
    "        # 按相似度排序\n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "        return {'results': results}\n",
    "\n",
    "    def arxiv_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行ArXiv搜索，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行ArXiv搜索，限制结果数量为{max_results}...\")\n",
    "        api_url = f'http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}'\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            #logging.debug(f\"ArXiv API response (first 500 chars): {response.text[:500]}...\")\n",
    "            arxiv_results = self.parse_arxiv_response(response.text, query)\n",
    "            logging.debug(f\"ArXiv搜索返回结果数量: {len(arxiv_results.get('results', []))}\")\n",
    "            return arxiv_results\n",
    "        else:\n",
    "            logging.error(f\"ArXiv搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"ArXiv搜索失败\"}\n",
    "\n",
    "    def parse_arxiv_response(self, xml_data: str, query: str) -> Dict:\n",
    "        \"\"\"解析ArXiv的响应数据\"\"\"\n",
    "        tree = ElementTree.fromstring(xml_data)\n",
    "        results = []\n",
    "        for entry in tree.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "\n",
    "            link = \"\"\n",
    "            for link_element in entry.findall(\"{http://www.w3.org/2005/Atom}link\"):\n",
    "                if link_element.get(\"rel\") == \"alternate\":\n",
    "                    link = link_element.get(\"href\")\n",
    "                    break\n",
    "\n",
    "            if not link:\n",
    "                link_element = entry.find(\"{http://www.w3.org/2005/Atom}link\")\n",
    "                if link_element is not None:\n",
    "                    link = link_element.get(\"href\", \"\")\n",
    "\n",
    "            # 计算标题和摘要与查询的相似度\n",
    "            title_similarity = compute_similarity(query, title)\n",
    "            summary_similarity = compute_similarity(query, summary)\n",
    "            overall_similarity = 0.7 * title_similarity + 0.3 * summary_similarity\n",
    "\n",
    "            # 提取发布日期\n",
    "            # date = entry.find(\"{http://www.w3.org/2005/Atom}published\").text if entry.find(\"{http://www.w3.org/2005/Atom}published\") is not None else \"\"\n",
    "\n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'snippet': summary[:1400],\n",
    "                'link': link,\n",
    "                #'date': date,\n",
    "                'similarity': overall_similarity\n",
    "            })\n",
    "\n",
    "        # 按照相似度进行排序\n",
    "        results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "        return {\"results\": results}\n",
    "\n",
    "    def google_arxiv_search(self, query: str, max_results) -> Dict:\n",
    "        \"\"\"执行Google搜索，用于搜索ArXiv文献，并限制结果数量\"\"\"\n",
    "        logging.info(f\"执行Google搜索用于查找ArXiv文献，限制结果数量为{max_results}...\")  \n",
    "        api_url = f\"https://google.serper.dev/search?q={query}&num={max_results}\"\n",
    "        headers = {'X-API-KEY': self.serper_api_key}  \n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            logging.debug(f\"Google_ArXiv搜索返回结果数量: {len(search_results.get('organic', []))}\")\n",
    "            if 'organic' in search_results:\n",
    "                search_results['organic'] = search_results['organic'][:max_results]\n",
    "            #logging.debug(f\"Google ArXiv搜索结果: {search_results}\") \n",
    "            return self.parse_google_results(search_results, query)\n",
    "        else:\n",
    "            logging.error(f\"Google ArXiv搜索失败，状态码: {response.status_code}\")\n",
    "            return {\"error\": \"Google ArXiv搜索失败\"}\n",
    "    \n",
    "    def integrate_with_large_model(self, search_results: Dict) -> str:\n",
    "        \"\"\"调用大语言模型进行整合\"\"\"\n",
    "        logging.info(\"调用大语言模型进行整合搜索结果...\")  \n",
    "    \n",
    "        # 将搜索结果转换为大语言模型所需的格式\n",
    "        search_results_str = json.dumps(search_results, ensure_ascii=False, cls=NumpyEncoder)\n",
    "\n",
    "        try:\n",
    "            # 调用大语言模型的API进行结果整合\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                # model=\"qwen-plus\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"你是一个内容整理助手，负责根据用户提供的信息整合搜索结果并生成报告。使用中文。不要md格式。\"\n",
    "                            \"将报告中的内容，以：\\\"来源：...（Google/arXiv/Google_arXiv等平台） \\n 标题：... \\n 摘要：... \\n 原文网址：...（只使用搜索结果中提供的真实链接）\\n BERT嵌入的余弦相似度:... \\\"的形式呈现出来。\"\n",
    "                            \"如果搜索结果中没有提供原文网址，则写'原文网址：未提供'。不要编造或猜测网址。\"\n",
    "                            \"报告使用用户交谈时的语言，如果原文不是，则准确的转化为用户使用的语言。目前的用户使用的是中文，将结果也转化为中文！\"\n",
    "                            \"如果无法完成就直接翻译用snippet的内容回答将\\\"摘要：\\\"改为\\\"片段：\\\"。\"\n",
    "                            \"并且回答严格按照规范来，就算无法完成任务也不要说别的不符合规范的话。\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"请整合以下搜索结果并生成最终报告：{search_results_str}\"\n",
    "                    }\n",
    "                ],\n",
    "                stream=False\n",
    "            )\n",
    "            if response and hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "                integration_result = response.choices[0].message.content\n",
    "                logging.debug(f\"大语言模型整合结果: {integration_result}\")\n",
    "            else:\n",
    "                raise ValueError(\"API响应没有有效的choices字段\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"调用大语言模型整合时发生错误: {e}\")\n",
    "            integration_result = \"由于API错误，无法生成整合结果。\"\n",
    "        \n",
    "        return integration_result\n",
    "    \n",
    "    def _extract_interest_from_content(self, title: str, snippet: str, weight_adjustment: float = 0.03):\n",
    "        \"\"\"从内容中提取兴趣点并更新用户模型\"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "        combined_text = f\"{title}\\n{snippet}\"\n",
    "\n",
    "        # 提取主题\n",
    "        prompt = f\"\"\"\n",
    "        请从以下文本中提取最多3个核心学术或专业主题。\n",
    "        只返回主题列表，格式为JSON数组，例如：[\"强化学习\", \"计算机视觉\", \"神经网络\"]\n",
    "\n",
    "        文本:\n",
    "        {combined_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的主题提取助手，擅长从文本中识别核心学术主题。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            topics_text = response.choices[0].message.content\n",
    "\n",
    "            try:\n",
    "                topics = json.loads(topics_text)\n",
    "\n",
    "                # 更新每个主题的权重\n",
    "                for topic in topics:\n",
    "                    self.user_profile_manager.update_interest_weights(user_id, topic, weight_adjustment)\n",
    "                    logging.info(f\"从内容中提取用户兴趣: {topic}, 调整: +{weight_adjustment}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"提取内容主题时出错: {e}\")\n",
    "\n",
    "    def generate_report(self, search_results: Dict) -> str:\n",
    "        \"\"\"生成最终的搜索报告\"\"\"\n",
    "        logging.info(\"生成最终的搜索报告...\") \n",
    "        report = []\n",
    "        platform_type = self.context.get('platform')\n",
    "        \n",
    "        # 先构建Google、ArXiv等来源的报告内容\n",
    "        for source, data in search_results.items():\n",
    "            if 'error' in data:\n",
    "                report.append(f\"来源：{source}\\n错误：{data['error']}\")\n",
    "            else:\n",
    "                for item in data.get('results', [])[:3]: \n",
    "                    if isinstance(item, dict): \n",
    "                        title = item.get('title', '')\n",
    "                        snippet = item.get('snippet', '')\n",
    "                        link = item.get('link', '')\n",
    "                        report.append(f\"来源：{source}\\n标题：{title}\\n摘要：{snippet}...\\n链接：{link}\")\n",
    "\n",
    "                    # 如果有用户ID，记录这个内容为潜在兴趣点\n",
    "                    # if 'user_id' in self.context and link:\n",
    "                    #     # 从标题中提取可能的兴趣点\n",
    "                    #     self._extract_interest_from_content(title, snippet, weight_adjustment=0.03)\n",
    "\n",
    "                    #     # 记录内容交互\n",
    "                    #     self.user_profile_manager.record_interaction(\n",
    "                    #         self.context['user_id'],\n",
    "                    #         link,\n",
    "                    #         \"search_result\"\n",
    "                    #     )\n",
    "\n",
    "        # 再调用大语言模型整合结果\n",
    "        final_report = self.integrate_with_large_model(search_results)\n",
    "    \n",
    "        # 最终替换为大语言模型整合后的内容\n",
    "        report.clear()\n",
    "\n",
    "        # 如果有用户ID，记录这个内容为潜在兴趣点\n",
    "        # if 'user_id' in self.context and link:\n",
    "        #     # 从标题中提取可能的兴趣点\n",
    "        #     self._extract_interest_from_content(title, snippet, weight_adjustment=0.03)\n",
    "\n",
    "        #     # 记录内容交互\n",
    "        #     self.user_profile_manager.record_interaction(\n",
    "        #         self.context['user_id'],\n",
    "        #         link,\n",
    "        #         \"search_result\"\n",
    "        #     )\n",
    "\n",
    "        # 如果有用户ID，添加个性化推荐\n",
    "        # if 'user_id' in self.context:\n",
    "        #     # 应用时间衰减模型\n",
    "        #     self.user_profile_manager.apply_time_decay(self.context['user_id'])\n",
    "\n",
    "        #     # 获取推荐内容\n",
    "        #     try:\n",
    "        #         recommendations = self.user_profile_manager.generate_recommendations(self.context['user_id'], count=3)\n",
    "\n",
    "        #         # 添加推荐内容到报告\n",
    "        #         if recommendations and len(recommendations) > 0:\n",
    "        #             rec_text = \"\\n\\n--- 基于您的兴趣，我们还为您推荐以下主题 ---\\n\\n\"\n",
    "        #             for rec in recommendations:\n",
    "        #                 rec_text += f\"主题：{rec.get('topic', '')}\\n\"\n",
    "        #                 rec_text += f\"原因：{rec.get('reason', '')}\\n\\n\"\n",
    "\n",
    "        #             report.append(rec_text)\n",
    "        #     except Exception as e:\n",
    "        #         logging.error(f\"生成推荐时出错: {e}\")\n",
    "\n",
    "        report.append(f\"\\n\\n--- 根据您选择的【{platform_type}】平台，近几日的行业内最新进展已整理好，请查收！ ---\\n\")\n",
    "        report.append(final_report)\n",
    "\n",
    "        logging.debug(f\"生成的报告内容: {report}\")\n",
    "        return \"\\n\\n\".join(report)\n",
    "\n",
    "    def send_email(self, report: str):\n",
    "        \"\"\"发送邮件（示例）\"\"\"\n",
    "        logging.info(\"准备发送邮件...\")\n",
    "        if 'email' in self.context and report:\n",
    "            print(f\"已发送邮件到 {self.context['email']}:\\n{report}\")\n",
    "        else:\n",
    "            print(\"未找到邮件地址或报告为空，未发送邮件。\")\n",
    "\n",
    "    def process_user_feedback(self, content_id: str, feedback_type: str, feedback_text: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        处理用户对内容的反馈\n",
    "\n",
    "        Args:\n",
    "            content_id: 内容ID（URL或其他标识）\n",
    "            feedback_type: 反馈类型（\"like\"、\"dislike\"、\"save\"、\"share\"等）\n",
    "            feedback_text: 反馈文本（可选）\n",
    "\n",
    "        Returns:\n",
    "            处理结果描述\n",
    "        \"\"\"\n",
    "        if 'user_id' not in self.context:\n",
    "            return \"无用户信息，无法处理反馈\"\n",
    "\n",
    "        user_id = self.context['user_id']\n",
    "\n",
    "        # 记录交互\n",
    "        self.user_profile_manager.record_interaction(user_id, content_id, feedback_type)\n",
    "\n",
    "        # 根据反馈类型调整兴趣权重\n",
    "        if feedback_type in (\"like\", \"save\", \"share\"):\n",
    "            # 提取内容关联主题并增加权重\n",
    "            try:\n",
    "                # 获取内容摘要（实际应用中可能需要从数据库或通过URL获取）\n",
    "                content_summary = feedback_text if feedback_text else \"用户喜欢的内容\"\n",
    "\n",
    "                # 提取主题\n",
    "                prompt = f\"\"\"\n",
    "                请从以下用户反馈中提取最多2个可能的兴趣主题。\n",
    "                只返回主题列表，格式为JSON数组。\n",
    "\n",
    "                用户反馈类型: {feedback_type}\n",
    "                反馈内容: {content_summary}\n",
    "                \"\"\"\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长分析用户反馈。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                topics_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    topics = json.loads(topics_text)\n",
    "\n",
    "                    # 增加各主题权重\n",
    "                    for topic in topics:\n",
    "                        self.user_profile_manager.update_interest_weights(user_id, topic, 0.1)\n",
    "                        logging.info(f\"基于正面反馈增加兴趣权重: {topic} +0.1\")\n",
    "\n",
    "                    return f\"已记录您对'{content_id}'的{feedback_type}反馈，并更新了您的兴趣模型\"\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "                    return \"已记录您的反馈，但分析主题时出错\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理反馈时出错: {e}\")\n",
    "                return f\"处理反馈时出错: {str(e)}\"\n",
    "\n",
    "        elif feedback_type == \"dislike\":\n",
    "            # 提取内容关联主题并减少权重\n",
    "            try:\n",
    "                content_summary = feedback_text if feedback_text else \"用户不喜欢的内容\"\n",
    "\n",
    "                # 提取主题\n",
    "                prompt = f\"\"\"\n",
    "                请从以下用户不喜欢的内容中提取最多2个可能的兴趣主题。\n",
    "                只返回主题列表，格式为JSON数组。\n",
    "\n",
    "                不喜欢的内容: {content_summary}\n",
    "                \"\"\"\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=CONFIG[\"MODELS\"][\"LLM\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"你是一个专业的兴趣分析助手，擅长分析用户反馈。\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                topics_text = response.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    topics = json.loads(topics_text)\n",
    "\n",
    "                    # 减少各主题权重\n",
    "                    for topic in topics:\n",
    "                        self.user_profile_manager.update_interest_weights(user_id, topic, -0.1)\n",
    "                        logging.info(f\"基于负面反馈减少兴趣权重: {topic} -0.1\")\n",
    "\n",
    "                    return f\"已记录您对'{content_id}'的不喜欢反馈，并更新了您的兴趣模型\"\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"无法解析主题JSON: {topics_text}\")\n",
    "                    return \"已记录您的反馈，但分析主题时出错\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"处理反馈时出错: {e}\")\n",
    "                return f\"处理反馈时出错: {str(e)}\"\n",
    "\n",
    "        return f\"已记录您对'{content_id}'的{feedback_type}反馈\"\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import docx\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "class ResumeReader:\n",
    "    \"\"\"用于读取多种格式简历文件的类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.supported_formats = {\n",
    "            '.txt': self.read_txt,\n",
    "            '.pdf': self.read_pdf,\n",
    "            '.docx': self.read_docx,\n",
    "            '.doc': self.read_doc,\n",
    "            '.xlsx': self.read_excel,\n",
    "            '.xls': self.read_excel,\n",
    "            '.jpg': self.read_image,\n",
    "            '.jpeg': self.read_image,\n",
    "            '.png': self.read_image,\n",
    "        }\n",
    "        logging.info(\"初始化ResumeReader，支持的格式：%s\", list(self.supported_formats.keys()))\n",
    "\n",
    "    def read_resume(self, file_path=None):\n",
    "        \"\"\"读取简历文件或请求用户输入\"\"\"\n",
    "        if not file_path:\n",
    "            choice = input(\"请选择输入方式：1.直接输入文本 2.上传文件 (如有需要进行用户画像构建请输入数字，不需要可回车（或确认）跳过): \")\n",
    "\n",
    "            if choice == \"\":\n",
    "                return \"\"\n",
    "            elif choice == \"1\":\n",
    "                return input(\"请输入您的简历文本：\")\n",
    "            elif choice == \"2\":\n",
    "                file_path = input(\"请输入简历文件的完整路径：\").strip().strip('\"')\n",
    "            else:\n",
    "                logging.warning(\"无效的选择，默认使用文本输入方式\")\n",
    "                return input(\"请输入您的简历文本：\")\n",
    "\n",
    "        while not os.path.exists(file_path):\n",
    "            logging.error(f\"文件不存在: {file_path}\")\n",
    "            choice = input(f\"文件不存在: {file_path}\\n请输入有效的文件路径，或输入 'q' 退出：\").strip()\n",
    "\n",
    "            if choice.lower() == 'q':\n",
    "                logging.info(\"用户选择退出\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                file_path = choice.strip()\n",
    "\n",
    "        file_path = file_path.strip('\\\"')\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if file_ext not in self.supported_formats:\n",
    "            logging.warning(f\"不支持的文件格式: {file_ext}，支持的格式有: {list(self.supported_formats.keys())}\")\n",
    "            return self.ask_for_input()\n",
    "\n",
    "        # 调用相应的文件读取方法\n",
    "        try:\n",
    "            text = self.supported_formats[file_ext](file_path)\n",
    "            logging.info(f\"成功读取{file_ext}格式简历文件\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"读取文件时出错: {e}\")\n",
    "            return self.ask_for_input()\n",
    "\n",
    "    def ask_for_input(self):\n",
    "        \"\"\"帮助用户提供错误反馈并重新输入\"\"\"\n",
    "        return input(\"请输入您的简历文本：\")\n",
    "\n",
    "    def read_txt(self, file_path):\n",
    "        \"\"\"读取txt文本文件\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # 尝试其他编码\n",
    "            with open(file_path, 'r', encoding='gbk') as f:\n",
    "                return f.read()\n",
    "\n",
    "    def read_pdf(self, file_path):\n",
    "        \"\"\"读取PDF文件\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def read_docx(self, file_path):\n",
    "        \"\"\"读取Word docx文件\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    def read_doc(self, file_path):\n",
    "        \"\"\"读取旧版 Word doc文件 (需要转换)\"\"\"\n",
    "        logging.warning(\"直接读取.doc文件需要额外依赖，建议转换为.docx或.pdf格式\")\n",
    "        return f\"无法直接读取.doc文件: {file_path}，请转换为.docx或.pdf格式后重试。\"\n",
    "\n",
    "    def read_excel(self, file_path):\n",
    "        \"\"\"读取Excel文件\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    def read_image(self, file_path):\n",
    "        \"\"\"使用OCR读取图片中的文本\"\"\"\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(img, lang='chi_sim+eng')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"OCR处理图片时出错: {e}\")\n",
    "            return f\"OCR处理图片时出错: {e}\"\n",
    "\n",
    "def collect_user_input() -> Dict:\n",
    "    \"\"\"收集真实用户输入\"\"\"\n",
    "    print(\"\\n===== 欢迎使用KnowlEdge系统 =====\")\n",
    "    print(\"请提供以下信息，以便我们为您提供个性化的行业知识更新\")\n",
    "\n",
    "    # user_name = input(\"请输入您的用户名: \").strip()\n",
    "    # occupation = input(\"请输入您的职业: \").strip() or \"算法工程师\"\n",
    "    # days = int(input(\"请输入获取知识更新周期（天数，默认10）: \").strip() or \"10\")\n",
    "    # platform = input(\"请输入消息来源平台（学术期刊/新闻类/综合类，默认学术期刊）: \").strip() or \"学术期刊\"\n",
    "    # content_type = input(\"请输入关注领域（如：大语言模型，默认大语言模型）: \").strip() or \"大语言模型\"\n",
    "    # email = input(\"请输入您的邮箱（用于接收报告和识别用户）: \").strip() or \"example@example.com\"\n",
    "\n",
    "    print(\"\\n您的信息已收集完毕，系统将基于这些信息为您提供个性化服务\")\n",
    "\n",
    "    user_name = \"Tssword1\"\n",
    "    occupation = \"算法工程师\"\n",
    "    days = 7\n",
    "    platform = \"学术期刊\"\n",
    "    content_type = \"大语言模型\"\n",
    "    email = \"1145141@qq.com\"\n",
    "\n",
    "    return {\n",
    "        \"user_name\": user_name,\n",
    "        \"occupation\": occupation,\n",
    "        \"day\": days,\n",
    "        \"platform\": platform,\n",
    "        \"content_type\": content_type,\n",
    "        \"email\": email\n",
    "    }\n",
    "    \n",
    "async def main():\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    print(\"\\n===== KnowlEdge系统启动 =====\")\n",
    "\n",
    "    # 验证数据库\n",
    "    if not verify_database():\n",
    "        print(\"数据库验证失败，系统可能无法正常工作\")\n",
    "        choice = input(\"是否继续运行? (y/n): \").strip().lower()\n",
    "        if choice != 'y':\n",
    "            print(\"系统退出\")\n",
    "            return\n",
    "\n",
    "    # 检查系统初始化状态\n",
    "    if not os.path.exists(CONFIG[\"DATA_DIR\"]) or not os.path.exists(CONFIG[\"DB_PATH\"]):\n",
    "        print(\"系统尚未初始化，正在进行初始化...\")\n",
    "        # 导入并运行初始化脚本\n",
    "        try:\n",
    "            import init_system\n",
    "            init_result = init_system.main()\n",
    "            if not init_result:\n",
    "                print(\"系统初始化失败，请检查日志并解决问题后重试\")\n",
    "                return\n",
    "        except ImportError:\n",
    "            print(\"找不到初始化脚本，请确保init_system.py文件存在\")\n",
    "            return\n",
    "\n",
    "    workflow = KnowledgeFlow()\n",
    "    print(\"KnowledgeFlow引擎已初始化\")\n",
    "\n",
    "    # 步骤 1：收集用户输入\n",
    "    print(\"\\n步骤 1/6: 收集用户信息\")\n",
    "    user_input = collect_user_input()\n",
    "    workflow.start_node(user_input)\n",
    "    print(\"用户信息已收集并处理\")\n",
    "\n",
    "    # 步骤 2：分析用户画像（可选）\n",
    "    print(\"\\n步骤 2/6: 用户画像分析\")\n",
    "    resume_reader = ResumeReader()\n",
    "    print(\"请提供您的简历以进行更精确的用户画像分析（可选）\")\n",
    "    cv_text = resume_reader.read_resume()\n",
    "\n",
    "    # 构建用户画像\n",
    "    workflow.build_user_profile(user_input, cv_text)\n",
    "\n",
    "    # 步骤 3：构建搜索参数\n",
    "    print(\"\\n步骤 3/6: 构建搜索参数\")\n",
    "    print(\"正在根据您的需求和兴趣构建搜索参数...\")\n",
    "    queries = await workflow.build_search_query()\n",
    "    print(f\"搜索参数构建完成，将在以下平台搜索: {', '.join(queries.keys())}\")\n",
    "\n",
    "\n",
    "    # 步骤 4：执行Google搜索、ArXiv搜索、和Google搜索ArXiv文献\n",
    "    print(\"\\n步骤 4/6: 执行搜索\")\n",
    "    print(f\"正在搜索与{user_input['content_type']}相关的最新信息...\")\n",
    "    search_results = workflow.execute_search(queries)\n",
    "    result_count = sum(len(data.get('results', [])) for source, data in search_results.items() if 'results' in data)\n",
    "    print(f\"搜索完成，共找到 {result_count} 条相关信息\")\n",
    "\n",
    "    # 步骤 5：生成报告\n",
    "    print(\"\\n步骤 5/6: 生成报告\")\n",
    "    print(\"正在整合搜索结果并生成报告...\")\n",
    "    report = workflow.generate_report(search_results)\n",
    "    print(\"报告生成完成\")\n",
    "\n",
    "    # 步骤 6：发送邮件\n",
    "    print(\"\\n步骤 6/6: 发送报告\")\n",
    "    workflow.send_email(report)\n",
    "\n",
    "    print(\"流程执行完成\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0940e9efdc9d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
